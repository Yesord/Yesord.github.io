[{"id":"3d0614aff1cb1047dc60965f7de9aa6f","title":"ComfyUI初体验--个性化二维码生成","content":"前言我在2022年就有在关注ai绘画领域，当时stable diffusion算刚刚出圈，现在flux都出来了，sd都更到SDXL了。ai绘画基本都使用WebUI作为GUI界面，虽然说WebUI也不是不好用，但就是比较传统。随着WebUI停止维护，许多的ai绘画工作者或爱好者都转到了ComfyUI的阵地，正好最近ComfyUI也更新了2.0版本的前端GUI，flux也说很牛，我上半年也搞了张4070super，种种机缘巧合之下，我又拾起了以前落下的AI绘画 （炼丹）开始玩。\n\nSD1代的上古遗骸–图生图，校徽拟人\n\n\n\n\n\n\n\nComfyUI\n\n\n\n\n\n\nComfyUI官网\nComfyUI官网\n\n\nComfyUI的下载安装网上教程业已详尽，我不过多赘述。\n\n\n\n\n\n\nTIP\n\n这里推荐一个入门视频\n不想动脑可以用秋叶大佬秋葉aaaki的ai绘画整合包\n\n\n\n我是自个上 ComfyUI GitHub库拉的release\n把release下载下来之后解压就可以看到\nwindows用户就可以直接运行如下的.bat批处理文件开启ComfyUI了\n\n如果电脑没Nvidia显卡就用第一个cpu版的，如果有就可以用第二个gpu版的。\n\n如果想通过命令行启动就要进到ComfyUI目录下找到如下的main.py执行（需要自己配环境，建议使用conda）\n\n\nbashpip install -r requirements.txt\npython main.py\n\n\n\n\n\n\nComfyUI使用体验\nComfyUI使用起来就像是画一个流程图，搞清楚你想要的生成图形的流程，用上ComfyUI的节点可以轻松完成很多的图形处理和生成工作。\n\n\n二维码生成思路参考的这篇文章Stable Diffusion QR Code 101\n总体流程参考二维码图像进行控制生成（图生图），使用prompt去控制生成的对象。\n二维码制作使用的是Anthony Fu大佬的QRCode制作器，这个二维码制作器可以很容易生成美观的二维码基模\n模型选取我这里只使用了controlnet和checkpoint这两个模型下载链接\ncheckpoint模型Checkpoint模型是真正的Stable Diffusion模型，包含了生成图像所需的所有内容，不需要任何额外的文件。我使用的是MeinaMix v10是基于Stable diffusion 1.5 训练出来的二次元风格特化的模型\n\n\n\n\n\n\n\n注意\n下载了checkpoint模型文件之后要把它放到ComfyUI的models&#x2F;checkpoint文件夹下\n\ncontrolnet模型ControlNet是一个控制预训练图像扩散模型（例如 Stable Diffusion）的神经网络。它允许输入调节图像，然后使用该调节图像来操控图像生成。能干的事情很多，想详细了解可以看这篇blog这两个网络是SD1.5的控制生成二维码的控制模型\n\n\n\n\n\n\n\n注意\n下载了controlnet模型文件之后要把它放到ComfyUI的models&#x2F;controlnet文件夹下\n\n搭建工作流参考工作流\n\n\n\n\n\n\n大概流程就是\n\n读取二维码图像，写prompt\n送入controlnet控制生成\n送入checkpoint第一次生成512x512的粗图\n读取人脸做精修\n生成图像超分放大\n\n\n\n\n生图结果展示\n\n\nshow more\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING\n刚把json文件导入ComfyUI可能会看到很多的节点缺失，ComfyUI是提供了一键安装的选项，只要直接安装等待安装结束重启ComfyUI即可使用。\n\n\n后记ComfyUI还挺好玩的吧\n\n嗨害嗨\n其实这篇blog的封面就是MeinaMix生成的哦\n\n\n","slug":"ComfyUI初体验-个性化二维码生成","date":"2024-11-05T14:59:50.000Z","categories_index":"AI art","tags_index":"Meinamix,ComfyUI","author_index":"PIKO"},{"id":"e7674ca755d44195295fe130df18e24b","title":"朴素贝叶斯-邮件分类编程实践","content":"前言本项目是基于朴素贝叶斯算法的侮辱性和非侮辱性邮件分类，使用的编程语言是python，是笔者在校系统性学习机器学习的课程小项目。\n项目前置python 版本要求\n\n\n\n\n\nTIP\npython&gt;&#x3D;3.10\n\n\n库依赖要求\n\n\n\n\n\nTIP\n数据处理\nnumpy &lt;&#x3D; 1.26.4pandas\n\n\n\n\n\n\n\nTIP\nGoogle API\ngoogle-auth-oauthlibgoogle-auth-httplib2google-api-python-client\n\n实现过程工程框架\n\n\n\n\n\n\n\nWARNING\nApp:存放用户代码，包括训练脚本train和测试脚本testdata:存放数据文件，包括数据集、OAuth登录令牌utils:存放工具集，包括gmail工具、数据准备工具、朴素贝叶斯类model:存放模型参数文件，贝叶斯模型参数\n\n本项目的工程文件已上传GitHub\n朴素贝叶斯类实现朴素贝叶斯算法基于贝叶斯定理，通过计算先验概率和后验概率来进行分类。在处理文本数据时，将邮件内容中的每个词视为特征，利用频率来估计条件概率。这种方法简单但有效，尤其适合文本分类任务。\n\n详细公式\n计算侮辱邮件的先验概率\n计算侮辱邮件条件下每个词的后验概率\n计算得分\n计算准确率\n\n训练训练步骤\n\n\n\n\n\nTIP\nSTEP1: 对分类器进行初始化，将每个类别的先验概率、存储每个类别下每个词的条件概率、词汇表、侮辱非侮辱类别标签进行初始化。STEP2: 遍历所有的邮件，将侮辱邮件的先验概率计算出来。STEP3: 根据邮件的词语使用，将词语记录到词库之中，并对每个词出现的频率进行统计得到训练集侮辱或非侮辱邮件条件下每个词出现的数量。STEP4: 得到每个词在侮辱或非侮辱邮件条件下的后验概率。STEP5: 所有邮件遍历完成之后保存模型参数。\n\n\n代码实现python\ndef train(self, training_set):\n        # 训练分类器\n        class_counts = &#123;&#125;  # 存储每个类别的文档数量\n        for doc, label in training_set:\n            if label not in class_counts:\n                class_counts[label] = 0\n            class_counts[label] += 1\n            for word in doc:\n                if word not in self.vocabulary:\n                    self.vocabulary[word] = &#123;&#39;spam&#39;: 0, &#39;ham&#39;: 0&#125;\n                if label == &#39;0&#39;:  # 假设 &#39;0&#39; 表示垃圾邮件\n                    self.vocabulary[word][&#39;spam&#39;] += 1\n                else:  # 假设 &#39;1&#39; 表示非垃圾邮件\n                    self.vocabulary[word][&#39;ham&#39;] += 1\n        self.class_labels = list(class_counts.keys())  # 获取所有类别标签\n        num_docs = len(training_set)  # 总文档数量\n        for label, count in class_counts.items():\n            self.class_priors[label] = count / num_docs  # 计算先验概率\n        # 计算后验概率\n        word_counts = &#123;label: &#123;&#125; for label in self.class_labels&#125;  # 初始化词频统计\n        for word, counts in self.vocabulary.items():\n            word_counts[&#39;0&#39;][word] = counts[&#39;spam&#39;]\n            word_counts[&#39;1&#39;][word] = counts[&#39;ham&#39;]\n        for label in self.class_labels:\n            total_count = sum(word_counts[label].values())  # 计算总词数\n            self.conditional_prob[label] = &#123;&#125;\n            for word in self.vocabulary:\n                count = word_counts[label].get(word, 0)\n                # 使用拉普拉斯平滑\n                self.conditional_prob[label][word] = (count + self.alpha) / (total_count + self.alpha * len(self.vocabulary))\n        self.save_model(model_path)\n        print(&quot;训练完成.&quot;)预测预测步骤\n\n\n\n\n\nTIP\nSTEP1: 获取训练得到的先验概率。STEP2: 遍历侮辱与非侮辱条件下的每个词，获得其后验概率，并与先验结合得到得分。STEP3: 根据得分结果，对邮件是否为侮辱邮件进行分类。\n\n代码实现pythondef predict(self, docs):\n        # 预测文档的类别\n        predictions = []\n        for doc in docs:     \n            scores = &#123;label: np.log(self.class_priors[label]) for label in self.class_labels&#125;\n            for label in self.class_labels:\n                for word in doc:\n                    if word in self.vocabulary:\n                        scores[label] += np.log(self.conditional_prob[label][word])\n                    else:\n                        # 对于没见过的词，使用拉普拉斯平滑\n                        scores[label] += np.log(self.alpha / (self.alpha * len(self.vocabulary)))\n            # 将对数概率转换为实际概率\n            max_log_score = max(scores.values())\n            exp_scores = &#123;label: np.exp(score - max_log_score) for label, score in scores.items()&#125;\n            total_exp_scores = sum(exp_scores.values())\n            probabilities = &#123;label: exp_score / total_exp_scores for label, exp_score in exp_scores.items()&#125;\n            prediction = max(probabilities, key=scores.get)  # 选择概率最大的类别作为预测结果\n            predictions.append(prediction)\n        return predictions评估评估步骤\n\n\n\n\n\nTIP\nSTEP1: 对测试集进行预测。STEP2: 计算准确率。\n\n代码实现pythondef evaluate(self, test_set):\n        # 评估分类器的性能\n        predictions = self.predict([doc for doc, label in test_set])  # 对测试集进行预测\n        correct = np.sum([label == pred for (doc, label), pred in zip(test_set, predictions)])  # 计算正确预测的数量\n        return correct / len(test_set)  # 返回准确率数据处理工具实现在处理邮件数据时，数据的清洗和预处理至关重要。通过将邮件内容拆分为标签和文本，并进一步拆分为词，能够有效地构建训练集和测试集，为后续的模型训练提供基础\n\n\n\n\n\n\n\nWARNING\n我们使用的数据集：训练集为spam_train.txt，包含5000封邮件（txt 5000行）；测试集为spam_test.txt（txt 1000行），包含1000封邮件，其中侮辱类邮件行首用0标识，非侮辱类邮件行首用1标识。\n\n\n首先，根据提供的数据集格式为.txt，同时可以得到数据集内部数据的格式为label为0表示侮辱邮件，label为1表示非侮辱邮件，Text每个词之间以空格隔开。\n处理步骤因此处理步骤可分为以下4步\n\n\n\n\n\n\nTIP\nSTEP1: 读取数据集的.txt的每一行STEP2: 检索第一个空格，将label和Text拆分，label存入标签列表里。STEP3: 遍历剩余的每个空格，将Text里的词进行拆分，将词存入数据列表里。STEP4: 遍历完每一行之后输出标签列表和数据列表。\n\n注：标签列表存的是数据集每封邮件是否是侮辱邮件的标签，数据列表存的是数据集每封邮件的内容，是以每邮件每词进行拆分的二维列表。\n代码实现python\ndef load_data():\n    try:\n        train_labels, train_data = _split_data_form_txt(train_data_path)\n        test_labels, test_data = _split_data_form_txt(test_data_path)\n    except FileNotFoundError:\n        print(&quot;The data files are not found. Please check the data directory.&quot;)\n        return None, None, None, None\n    return train_labels, train_data, test_labels, test_data\n\n\ndef _split_data_form_txt(data_path):\n    labels = []\n    data = []\n    with open(data_path, &#39;r&#39;, encoding=&#39;utf-8&#39;) as file: # 读取数据\n        lines = file.readlines()\n    for line in lines:\n        split_line = line.strip().split(&quot; &quot;, 1)\n        labels.append(split_line[0])\n        data.append(split_line[1].split())\n    return labels, dataGmail工具实现在访问 Gmail API 时，由于需要设置 OAuth2.0 认证以及代理的使用，我遇到了一些配置上的困难。通过查阅文档和进行调试，最终成功实现了邮件的读取和分类\n\n实现思路首先在Google Cloud上创建项目\n\nDetails\n\n\n并开启Gmail API的调用权限与服务\n\nDetails\n\n\n\n将OAuth2.0客户端ID密钥等详细信息保存到本地pickle文件，并通过这些信息连接到Google Cloud上的项目，开始调用Gmail API，对Gmail邮件进行读取等操作。\n\n客户端打开如图\n\n\n\n部分代码pythonimport pickle\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom google.auth.transport.requests import Request\nfrom email.mime.text import MIMEText\n #如果修改了这些作用域，删除token.pickle文件。\nSCOPES = [&#39;https://www.googleapis.com/auth/gmail.modify&#39;]\n\n# 设置代理 需要设置不然中国大陆无法访问google邮箱\nos.environ[&#39;HTTP_PROXY&#39;] = &#39;http://127.0.0.1:7890&#39;\nos.environ[&#39;HTTPS_PROXY&#39;] = &#39;http://127.0.0.1:7890&#39;\n\ndef get_gmail_service():\n    creds = None\n    # token.pickle文件存储了用户的访问和刷新令牌\n    if os.path.exists(token_path):\n        with open(token_path, &#39;rb&#39;) as token:\n            creds = pickle.load(token)\n            print(&#39;Loaded token&#39;)\n    # 如果没有有效的凭证，让用户登录\n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            creds.refresh(Request())\n            print(&#39;Refreshed token&#39;)\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file(\n                json_path, SCOPES)\n            creds = flow.run_local_server(port=12345)\n            print(&#39;New token&#39;)\n        # 保存凭证以供下次运行使用\n        with open(token_path, &#39;wb&#39;) as token:\n            pickle.dump(creds, token)\n\ndef read_messages(service, user_id=&#39;me&#39;, label_ids=[&#39;INBOX&#39;], max_results=10, is_print=False):\n    try:\n        msg_list = []\n        messages = service.users().messages().list(userId=user_id, \n                                                   labelIds=label_ids, \n                                                   maxResults=max_results).execute()\n        \n        for message in messages[&#39;messages&#39;]:\n            msg = service.users().messages().get(userId=user_id, id=message[&#39;id&#39;]).execute()\n            if is_print:\n                print(f&quot;Subject: &#123;_get_subject(msg)&#125;&quot;)\n                print(f&quot;From: &#123;_get_sender(msg)&#125;&quot;)\n                print(f&quot;Snippet: &#123;msg[&#39;snippet&#39;]&#125;\\n&quot;)\n            msg_list.append(msg)\n        return msg_list\n    except Exception as error:\n        print(f&#39;An error occurred: &#123;error&#125;&#39;)\n\ndef send_message(service, user_id, subject, body, to):\n    message = MIMEText(body)\n    message[&#39;to&#39;] = to\n    message[&#39;subject&#39;] = subject\n    raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode(&#39;utf-8&#39;)\n    \n    try:\n        message = service.users().messages().send(userId=user_id, body=&#123;&#39;raw&#39;: raw_message&#125;).execute()\n        print(f&quot;Message Id: &#123;message[&#39;id&#39;]&#125;&quot;)\n        return message\n    except Exception as error:\n        print(f&#39;An error occurred: &#123;error&#125;&#39;)\n        return None测试结果测试集准确率运行train.py脚本可以得到测试集的准确率为 97.4% 。说明该朴素贝叶斯分类器在测试集上的分类表现良好，能够正确分类大多数测试样本。\n实际邮件的预测效果手动输入文本\n\nDetails\n\n\n\n先验概率是从模型计算分数并进行归一化得到的。该朴素贝叶斯分类器能简单的根据邮件的内容（例如“love”“stupid”之类的词）进行侮辱邮件的分类。简单邮件的分类表现不错。\n从Gmail获取最近的一封邮件\nDetails\n\n\n运行test.py脚本能有效地读出测试gmail邮箱中邮件的信息并进行是否为侮辱邮件的简单判断。但是实际效果还是不太好，可能是由于数据集数据量或数据质量受限。\n心得与体会总结\n\n\n\n\n\n\nWARNING\n在本次项目中，我深入学习和应用了朴素贝叶斯算法进行邮件分类，主要针对侮辱性和非侮辱性邮件的识别。这一过程让我对贝叶斯的基本原理有了更深刻的理解，尤其是在文本分类方面的应用。在未来的学习中，我希望能继续深化对机器学习算法的理解，并尝试更复杂的模型和数据集。\n\n\n改进方向\n\n\n\n\n\n\nWARNING\n在实际应用中，数据集的质量直接影响到模型的效果。虽然使用了5000封邮件进行训练，但数据的多样性和标签的准确性仍需要关注。在特征提取方面，可以使用一些技巧去提取关键词，而不是将所有词都作为词库，这样在关键词上的处理可以提高模型的表现。在之后的学习可以学到更多的机器学习算法，可以尝试使用它们来对比朴素贝叶斯算法的效果，看看是否能进一步提高分类准确率。\n\n","slug":"朴素贝叶斯-邮件分类编程实践","date":"2024-10-06T14:34:15.000Z","categories_index":"MashineLearning","tags_index":"python","author_index":"PIKO"},{"id":"5ba4847e568bd703f1becbdbb108530a","title":"python环境搭建","content":"前言作为工科生，环境搭建是必不可少的一种能力，学会环境搭建是小白步向菜鸟的第一步。python是一门相对较新的语言，我记得在我读中学的时候这个时候python还没出3，当时懵懵懂懂的，听网上的言论，感觉python是一个和basic、scratch一样的新手入门级编程语言，但实际上python是一个很好用的编程语言，突出一个生态多库多能实现的功能很多涉及的领域也很多，像是网络爬虫、神经网络、软件自动化甚至是硬件操控都可以，感兴趣的话可以去多多了解一下，话不多说，我们准备开始了。\n\n方案\n\n\n\n\n\n\n方案一：Anaconda + Visual Studio Code\nAnaconda是一个python多功能工具箱\n就像手机的应用商店，Anaconda提供了一个集成的环境，可以轻松安装、更新和管理各种Python库和工具。你只需通过简单的命令，就能获取你需要的应用（库），让开发和分析变得简单。\nVisual Studio Code是一个代码编辑器（之后简称Vscode）\nVSCode就像一把瑞士军刀，集成了多种工具和功能。无论你需要剪刀、螺丝刀还是开瓶器，VSCode都能满足你的需求。它支持多种编程语言、调试工具和插件，让你在同一个环境中完成各种任务。\n\n\n方案二pycharm之后有机会再做吧\nAnaconda配置windows端Anaconda安装\n\n\n\n\n\nAnaconda官网\nAnaconda官网\n\n\nSTEP1 点击这个 skip registration 跳过注册\n\nSTEP2 进入下载界面点击Downlaod\n\n\n\n\n\n\n\n\n注意\n\nAnaconda占用的存储空间比较大，尽量不要下到C盘，如果分了很多个盘的话。不然你的C盘很快就要红温了。\n需要记住Anaconda的安装路径，之后步骤需要使用\n\n\n\nSTEP3 正常下载完成会让你打开Anaconda Navigator，暂时不用管它，右上角关闭，我们这个教程不会使用到，感兴趣我之后可以再做。\n\n至此Anaconda下载完毕！\n环境变量配置安装完毕后我们要开始配置环境变量了\n\n\n\n\n\n\n环境变量是个啥\n当你在厨房做饭时，你需要各种调料（盐、糖、酱油等）。这些调料放在一个固定的地方（调料架）上，方便你随时取用。环境变量就像是这个调料架，存放着各种系统和应用程序需要的配置信息，方便程序随时读取。\n\n\nSTEP1 找到你windows的开始键，按下进入搜索，搜索环境变量，点击“编辑系统环境变量”\n\nSTEP2 先按下“path”，再点击“编辑”，进入配置环境变量路径的列表\n\n\n\n\n\n\n\nTIP\n配置环境变量路径的列表如下图所示\n\n双击已有的就可以修改已有的环境变量，双击空白列表处就可以添加新的环境变量\n\n\nSTEP3 需要去Anaconda的安装路径下找到图中的三个文件夹，并将它们加入环境变量路径列表中\n\n至此环境变量配置完毕！\n创建环境STEP1 打开Anaconda Prompt\n\n\n\n\n\n\n\n打开的命令行界面如图所示\n\n(base) 代表的是你当前所处的虚拟环境，base是基础虚拟环境，是下载anaconda后的默认的虚拟环境。\n\n\nSTEP2 创建环境\n在Prompt命令行界面输入如下命令\nbashconda create -n Env_Name python=3.9 \n\n\n\n\n\n\nconda create 参数解析\n-n 代表取名 Env_Name 可以换成任意你喜欢的名字，但是得是英文哦python&#x3D;3.9 表示该环境预装python3.9\n\n\n之后命令行打印这个界面按下y就会继续安装\n\n命令行打印这些信息就是安装成功啦\n\nSTEP3 启动环境\n在Prompt命令行界面输入如下命令\nbashconda activate Env_Name #注意替换Env_Name忘了自己刚刚取的环境名字可以输入以下命令查看\nbashconda env list #查看Anaconda的所有环境当前路径前面的(base)变成了(Env_Name)就是成功启动环境了\n\n\n\n\n\n\n\nconda activate &amp; conda deactivate\nconda activate 表示激活环境 后面接你的环境名conda deactivate 表示退出环境 后面接你的环境名\n\n\n至此创建环境完毕！\nMac端Anaconda安装\n\n\n\n\n\nAnaconda官网\nAnaconda官网\n\n\nSTEP1 去Anaconda官网下载注意你的Mac是M系列芯片还是Intel芯片\n\nSTEP2 “Download”里打开安装包\n\nSTEP3 一路点击继续，等待安装结束\n\n\nClick to see more\n整个安装流程是往你的用户路径下的.zshrc和.bash_profile文件里写conda的环境变量，可以通过以下命令去终端查看究竟往这两个用户配置文件里面写了什么。\nbashnano ~/.zshrc # 查看.zshrc的内容\nnano ~/.bash_profile # 查看.bash_profile的内容环境变量大概如图所示\n\n所以使用安装包安装anaconda不需要自己配置anaconda的环境变量，但如果以后要移动anaconda文件夹的位置就需要去重写环境变量\n\n\nSTEP3 打开你的终端，如果工作目录前面出现(base)即是安装成功\n\n至此Anaconda安装完毕！\n创建环境操作与windows端一致，在终端进行即可。\nLinux端有时间我再加上，在这里先插个眼\nVscode配置\nClick to see more\nVscode在Mac、Windows、Linux下都差不多我就不分着写了。\n\nVscode下载\n\n\n\n\n\nVscode官网\nVscode官网\n\n\n扩展推荐\nClick to see more\n\npython相关扩展没了你别想用Vscode开发python程序\nC&#x2F;C++相关扩展没了你别想用Vscode开发C&#x2F;C++程序\nChinese好好想想为什么你需要这个插件\n改变图标真看不惯vscode的文件图标显示吧\n代码补全代码补完计划\n书签给你的bug留个存档点\n\n\n\n尝试HelloworldSTEP1 创建Helloworld.py文件，并在工作区打开\n\nSTEP2 Vscode右下角点击“选择python解释器”\n\nSTEP3 选择你在Anaconda中创建的虚拟环境\n\nSTEP4 Hello,World!在Helloworld.py文件中添加如下代码\npythonprint(&quot;Hello, World!&quot;) # 你好，世界！至此Vscode配置完毕！\n之后这个工作区的python开发就是使用的Anaconda里你创建的虚拟环境安装库什么的就可以在Vscode的终端里进行安装了\n\n\n\n\n\n\n\n提一嘴\npython库安装\n使用pip工具在终端进行库安装（以安装numpy为例）\nbashpip install numpy千万要注意你是在哪个环境进行安装的，注意前头的 ( )\n\n\n后记恭喜你，已经成功从环境搭建小白进化成菜鸟了！\n\n","slug":"python环境搭建","date":"2024-09-09T14:55:02.000Z","categories_index":"","tags_index":"python","author_index":"PIKO"},{"id":"46d63a6191ce7c7d772a0f14338f0d34","title":"三子棋对弈装置--视觉部分","content":"前言本篇blog是本人在打2024电赛E题学习到的相关知识的整理，由于我们选择的方案是深度数据和RGB数据融合的方法来判断棋子的位置。不同于大多数人选择的openmv作为视觉传感，所以我觉得在实现思路上会相对不那么随大流一点，当然我也会提供我自己个人的传统RGB 2DV的思路。在此，我仅会提供部分代码以供参考。\n\n自我吐槽\n由于当时全心全意在攒这个代码，没去做照片的记录，所以以下很多的代码没有效果图的作证会有点难受，现在真的有点想穿越到那个时候给自己来一巴掌\n\n\n硬件组成\n\n\n\n\n\n \n\nprocessor: jetson nano 4G\ni-TOF camera: orbbec Femto Bolt\n\n\n\n我们使用 jetson nano 作为处理器，读取深度相机 Femto Bolt 的视频流并作相应的图像处理，最终将识别出的棋子和棋盘坐标提取出来，通过串口传到下位机MCU进行控制。\n\n\n软件部分在 jetson nano 上运行的程序\n数据流获取应用奥比中光的orbbecSDK去采深度相机的数据。orbbecSDK的环境配置不过多赘述，不是本篇的重点，如果读者感兴趣，我未来或许会再做。\n不同厂商的相机SDK肯定是不一样的，这里仅作奥比中光相机通过orbbecSDK的数据流获取思路和代码实现(奥比中光相机的SDK和微软realsense相机的SDK是比较相像的应该可以有所借鉴)\n获取思路\n\n\n\n\n\n\nWARNING\n获取视频流管道类Pipeline和视频流配置类Config。获取对RGB和Depth流的配置并使能，主要可以调帧率和分辨率，如果有需要做RGBD对齐这里也可以做。将管道的数据输出成Frameset类的帧格式，Frameset类有很多帧相关的方法。将每一帧Frameset帧数据给转换成numpy数组便可以通过opencv再做处理。\n\n代码实现python\nfrom pyorbbecsdk import Config \nfrom pyorbbecsdk import OBError\nfrom pyorbbecsdk import OBSensorType, OBFormat\nfrom pyorbbecsdk import Pipeline, FrameSet\nfrom pyorbbecsdk import VideoStreamProfile\nfrom utils import frame_to_bgr_image\nfrom pyorbbecsdk import *\n\n#相机初始化\ndef camera_init():\n    pipeline = Pipeline()\n    config = Config()\n    return pipeline, config\n\n# 配置RGB流\ndef config_color_stream(pipeline,config):\n    \n    try:\n        profile_list = pipeline.get_stream_profile_list(OBSensorType.COLOR_SENSOR)\n        try:\n            color_profile: VideoStreamProfile = profile_list.get_stream_profile_by_index(1)\n        except OBError as e:\n            print(e)\n            color_profile = profile_list.get_default_video_stream_profile()\n            print(&quot;color profile: &quot;, color_profile)\n        config.enable_stream(color_profile)\n    except Exception as e:\n        print(e)\n        return config\n\n# 配置深度流\ndef config_depth_stream(pipeline,config):\n    try:\n        profile_list = pipeline.get_stream_profile_list(OBSensorType.DEPTH_SENSOR)\n        try:\n            depth_profile = profile_list.get_stream_profile_by_index(3)\n        except OBError as e:\n            print(e)\n            depth_profile = profile_list.get_default_video_stream_profile()\n            print(&quot;depth profile: &quot;, depth_profile)\n        config.enable_stream(depth_profile)\n    except Exception as e:\n        print(e)\n        return config\n\n# 配置RGBD对齐\ndef config_color_depth_align(pipeline,config):\n    pipeline.enable_frame_sync()\n    config.set_align_mode(OBAlignMode.SW_MODE)\n\n# 读颜色深度帧\ndef read_color_depth_frame(pipeline):\n    frame = pipeline.wait_for_frames(100)\n    if frame is None:\n        return None, None\n    color_frame = frame.get_color_frame()\n    if color_frame is None:\n        return None, None\n    depth_frame = frame.get_depth_frame()\n    if depth_frame is None:\n        return None, None\n    \n    width = depth_frame.get_width()\n    height = depth_frame.get_height()\n    scale = depth_frame.get_depth_scale()\n\n    depth_data = np.frombuffer(depth_frame.get_data(), dtype=np.uint16)\n    depth_data = depth_data.reshape((height, width))\n\n    depth_data = depth_data.astype(np.float32) * scale\n    depth_data = np.where((depth_data &gt; MIN_DEPTH) &amp; (depth_data &lt; MAX_DEPTH), depth_data, 0)\n    depth_data = depth_data.astype(np.uint16)\n\n    depth_image = cv2.normalize(depth_data, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n    depth_image = cv2.applyColorMap(depth_image, cv2.COLORMAP_JET)\n    color_image = frame_to_bgr_image(color_frame)\n    print(depth_image.shape,color_image.shape)\n    return color_image, depth_image\n\n# 读颜色帧\ndef read_color_frame(pipeline):\n    frame = pipeline.wait_for_frames(100)\n    if frame is None:\n        return None\n    color_frame = frame.get_color_frame()\n    if color_frame is None:\n        return None\n    color_image = frame_to_bgr_image(color_frame)\n    return color_image\n\n# 读深度帧\ndef read_depth_frame(pipeline):\n    frame = pipeline.wait_for_frames(100)\n    if frame is None:\n        return None\n    depth_frame = frame.get_depth_frame()\n    if depth_frame is None:\n        return None\n    \n    width = depth_frame.get_width()\n    height = depth_frame.get_height()\n    scale = depth_frame.get_depth_scale()\n\n    depth_data = np.frombuffer(depth_frame.get_data(), dtype=np.uint16)\n    depth_data = depth_data.reshape((height, width))\n\n    depth_data = depth_data.astype(np.float32) * scale\n    depth_data = np.where((depth_data &gt; MIN_DEPTH) &amp; (depth_data &lt; MAX_DEPTH), depth_data, 0)\n    depth_data = depth_data.astype(np.uint16)\n\n# 读深度颜色帧\ndef read_color_depth_frame(pipeline):\n    frame = pipeline.wait_for_frames(100)\n    if frame is None:\n        return None, None\n    color_frame = frame.get_color_frame()\n    if color_frame is None:\n        return None, None\n    depth_frame = frame.get_depth_frame()\n    if depth_frame is None:\n        return None, None\n    \n    width = depth_frame.get_width()\n    height = depth_frame.get_height()\n    scale = depth_frame.get_depth_scale()\n\n    depth_data = np.frombuffer(depth_frame.get_data(), dtype=np.uint16)\n    depth_data = depth_data.reshape((height, width))\n\n    depth_data = depth_data.astype(np.float32) * scale\n    depth_data = np.where((depth_data &gt; MIN_DEPTH) &amp; (depth_data &lt; MAX_DEPTH), depth_data, 0)\n    depth_data = depth_data.astype(np.uint16)\n    color_image = frame_to_bgr_image(color_frame)\n    return color_image,depth_data相机标定\nWhat is the camera celibration ?\n相机标定（Camera Calibration）是计算机视觉中的一个重要步骤，旨在确定相机的内参和外参。内参包括焦距、光学中心和畸变系数，外参包括相机在世界坐标系中的位置和方向。通过相机标定，可以将二维图像中的点映射到三维空间中的点，从而实现精确的测量和重建。\n相机标定的主要步骤包括：\n\n拍摄标定图像：通常使用棋盘格图案，拍摄多张不同角度的图像。\n检测角点：在每张图像中检测棋盘格的角点。\n计算内参和外参：使用检测到的角点和已知的棋盘格尺寸，通过算法（如张正友标定法）计算相机的内参和外参。\n校正图像：使用计算出的参数校正图像中的畸变。:::\n\n标定思路:::warning我们采用的是固定相机机位的方法，如果想要将我们的运动机构和相机传感做联系是要做手眼标定的。因为我们使用的运动机构是龙门架，由于龙门架自身的特性，我们只需要将相机标定到我们的棋盘平面即可得到比较精准的结果，龙门架的初始点只需要做简单的平移就可以做到与相机标定平面的对应。我们使用张正友标定法将相机标定到棋盘平面之上，使用 11x8 grid size 20mm的棋盘格标定板进行标定。\n对张正友标定法感兴趣的读者可以去看这篇论文《A Flexible New Technique for Camera Calibration》\n\n这里提供一个脚本做相机标定\n\nA script for camera celibration.\npython\nimport cv2\nimport numpy as np\nimport os\n\nscript_dir = os.path.dirname(os.path.abspath(__file__)) #&lt;-- absolute dir the script is in\n# 图片在当前文件夹的位置\nfile_in = os.path.join(script_dir,&#39;p1&#39;)   # 原始图片存放位置\nfile_out = os.path.join(script_dir,&#39;p3&#39;)   # 最后图片的保存位置\n\n# 棋盘格模板规格，只算内角点个数，不算最外面的一圈点\nw = 11\nh = 8\n\n# 没考虑棋盘格的大小，只考虑角点的数量，因此计算出来的平移量要乘上棋盘格的大小\n\n# 找棋盘格角点\n# 世界坐标系中的棋盘格点，在张正友标定法中认为Z = 0\n# mgrid创建了大小为8×5×2的三维矩阵，在reshape成二维以后赋给objp，objp最后为(0,0,0), (1,0,0), (2,0,0) ....,(8,5,0)\nobjp = np.zeros((w * h, 3), np.float32)   # 大小为wh×3的0矩阵\nobjp[:, :2] = np.mgrid[0:w, 0:h].T.reshape(-1, 2)   # :2是因为认为Z=0\nobjpoints = []  # 储存在世界坐标系中的三维点\nimgpoints = []  # 储存在图像平面的二维点\n\nimages = os.listdir(file_in)   # 读入图像序列\ni = 0\nimg_h = 0\nimg_w = 0\n\n# 算法迭代的终止条件，第一项表示迭代次数达到最大次数时停止迭代，第二项表示角点位置变化的最小值已经达到最小时停止迭代\ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\nfor fname in images:\n    img = cv2.imread(file_in + &#39;/&#39; + fname)\n    img_h = np.size(img, 0)\n    img_w = np.size(img, 1)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   # RGB转灰度\n    # 找到棋盘格角点，存放角点于corners，如果找到足够点对，将其存储起来，ret为非零值\n    ret, corners = cv2.findChessboardCorners(gray, (w, h), None)\n    # 检测到角点后，进行亚像素级别角点检测，更新角点\n    if ret == True:\n        i += 1\n        # 输入图像gray；角点初始坐标corners；搜索窗口为2*winsize+1；表示窗口的最小（-1.-1）表示忽略；求角点的迭代终止条件\n        cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)\n        objpoints.append(objp)   # 空间坐标\n        imgpoints.append(corners)  # 角点坐标即图像坐标\n        # 角点显示\n        cv2.drawChessboardCorners(img, (w, h), corners, ret)\n        cv2.imshow(&#39;findCorners&#39;, img)\n        cv2.imwrite(file_out + &#39;/print_corners&#39; + str(i) + &#39;.jpg&#39;, img)\n        cv2.waitKey(0)\ncv2.destroyAllWindows()\n\n&quot;&quot;&quot;\n求解参数\n输入：世界坐标系里的位置；像素坐标；图像的像素尺寸大小；\n输出：\nret: 重投影误差；\nmtx：内参矩阵；\ndist：畸变系数；\nrvecs：旋转向量 （外参数）；\ntvecs：平移向量 （外参数）；\n&quot;&quot;&quot;\nret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n\nprint((&quot;ret（重投影误差）:&quot;), ret)\nprint((&quot;mtx（内参矩阵）:\\n&quot;), mtx)\nprint((&quot;dist（畸变参数）:\\n&quot;), dist)  # 5个畸变参数，(k_1,k_2,p_1,p_2,k_3)\nprint((&quot;rvecs（旋转向量）:\\n&quot;), rvecs)\nprint((&quot;tvecs（平移向量）:\\n&quot;), tvecs)\n\n\n# 优化内参数和畸变系数\n# 使用相机内参mtx和畸变系数dist，并使用cv.getOptimalNewCameraMatrix()\n# 通过设定自由自由比例因子alpha。\n# 当alpha设为0的时候，将会返回一个剪裁过的将去畸变后不想要的像素去掉的内参数和畸变系数；\n# 当alpha设为1的时候，将会返回一系个包含额外黑色像素点的内参数和畸变数，并返回一个ROI用于将其剪裁掉。\nnewcameramtx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (img_w, img_h), 0, (img_w, img_h))\n\n\n# 矫正畸变\nimg2 = cv2.imread(file_in + &#39;\\\\img_0.jpg&#39;) # 第几张图片\ndst = cv2.undistort(img2, mtx, dist, None, newcameramtx)\ncv2.imwrite(file_out + &#39;/calibresult.jpg&#39;, dst)\nprint(&quot;newcameramtx（优化后相机内参）:\\n&quot;, newcameramtx)\n\n# 反投影误差total_error,越接近0，说明结果越理想。\ntotal_error = 0\nfor i in range(len(objpoints)):\n    imgpoints2, _ = cv2.projectPoints(objpoints[i], rvecs[i], tvecs[i], mtx, dist)   # 计算三维点到二维图像的投影\n    error = cv2.norm(imgpoints[i], imgpoints2, cv2.NORM_L2) / len(imgpoints2)   # 反投影得到的点与图像上检测到的点的误差\n    total_error += error\nprint((&quot;total error: &quot;), total_error / len(objpoints))   # 记平均\n\n# 将旋转向量转换为旋转矩阵\nR, _ = cv2.Rodrigues(rvecs[0])\n# 构建外参矩阵\nextrinsic_matrix = np.hstack((R, tvecs[0]))\nprint(&quot;外参矩阵：&quot;)\nprint(extrinsic_matrix)\n\n坐标系映射做完相机的内外参标定之后就可以通过深度数据、相机内参K和相机外参E去建立标定的世界坐标系和我们相机的像素坐标系之间的映射关系。\n原理公式\n\n\n\n\n\n\n!!!\n捏嘿^o^待补充~~~~\n\n\n代码实现python\n# 世界坐标系到像素坐标系的投影\ndef project_world_to_pixel(point_3d):\n    # 将3D点转换为齐次坐标\n    point_world = np.vstack((point_3d.reshape(3, 1), 1))\n    # 将3D点从世界坐标系转换到相机坐标系\n    point_camera = Rt @ point_world\n    # 求出齐次坐标下的像素坐标\n    point_pixel_homogeneous = K @ point_camera[:3]\n    # 归一化像素坐标\n    point_pixel = point_pixel_homogeneous / point_pixel_homogeneous[2]\n    return point_pixel[:2]\n\n# 像素坐标系到世界坐标系的投影\ndef project_pixel_to_world(point_pixel, depth=t[2]):\n    Rt_extended = np.vstack((Rt, [0, 0, 0, 1]))\n    point_pixel_homogeneous = np.vstack((point_pixel, 1))\n    point_camera = np.linalg.inv(K) @ point_pixel_homogeneous\n    point_camera = point_camera * depth\n    point_camera_homogeneous = np.vstack((point_camera, 1))\n    point_world_homogeneous = np.linalg.inv(Rt_extended) @ point_camera_homogeneous\n    point_world = point_world_homogeneous[:3] / point_world_homogeneous[3]\n    return point_world棋盘检测由于比赛的时间关系，我采用的是传统2D视觉图像处理的方法来检测棋盘。\n检测思路\n\n\n\n\n\n\nWARNING\n采用传统轮廓检测算法，首先进行canny边缘提取，通过设置相应的阈值实现将棋盘的边缘进行提取，再执行膨胀操作，将内外边缘合一，通过设置面积特征、形状特征和边长特征的提取，将我对棋盘的ROI区域画出来，完成对棋盘外轮廓的确定。再通过棋盘三等分的几何特性，我将九个格子的十六个格点分别进行提取，再根据这十六个格点相邻之间的像素坐标关系，计算出各个格子的中心点的像素坐标值，由此完成对棋盘的识别。\n\n\n\n\n\n\n\nMY VIEW\n优点：图像处理算法相对简单运行起来帧率高。缺陷：需要固定相机位，如果相机位置改变则需要改很多参数。\n\n\n代码实现python\n# 找棋盘格\ndef find_chessboard(color_image, depth_image=None):\n    find_center_method = 1\n    pixels_threshold = 80\n    area_threshold = 50\n    appo_contour = None\n    grid_size = None\n    sorted_center_list = [] # 排序后的中心点像素坐标列表\n\n    # 转换为灰度图像\n    gray_image = cv2.cvtColor(color_image, cv2.COLOR_RGB2GRAY)\n    # 高斯模糊\n    # blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n    # 边缘检测\n    edged = cv2.Canny(gray_image, 75, 150)\n    \n    # 膨胀处理\n    kernel = np.ones((5, 5), np.uint8)\n    dilated = cv2.dilate(edged, kernel, iterations=1)\n    # cv2.imshow(&quot;dilated&quot;,dilated)\n    # 选取ROI区域\n    dilated_ROI = dilated[10:700,100:900]\n    # ROI与原图像对齐\n    dilated_ROI = extend_frame_boarden(dilated_ROI)\n    cv2.imshow(&quot;dilated_ROI&quot;,dilated_ROI)\n    # 查找轮廓\n    contours, _ = cv2.findContours(dilated_ROI, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n    if len(contours) &gt; 0:\n        # 筛选出最大的轮廓\n        largest_contour = contours[:2]\n        for contour in largest_contour:\n            area = cv2.contourArea(contour)\n            \n            # 近似多边形\n            epsilon = 0.02 * cv2.arcLength(contour, True)\n            approx = cv2.approxPolyDP(contour, epsilon, True)\n            # print(area,approx)\n            if len(approx) == 4:\n               if area &gt;=30000 and area &lt;= 50000 :\n                appo_contour = contour\n                    # corners = approx.reshape((4, 2))\n                break\n\n        if appo_contour is not None:\n            corners = approx.reshape((4, 2))\n            tl = corners[0]\n            bl = corners[1]\n            br = corners[2]\n            tr = corners[3]\n            # 计算3x3格子的交叉点\n            cross_points = []\n            for i in range(4):\n                for j in range(4):\n                    # 线性插值计算交叉点\n                    cross_x = int((tl[0] * (3 - i) + tr[0] * i) * (3 - j) / 9 +\n                                (bl[0] * (3 - i) + br[0] * i) * j / 9)\n                    cross_y = int((tl[1] * (3 - i) + tr[1] * i) * (3 - j) / 9 +\n                                (bl[1] * (3 - i) + br[1] * i) * j / 9)\n                    cross_points.append((cross_x, cross_y))\n                    cv2.circle(color_image, (cross_x, cross_y), 3, (0, 255, 0), -1)\n\n            distances = []\n            # 计算每行相邻交叉点的距离\n            for i in range(4):\n                for j in range(3):\n                    p1 = cross_points[i * 4 + j]\n                    p2 = cross_points[i * 4 + j + 1]\n                    distance = np.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)\n                    distances.append(distance)\n            # 计算每列相邻交叉点的距离\n            for j in range(4):\n                for i in range(3):\n                    p1 = cross_points[i * 4 + j]\n                    p2 = cross_points[(i + 1) * 4 + j]\n                    distance = np.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)\n                    distances.append(distance)\n            grid_size = np.mean(distances)\n            # print(&quot;grid_size is&quot;, grid_size)\n            centers = []\n            \n            # 找格子中心点方法一：直接根据顶点计算\n            #if find_center_method == 1:\n            for i in range(3):\n                for j in range(3):\n                    center_x = int((cross_points[i * 4 + j][0] + cross_points[i * 4 + j + 1][0] + cross_points[(i + 1) * 4 + j][0] + cross_points[(i + 1) * 4 + j + 1][0]) / 4)\n                    center_y = int((cross_points[i * 4 + j][1] + cross_points[i * 4 + j + 1][1] + cross_points[(i + 1) * 4 + j][1] + cross_points[(i + 1) * 4 + j + 1][1]) / 4)\n                    centers.append((center_x, center_y))\n                    cv2.circle(color_image, (center_x, center_y), 2, (0, 255, 0), -1)\n            # elif find_center_method == 2:\n            # 对找到的中心点进行编号, y + x 最大就是右下角，最小就是左上角， y-x 最大就是左下角，y-x 最小就是右上角，其它几个点根据在旁边两个点中间判断\n            if len(centers) == 9:\n                centers = np.array(centers)\n                rect = np.zeros((9, 2), dtype=&quot;float32&quot;)\n                dist = np.zeros(9, dtype=&quot;float32&quot;)\n                s = centers.sum(axis=1)\n                idx_0 = np.argmin(s)\n                idx_8 = np.argmax(s)\n                diff = np.diff(centers, axis=1)\n                idx_2 = np.argmin(diff)\n                idx_6 = np.argmax(diff)\n                rect[0] = centers[idx_0]\n                rect[2] = centers[idx_2]\n                rect[6] = centers[idx_6]\n                rect[8] = centers[idx_8]\n                #   其它点\n                calc_center = (rect[0] + rect[2] + rect[6] + rect[8]) / 4\n                mask = np.zeros(centers.shape[0], dtype=bool)\n                idxes = [1, 3, 4, 5, 7]\n                mask[idxes] = True    \n                # 筛选出其他中心点\n                others = centers[mask]\n                # 找到最左、最右、最上和最下的点的索引\n                idx_l = others[:,0].argmin()\n                idx_r = others[:,0].argmax()\n                idx_t = others[:,1].argmin()\n                idx_b = others[:,1].argmax()\n                found = np.array([idx_l, idx_r, idx_t, idx_b])\n                mask = np.isin(range(len(others)), found, invert=False)\n                idx_c = np.where(mask == False)[0]\n                if len(idx_c) == 1:\n                    rect[1] = others[idx_t]\n                    rect[3] = others[idx_l]\n                    rect[4] = others[idx_c]\n                    rect[5] = others[idx_r]\n                    rect[7] = others[idx_b]\n                    # 写编号\n                    for i in range(9):\n                                                                        \n                        # print(f&quot;&#123;i&#125;格的中心点&quot;,rect[i][0],rect[i][1])\n                        cv2.putText(color_image, str(i+1), (int(rect[i][0]), int(rect[i][1])), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n                        sorted_center_list.append((int(rect[i][0]), int(rect[i][1])))\n                else:\n                    # 大于 45度的情况\n                    print(&quot;&gt; 45 degree&quot;) \n        else:\n            print(&quot;without appopriate contour&quot;)\n    return sorted_center_list if &#39;sorted_center_list&#39; in locals() else [], grid_size if &#39;grid_size&#39; in locals() else None棋子检测这里提供两种方法\n3DV方法3D视觉方法\n实现思路\n\n\n\n\n\n\nWARNING\n采用二值化+3D点云数据融合分辨黑色棋子和白色棋子。首先，通过对图像滤波再做二值化处理就可以得到黑色棋子的完美轮廓，通过棋子形状和面积等方法去提取出黑色棋子的中心坐标点。再通过对相机做depth和RGB的数据对齐，可以使得到的点云数据和RGB数据有一个对应关系，将点云数据提取出来之后，由于棋盘是一个光滑平面，可以通过ransac拟合平面，再通过点云处理将这个平面给隐藏，再通过 DBSCAN 聚类提取出每个棋子的点云，选取棋子点云中心点作为棋子的相机坐标系坐标，再将每个棋子的相机坐标系坐标映射到像素坐标系上，便可以得到棋子的像素坐标中心点，再将之前二值化得到的黑色棋子的中心坐标点剔除，剩下的就是白色棋子中心点像素坐标。由此，我们成功地完成了对棋子的识别。\n\n\n\n\n\n\n\nMY VIEW\n优点：识别基本不受环境光线的干扰，通过三维的信息来识别棋子，你放张棋子照片上去也不会误识别。缺点：点云数据量大，即使做了降采样和裁切之后对算力的要求仍旧很高，帧率很低，体验3帧电竞，对一些实时性有要求的场景不太适合。\n\n\n代码实现（仅代码块）python\n# 拟合平面ransac \ndef fit_plane(points):\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(points)\n    plane_model, inliers = pcd.segment_plane(distance_threshold=0.27,\n                                             ransac_n=3,\n                                             num_iterations=1000)\n    return plane_model, inliers\n\ndef detect_objects_on_plane(points, distance_threshold=5):\n    # 拟合平面模型，返回平面模型参数和内点索引\n    plane_model, inliers = fit_plane(points)\n    # 解包平面模型参数 a, b, c, d\n    [a, b, c, d] = plane_model\n    # 计算每个点到平面的距离\n    distances = np.abs(a * points[:, 0] + b * points[:, 1] + c * points[:, 2] + d) / np.sqrt(a**2 + b**2 + c**2)\n    # 找到距离大于阈值的点的索引\n    object_indices = np.where(distances &gt; distance_threshold)[0]\n    # 提取这些点作为检测到的物体\n    objects = points[object_indices]\n    objects_pcd = o3d.geometry.PointCloud()\n    objects_pcd.points = o3d.utility.Vector3dVector(objects)\n    cl, ind = objects_pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n    filtered_pcd = objects_pcd.select_by_index(ind)\n    # 返回检测到的物体点云 \n    return objects_pcd\n\n\n# 使用 DBSCAN 聚类算法提取不同物体\ndef extract_objects_from_pcd(objects_pcd, eps=1, min_points=10):\n    # 进行 DBSCAN 聚类\n    labels = np.array(objects_pcd.cluster_dbscan(eps=eps, min_points=min_points, print_progress=True))\n    # 获取聚类数目\n    max_label = labels.max()\n    print(f&quot;point cloud has &#123;max_label + 1&#125; clusters&quot;)\n    # 将不同聚类的点云提取出来\n    clusters = []\n    for i in range(max_label + 1):\n        cluster_indices = np.where(labels == i)[0]\n        cluster_pcd = objects_pcd.select_by_index(cluster_indices)\n        clusters.append(cluster_pcd)\n    return clusters\n\npcd = convert_to_o3d_point_cloud(np.array(points))\n# 降采样\ndownsampled_pcd = pcd.voxel_down_sample(voxel_size=0.01)\n# 裁剪点云\n# 定义包围盒的最小和最大边界点\n# 创建轴对齐包围盒\naabb = pcd.get_axis_aligned_bounding_box()\nmin_bound = aabb.min_bound\nmax_bound = aabb.max_bound\naabb1 = o3d.geometry.AxisAlignedBoundingBox([min_bound[0]+250,min_bound[1]+200,min_bound[2]],\n                                                [max_bound[0]-250,max_bound[1]-200,max_bound[2]])\ncropped_pcd = downsampled_pcd.crop(aabb1)\n# 将 Open3D 点云对象转换为 numpy 数组\npoints_np = np.asarray(cropped_pcd.points)\nobjects_pcd = detect_objects_on_plane(points_np)\n# 提取不同物体的点云\ncluster_pcd_list = extract_objects_from_pcd(objects_pcd)\n# 可视化处理后的点云\n# 创建坐标系对象\ncoordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=100, origin=[0, 0, 0])\no3d.visualization.draw_geometries([coordinate_frame, cluster_pcd_list[0]])2DV方法传统2D视觉方法\n实现思路\n\n\n\n\n\n\nWARNING\n采用背景减除+不同色域提取掩膜将黑色棋子和白色棋子在红色背景下提取出来。首先，采用固定机位的相机因此可以相对准确的标记出大致的ROI区域。根据环境预先获取没有棋子的相机RGB图，在处理时对相机的帧数据上与预先的背景RGB图的每个通道做绝对求差，比较两个图像的像素值，提取出前景对象。这样可以很好的将棋子的轮廓和背景之间进行分离。根据分离出来的颜色的对比程度，可以简易判断做掩膜提取效果较好的空间色域。将分离出来的前景图像在RGB空间中做色域提取，通过设置RGB颜色阈值可以将黑色棋子很容易的提取出来，均值滤波后通过K_Means聚类算法将轮廓聚集，二值化前景得到掩膜图。通过上述步骤，就成功地在掩膜图上提取到了特别完整的棋子轮廓，再通过轮廓检测将棋子的中心像素坐标值进行确定，将这个中心坐标和轮廓匹配对齐到RGB帧图像上，最终完成对棋子的识别。\n\n\n\n\n\n\n\nMY VIEW\n优点：识别速度快，算法简单。缺点：会受到环境光的影响，鲁棒性较差，只能使用在理想测试环境下，实际工况复杂需要更多的其他增强稳定性的算法，可以考虑机器学习那一套搞决策树、随机森林、支持向量机之类的（这个后续我也会跟进去学习的）。\n\n\n代码实现python# 自适应调节rgb阈值 \ndef adjust_rgb_range(image, base_lower, base_upper):\n    # 计算图像的加权平均亮度\n    brightness = np.mean(image[:, :, 0] * 0.299 + image[:, :, 1] * 0.587 + image[:, :, 2] * 0.114)\n\n    # 根据亮度调整色域范围\n    adjustment_factor = brightness / 128.0  # 假设 128 是中等亮度\n    lower_rgb = np.array([base_lower[0] * adjustment_factor, base_lower[1] * adjustment_factor, base_lower[2] * adjustment_factor], dtype=np.uint8)\n    upper_rgb = np.array([base_upper[0] * adjustment_factor, base_upper[1] * adjustment_factor, base_upper[2] * adjustment_factor], dtype=np.uint8)\n\n# 背景减除二值化\ndef background_bin_subtraction(background, current_frame, binary_threshold=100):\n    # 调整二值化阈值 binary_threshold 可以有效地筛选黑白棋\n    # 调整图像尺寸和通道数\n    if background.shape != current_frame.shape:\n        current_frame = cv2.resize(current_frame, (background.shape[1], background.shape[0]))\n        if len(background.shape) == 2 and len(current_frame.shape) == 3:\n            current_frame = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n        elif len(background.shape) == 3 and len(current_frame.shape) == 2:\n            current_frame = cv2.cvtColor(current_frame, cv2.COLOR_GRAY2BGR)\n    \n    foreground = cv2.absdiff(background, current_frame)\n    global foreground_image \n    foreground_image = foreground\n    gray_foreground = cv2.cvtColor(foreground, cv2.COLOR_BGR2GRAY)\n    _, binary_foreground = cv2.threshold(gray_foreground, binary_threshold, 255, cv2.THRESH_BINARY) \n    return binary_foreground\n\n# 背景减除RGB空间\ndef background_rgb_subtraction(background, current_frame):\n    rgb_low_threshold = np.array([0, 0, 160])\n    rgb_high_threshold = np.array([65, 50, 235])\n    # 调整图像尺寸和通道数\n    if background.shape != current_frame.shape:\n        current_frame = cv2.resize(current_frame, (background.shape[1], background.shape[0]))\n        if len(background.shape) == 2 and len(current_frame.shape) == 3:\n            current_frame = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n        elif len(background.shape) == 3 and len(current_frame.shape) == 2:\n            current_frame = cv2.cvtColor(current_frame, cv2.COLOR_GRAY2BGR)\n    \n    # 计算差值\n    foreground = cv2.absdiff(background, current_frame)\n    mask = cv2.inRange(foreground, rgb_low_threshold, rgb_high_threshold)\n    #cv2.imshow(&quot;mask&quot;, mask)\n    return mask\n\n# 找白色棋子\ndef find_white_piece(color_image,area_low_threshold = 1000, area_high_threshold = 10000):\n    # 棋子认定面积阈值\n    # area_low_threshold\n    # area_high_threshold\n    # 背景减除二值化\n    white_piece_center_list = []\n    foreground = background_bin_subtraction(background_image, color_image)\n\n    # 使用形态学操作\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n    # closed = cv2.morphologyEx(foreground, cv2.MORPH_CLOSE, kernel)\n    #腐蚀\n    erode = cv2.erode(foreground, kernel, iterations=5)\n    # 膨胀\n    dilate = cv2.dilate(erode, kernel, iterations=5)\n    # cv2.imshow(&quot;dilate&quot;,dilate)\n    # 查找轮廓\n    contours, _ = cv2.findContours(dilate.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    i = 0\n    for contour in contours:\n        # 计算并绘制外接矩形框\n        x, y, w, h = cv2.boundingRect(contour)\n        cv2.rectangle(foreground, (x, y), (x + w, y + h), (0, 255, 0), 2)\n        area = cv2.contourArea(contour)\n        if area &gt; area_low_threshold and area &lt; area_high_threshold:\n            i += 1\n            # 绘制轮廓\n            cv2.drawContours(foreground, [contour], -1, (255, 255, 0), 2)\n            \n            # 计算轮廓的矩\n            M = cv2.moments(contour)\n            \n            # 计算轮廓的质心\n            if M[&quot;m00&quot;] != 0:\n                cX = int(M[&quot;m10&quot;] / M[&quot;m00&quot;])\n                cY = int(M[&quot;m01&quot;] / M[&quot;m00&quot;])\n            else:\n                cX, cY = 0, 0\n            \n            ## 打印质心坐标\n            #print(f&quot;Contour &#123;i&#125; center: (&#123;cX&#125;, &#123;cY&#125;)&quot;)\n\n            # 添加到白棋中心点列表\n            white_piece_center_list.append((cX, cY))\n            # 在质心处绘制数字\n            cv2.putText(foreground, str(i), (cX , cY), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n    if i == 0:\n        print(&quot;without any chess&quot;)\n    return white_piece_center_list, foreground\n\n# 找黑色棋子\ndef fine_black_piece(color_image,area_low_threshold = 1000, area_high_threshold = 40000):\n    black_piece_center_list = []\n    foreground = background_rgb_subtraction(background_image, color_image)\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n    dilate = cv2.dilate(foreground, (5,5), iterations=2)\n    #腐蚀\n    erode = cv2.erode(dilate, kernel, iterations=3)\n    # 膨胀\n    dilate = cv2.dilate(erode, kernel, iterations=3)\n    # cv2.imshow(&quot;dilate&quot;,dilate)\n    contours, _ = cv2.findContours(dilate.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    i = 0\n    for contour in contours:\n        area = cv2.contourArea(contour)\n        # 计算并绘制外接矩形框\n        x, y, w, h = cv2.boundingRect(contour)\n        # print(w*h)\n        if area &gt; area_low_threshold and area &lt; area_high_threshold:\n            i += 1\n            # 绘制轮廓\n            cv2.drawContours(foreground, [contour], -1, (255, 255, 0), 2)\n            # 计算轮廓的矩\n            M = cv2.moments(contour)\n            # 计算轮廓的质心\n            if M[&quot;m00&quot;] != 0:\n                cX = int(M[&quot;m10&quot;] / M[&quot;m00&quot;])\n                cY = int(M[&quot;m01&quot;] / M[&quot;m00&quot;])\n            else:\n                cX, cY = 0, 0\n            # # 打印质心坐标\n            # print(f&quot;Contour &#123;i&#125; center: (&#123;cX&#125;, &#123;cY&#125;)&quot;)\n            black_piece_center_list.append((cX, cY))\n            # 在质心处绘制数字\n            cv2.putText(foreground, str(i), (cX , cY), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n    if i == 0:\n        print(&quot;without any chess&quot;)\n    return black_piece_center_list, foreground串口通信通过串口通信与下位机通信\n实现思路\n\n\n\n\n\n\nWARNING\n在做这个串口通信的时候，使用的是serial库用python来进行串口通信，这个其实是人家通过C++写的封装成python库来用的相对会有很多的受限。因为jetson nano没有硬件中断，所以只能采用多线程的方式来模拟中断。所以调用threading库去实现对串口数据的监听。\n\n代码实现python\nimport threading\nimport serial \n\n# 串口读取数据线程\nclass SerialCommunication:\n    def __init__(self, port, baudrate):\n        self.serial_port = serial.Serial(\n            port=port,\n            baudrate=baudrate,\n            bytesize=serial.EIGHTBITS,\n            parity=serial.PARITY_NONE,\n            stopbits=serial.STOPBITS_ONE,\n            timeout=1\n        )\n    \n    def read_from_port(self): # 监听串口数据\n        while True:\n            try:\n                if self.serial_port.in_waiting &gt; 0:\n                    data = self.serial_port.readline().decode(&#39;utf-8&#39;).rstrip()\n                    print(f&quot;Receive:&#123;data&#125;&quot;)\n                    # 读到的数据会存入data，可以自己再写类的接口把这个data读出来\n\n            except Exception as e:\n                print(f&quot;An error occurred: &#123;e&#125;&quot;)其他\n\n\n\n\n\n\nWARNING\n解决了最大头的图像处理部分之后剩下的其实就是一些简单的逻辑判断了，基本就是一些if-else，我感觉不太重要就不太想讲了。就是作三子棋对弈算法会麻烦一点不过也没什么的我就也放在这里了。\n\n这里提供一点我当时写的函数\n\n对接对弈算法的\npython\n# 检查棋子是否在格子内，只是简单判断，通过比较点的坐标和格子的边界来判断点是否在格子内，复杂的需要考虑坐标系映射\ndef is_point_in_grid(point, grid_center, grid_size, piece_radius):\n    half_size = grid_size / 2\n    return (grid_center[0] - half_size &lt;= point[0] &lt;= grid_center[0] + half_size and\n            grid_center[1] - half_size &lt;= point[1] &lt;= grid_center[1] + half_size)\n\n# 检查棋子在哪个格子内\ndef check_pieces_in_grids(grid_centers, black_piece_centers, white_piece_centers, grid_size, black_piece_radius=2, white_piece_radius=2):\n    grid_status = []\n    black_pieces_outside = []\n    white_pieces_outside = []\n\n    for grid_center in grid_centers:\n        status = &quot;empty&quot;\n        for black_center in black_piece_centers:\n            if is_point_in_grid(black_center, grid_center, grid_size, black_piece_radius):\n                status = &quot;black&quot;\n                break\n        for white_center in white_piece_centers:\n            if is_point_in_grid(white_center, grid_center, grid_size, white_piece_radius):\n                status = &quot;white&quot;\n                break\n        grid_status.append(status)\n\n    # 检查不在任何格子内的黑棋\n    for black_center in black_piece_centers:\n        in_any_grid = any(is_point_in_grid(black_center, grid_center, grid_size, black_piece_radius) for grid_center in grid_centers)\n        if not in_any_grid:\n            black_pieces_outside.append(black_center)\n\n    # 检查不在任何格子内的白棋\n    for white_center in white_piece_centers:\n        in_any_grid = any(is_point_in_grid(white_center, grid_center, grid_size, white_piece_radius) for grid_center in grid_centers)\n        if not in_any_grid:\n            white_pieces_outside.append(white_center)\n\n    return grid_status, black_pieces_outside, white_pieces_outside\n\n\n对弈算法\n使用的是在棋类对弈中广泛使用的Minimax算法。又名极小化极大算法，是一种找出失败的最大可能性中的最小值的算法。Minimax算法常用于棋类等由两方较量的游戏和程序。该算法是一个零总和算法，即一方要在可选的选项中选择将其优势最大化的选择，另一方则选择令对手优势最小化的方法。而系统中函数的具体实现是minmax函数，而best_move函数则是通过调用minmax函数来找到机器的最佳移动位置，其返回值为机器的最佳移动位置。具体来说，Minimax算法模拟了所有可能的棋盘状态，本设计中的minmax函数返回值为得分，用于表示当前棋盘对玩家的有利程度，其主要包含以下几个步骤：\n\n检查当前棋盘状态是否有胜利者。如果有胜利者，返回相应的分数。\n检查棋盘是否已满。如果已满，返回 0 表示平局。\n如果轮到机器移动，则尝试所有可能的移动，返回得分最高的移动。\n如果轮到人类移动，则尝试所有可能的移动，返回得分最低的移动。minmax函数的具体实现见附录1。\n\npython\n#################################################################################################\n# 定义常量\nPLAYER_X = 1    #机器棋\nPLAYER_O = 2    #玩家棋\nEMPTY = 0       #无\n\ndef check_winner(board):\n    # 检查行、列和对角线\n    lines = [\n        [board[0][0], board[0][1], board[0][2]],\n        [board[1][0], board[1][1], board[1][2]],\n        [board[2][0], board[2][1], board[2][2]],\n        [board[0][0], board[1][0], board[2][0]],\n        [board[0][1], board[1][1], board[2][1]],\n        [board[0][2], board[1][2], board[2][2]],\n        [board[0][0], board[1][1], board[2][2]],\n        [board[2][0], board[1][1], board[0][2]]\n    ]\n    for line in lines:\n        if line == [PLAYER_X, PLAYER_X, PLAYER_X]:\n            return PLAYER_X\n        if line == [PLAYER_O, PLAYER_O, PLAYER_O]:\n            return PLAYER_O\n    return None\n\ndef is_board_full(board):\n    return all(cell != EMPTY for row in board for cell in row)\n\ndef minimax(board, is_maximizing):\n    winner = check_winner(board)\n    if winner == PLAYER_X:\n        return 1\n    if winner == PLAYER_O:\n        return -1\n    if is_board_full(board):\n        return 0\n\n    if is_maximizing:\n        best_score = -float(&#39;inf&#39;)\n        for row in range(3):\n            for col in range(3):\n                if board[row][col] == EMPTY:\n                    board[row][col] = PLAYER_X\n                    score = minimax(board, False)\n                    board[row][col] = EMPTY\n                    best_score = max(score, best_score)\n        return best_score\n    else:\n        best_score = float(&#39;inf&#39;)\n        for row in range(3):\n            for col in range(3):\n                if board[row][col] == EMPTY:\n                    board[row][col] = PLAYER_O\n                    score = minimax(board, True)\n                    board[row][col] = EMPTY\n                    best_score = min(score, best_score)\n        return best_score\n\ndef best_move(board):\n    best_score = -float(&#39;inf&#39;)\n    move = (-1, -1)\n    for row in range(3):\n        for col in range(3):\n            if board[row][col] == EMPTY:\n                board[row][col] = PLAYER_X\n                score = minimax(board, False)\n                board[row][col] = EMPTY\n                if score &gt; best_score:\n                    best_score = score\n                    move = (row, col)\n\ndef get_board(chessboard_block_list,black_piece_list,white_piece_list,grid_size,state):\n    # 检查棋子在哪个格子内\n    grid_status, black_piece_outside_list, white_piece_outside_list = check_pieces_in_grids(\n                            chessboard_block_list, \n                            black_piece_list, \n                            white_piece_list, \n                            grid_size)\n    # 将棋盘状态映射为棋盘\n    if state == 0:\n        def map_board_state(board):\n            state_mapping = &#123;\n                &quot;empty&quot;: 0,\n                &quot;black&quot;: 1,\n                &quot;white&quot;: 2\n            &#125;\n            return [[state_mapping[cell] for cell in row] for row in board]\n        mapped_board = map_board_state(grid_status)\n        return mapped_board\n    else :\n        def map_board_state(board):\n            state_mapping = &#123;\n                &quot;empty&quot;: 0,\n                &quot;black&quot;: 2,\n                &quot;white&quot;: 1\n            &#125;\n            return [[state_mapping[cell] for cell in row] for row in board]\n        mapped_board = map_board_state(grid_status)\n        return mapped_board\n\ndef play_chess(now_board,chessboard_grid_world_list=None,black_piece_world_list=None,white_piece_world_list=None, serial_comm=None,k_flag=1):\n    global last_board\n    row,col = 0,0\n    for i in range(3):      ##第六问判断棋子是否变化\n                for j in range(3):\n                    if now_board[i][j]-last_board[i][j]&gt;0:\n                        row,col = i,j\n                    elif now_board[i][j]-last_board[i][j]&lt;-1:\n                        print(&quot;请不要拿走&quot;+str(i)+&quot;,&quot;+str(j)+&quot;黑棋子,请放回去&quot;)\n                        k_flag = 0\n                        code = i*3+j+1\n                        \n                        \n                        \n                    elif now_board[i][j]-last_board[i][j]&lt;0:\n                        k_flag = 0\n                        print(&quot;请不要拿走&quot;+str(i)+&quot;,&quot;+str(j)+&quot;白棋子,请放回去&quot;)\n                        code = i*3+j+1\n    if k_flag :     \n        last_board[row][col] = PLAYER_O     ##棋子被放置完之后记录上一步是玩家下的\n        print(&quot;对手完成：&quot;+str(row)+&quot;,&quot;+str(col))\n        \n        if check_winner(last_board):\n            print(last_board)\n            print(&quot;玩家 O 胜利!&quot;)\n            k_flag = 0\n        if is_board_full(last_board):\n            print(last_board)\n            print(&quot;平局!&quot;)\n            k_flag = 0\n\n    if k_flag:  ##如果标志位为1则继续此回合\n            # 机器回合\n            row, col = best_move(last_board)    ##判断最佳决策\n            print(&quot;机器的回合:&quot;+str(row)+&quot;,&quot;+str(col))\n            if serial_comm.turn4_flag == 1:\n                black_piece_world_interger_x, black_piece_world_decimal_x = split_float_number_to_str(black_piece_world_list[0][0]/10.0)\n                black_piece_world_interger_y, black_piece_world_decimal_y = split_float_number_to_str(black_piece_world_list[0][1]/10.0)\n                message_piece = &quot;(*&quot; + black_piece_world_interger_y + black_piece_world_decimal_y + black_piece_world_interger_x + black_piece_world_decimal_x + &quot;)&quot;\n                serial_comm.serial_port.write(message_piece.encode(&quot;utf-8&quot;))       ##等待机器将棋子放置完毕\n            elif serial_comm.turn5_flag == 1:\n                white_piece_world_interger_x, white_piece_world_decimal_x = split_float_number_to_str(white_piece_world_list[0][0]/10.0)\n                white_piece_world_interger_y, white_piece_world_decimal_y = split_float_number_to_str(white_piece_world_list[0][1]/10.0)\n                message_piece = &quot;(*&quot; + white_piece_world_interger_y + white_piece_world_decimal_y + white_piece_world_interger_x + white_piece_world_decimal_x + &quot;)&quot;\n                serial_comm.serial_port.write(message_piece.encode(&quot;utf-8&quot;))       ##等待机器将棋子放置完毕\n            chessboard_grid_world_interger_x, chessboard_grid_world_decimal_x = split_float_number_to_str(chessboard_grid_world_list[row*3+col][0]/10.0)\n            chessboard_grid_world_interger_y, chessboard_grid_world_decimal_y = split_float_number_to_str(chessboard_grid_world_list[row*3+col][1]/10.0)\n            message = &quot;(*&quot; + chessboard_grid_world_interger_y + chessboard_grid_world_decimal_y + chessboard_grid_world_interger_x + chessboard_grid_world_decimal_x + &quot;)&quot;\n            serial_comm.serial_port.write(message.encode(&quot;utf-8&quot;))       ##等待机器将棋子放置完毕\n            \n            print(&quot;机器完成:&quot;)\n            ##记录上一步机器下的位置\n            last_board[row][col] = PLAYER_X\n\n            ##判断游戏是否结束\n            if check_winner(last_board):\n                print(last_board)\n                print(&quot;机器 胜利!&quot;)\n                \n            if is_board_full(last_board):\n                print(last_board)\n                print(&quot;平局!&quot;)\n            \n#################################################################################################\n\n\n后记\n\n\n\n\n\nTIP\n四天三夜的电赛，人要颠了，好悬没把我攒昏过去。总算搞完电赛的总结了~~\n\n","slug":"三子棋对弈装置-视觉部分","date":"2024-08-12T14:55:02.000Z","categories_index":"electronic","tags_index":"ComputerVision","author_index":"PIKO"},{"id":"71bced55db8d61b5404a5480e19d78af","title":"树莓派数字识别——openCV方法","content":"前言本项目是树莓派识别数字的小例程电赛备赛时写的，实现对1-8的数字识别并与下位机通信控制小车进行运动\n\n\n\n项目前置python 版本要求\n\n\n\n\n\nTIP\npython&gt;&#x3D;3.6\n\n库依赖要求做图像处理\n\n\n\n\n\n\nTIP\nnumpyopencv-python\n\n做引脚操控\n\n\n\n\n\n\nTIP\nRPi.GPIOpyserialrpi-lgpi\n\n项目主体主要实现思路传统图像处理，使用模板匹配\n主要步骤:::warning\n\n对模板图像进行预处理，得到1-8总共8个模板\n对识别图像进行预处理，将图像二值化转换取出矩形区域组成ROI区域\nROI和模板做模板匹配:::\n\n详细过程仅展示部分源码，详细源码可以去 GitHub库\n模板图像预处理\n\n\n模板预览图本身是含有八个数字带外接矩形黑框的一张图，我们对它作二值化处理，在对二值化模板图像边缘提取，通过轮廓检测将模板的轮廓提取，在将其外接矩形坐标画出来，在对不同数字1-8进行排序，最后将整个图像按比例内截，在重排成（100，150）的固定的模板存入字典。\n\npython\n# 定义需识别数字\nto_detect_num = None\n\n# 硬件初始化\nser = hardware_init()\n\n    \ntemplate = cv2.imread(osp.join(template_dir, &#39;num_template.jpg&#39;))\ngray_template = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\nprint(&#39;二值化模板图像已读取&#39;)\n\n# 边缘检测\nedged_template = cv2.Canny(template, 80, 200) # 边缘低阈值75和高阈值20\n\n# 轮廓检测\ntemplate_contours, hierarchy = cv2.findContours(edged_template, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n# 读取模板轮廓的矩形坐标\nx, y, w, h = cv2.boundingRect(template_contours[1])\n\nprint(f&#39;检测到&#123;len(template_contours)&#125;个轮廓&#39;)\n\n# 模板数字排序 1-8 (太菜了写不出自动排序的脚本 &gt;_&lt; )\nsorted_template_contours = [template_contours[5], template_contours[4], template_contours[1],template_contours[0], template_contours[7], template_contours[6], template_contours[3], template_contours[2]]\n\n# 构造模板字典\ndigit_dict = &#123;&#125;\n\n# 分割模板图像\nfor (i, c) in enumerate(sorted_template_contours):\n    (x, y, w, h) = cv2.boundingRect(c)\n    roi = gray_template[y+10:y+h-10, x+10:x+w-10]\n    \n    roi = cv2.resize(roi, (100, 150))\n    digit_dict[i] = roi识别图像预处理从摄像头获取视频流，将其缩放固定比例减少后续的运算量，转换为灰度图并应用高斯模糊将锐度过高的区域排除边缘和消除噪点，再使用Canny边缘检测，提取边缘后进行膨胀，得到识别数字完整的矩形边缘轮廓。通过轮廓检测再将轮廓面积最大的前5个轮廓进行提取并排序，计算轮廓周长，进行近似轮廓，看是否能构成四边形，将能构成四边形的轮廓且面积符合一定要求的轮廓进行筛选，并通过两个矩形左x坐标的大小来判断数字的左右并重排。本身通过相机拍到的矩形数字框就不可能是正好的矩形，需要我们进行图像变换，因此进行透射变换，将提取到的四边形变换成矩形然后二值化。\n\n\n\n\n\npython\n# 初始化视频捕获对象\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    # 逐帧读取视频\n    start_time = time.time()\n    ret, frame = cap.read()\n    if not ret:\n        break\n    # 图像resize\n    ratio = frame.shape[0] / 500.0\n    orig_frame = frame.copy()\n    # 转换为灰度图像\n    gray_img = cv2.cvtColor(orig_frame, cv2.COLOR_BGR2GRAY)\n    # 应用高斯模糊\n    blur_img = cv2.GaussianBlur(gray_img, (5, 5), 0)\n    \n    # Canny 边缘检测\n    edged_img = cv2.Canny(blur_img, low_threshold, high_threshold) # 边缘低阈值75和高阈值200\n    cv2.imshow(&quot;edged_img&quot;,edged_img)\n    # 定义膨胀操作的核\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n    # 对边缘检测后的图像进行膨胀操作\n    dilated_img = cv2.dilate(edged_img, kernel, iterations=2)\n    cv2.imshow(&quot;Dilated Image&quot;, dilated_img)\n\n    \n    # 轮廓检测\n    contours, _ = cv2.findContours(edged_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)[:5]\n    \n    \n    i = 0\n    screenCnt = []  # 在循环开始前初始化screenCnt\n    output = []\n    for c in sorted_contours:\n        # 计算轮廓周长\n        peri = cv2.arcLength(c, True)\n        # 近似轮廓  0.02*peri为近似精度\n        approx = cv2.approxPolyDP(c, 0.02*peri, True)\n        \n        # 如果近似轮廓有四个顶点，则认为找到了数字边缘\n        if len(approx)==4 and cv2.contourArea(approx) &gt; 10000 and cv2.contourArea(approx) &lt; 45000:\n            #num_detect.shrink_approx(approx, 3)\n            \n            screenCnt.append(approx)\n            i += 1\n            if i == 2:\n                break\n    # 根据检测到的两个数字的x左坐标来判断左右并排序\n    \n    if len(screenCnt) == 2:\n        num_detect.left_right_sort(screenCnt)\n\n        # print(f&quot;左:&#123;screenCnt[0]&#125;, 右:&#123;screenCnt[1]&#125;&quot;)     \n\n\n    # 在尝试展示轮廓之前检查screenCnt是否已定义\n    if screenCnt != []:\n        cv2.drawContours(frame, screenCnt, -1, (255, 255, 0), 2)\n        postion_count = 0\n        warped_list = []\n        thresh_list = []\n        for c in screenCnt:\n            \n            # 检查轮廓c是否有点（即是否有效）\n            if c.size == 0:\n                print(&quot;找到一个空的轮廓，跳过。&quot;)\n                continue  # 跳过当前轮廓，继续下一个轮廓\n            # 透视变换\n            warped_list.append(num_detect.four_point_transform(orig_frame, c.reshape(4, 2) * ratio))\n            warped_list[postion_count] = cv2.cvtColor(warped_list[postion_count], cv2.COLOR_BGR2GRAY)\n            thresh_list.append(cv2.threshold(warped_list[postion_count], 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1])\n            \n            # 显示透视变换结果\n            cv2.imshow(f&quot;Thresh_&#123;postion_count&#125;&quot;, thresh_list[postion_count])ROI区域首先将图像预处理得到的二值化图像进行重排，变成（150，100）的图像，再进行高斯模糊和腐蚀操作滤除噪点，边缘检测再次提取矩形框边缘，轮廓检测提取矩形框轮廓，最后将整个图像按比例内截，在重排成（100，150）的ROI区域送给下步模板匹配。\n\n\n\n\n\npython\nif roi.size &gt; 0:  # 检查roi是否为空\n    roi = cv2.resize(thresh_list[postion_count], (100, 150))\n    # 定义结构元素\n    kernel = np.ones((5,5), np.uint8)\n\n    roi_blur = cv2.GaussianBlur(roi, (7, 7), 0)\n    \n    roi_edge = cv2.Canny(roi_blur, 80, 200)\n    roi_eroded = cv2.erode(roi_edge, kernel, iterations=2)\n    # 找到边缘的轮廓\n    roi_contours, _ = cv2.findContours(roi_edge, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    for cnt in roi_contours:\n        (x,y,w,h) = cv2.boundingRect(roi_edge)\n    \n    cv2.rectangle(roi, (x, y), (x+w, y+h), (0, 255, 0), 2)\n    cv2.imshow(&quot;roi_edge&quot;,roi_edge)\n    \n    \n    roi = cv2.resize(roi[y:y+h, x:x+w],(100, 150))\n    roi = cv2.resize(roi[0+15:150-15, 0+12:100-12],(100, 150))\n    cv2.imshow(f&quot;ROI&#123;postion_count&#125;&quot;, roi)\nelse:\n    print(&quot;ROI为空，跳过调整大小。&quot;)\n\npostion_count += 1模板匹配将字典里的数字与模板图像取出与识别图像的ROI区域进行模板匹配，找到最适合的数字，再进行个赋值。\npython\n# 初始化模板匹配\nscores = []\n\nfor (digit, digitROI) in digit_dict.items():\n    result = cv2.matchTemplate(roi, digitROI, cv2.TM_CCOEFF) # 模板匹配\n    (_, score, _, _) = cv2.minMaxLoc(result)\n    scores.append(score)\ntrust = False\n# 找到最适合的数字，从scores中找到最大值的索引 + 1     1~8\nfor score in scores:\n    if score &gt; 0.8:\n        trust = True      \nif trust:\n    output.append(str(scores.index(max(scores)) + 1))下位机通信通过串口去传输数据，将左右数字传给下位机MCU进行进一步的控制。\npython\nif output != []:# 如果识别结果不为空\n    if to_detect_num is not None: # 如果这不是第一次检测\n        for i in range(len(output)):\n            cv2.putText(frame, output[i], (screenCnt[i][0][0][0], screenCnt[i][0][0][1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n        if len(output) == 2:\n            print(f&quot;左：&#123;output[0]&#125;，右：&#123;output[1]&#125;&quot;)\n            if(hardware.UART_read(ser)==b&#39;O&#39;):\n                print(f&quot;发送&#123;output[0]&#125;、&#123;output[1]&#125;成功&quot;)\n                hardware.UART_write(ser, f&#39;(&#123;output[0]&#125;&#123;output[1]&#125;)\\r\\n&#39;.encode())\n    else:\n        while(hardware.UART_read(ser)==b&#39;O&#39;):\n            hardware.UART_write(ser, f&#39;(&#123;output[0]&#125;&#123;output[0]&#125;)\\r\\n&#39;.encode())\n            print(f&quot;发送&#123;output[0]&#125;成功&quot;)\n            to_detect_num = int(output[0])\n            print(f&quot;需要识别的数字为 &#123;output[0]&#125;&quot;)\n            print(&quot;等待下位机OK&quot;)\n            start_wait_OK = time.time()\n        \n            if(((start_wait_OK - time.time())*10)%10 &gt; 1):\n                start_wait_OK = time.time()\n                print(&quot;等待超时&quot;)\n                hardware.UART_write(ser, f&#39;(&#123;output[0]&#125;&#123;output[0]&#125;)\\r\\n&#39;.encode())\n                print(f&quot;发送&#123;output[0]&#125;成功&quot;)识别效果仅部分图片，有待补充\n\n\n\n\n\n\n  \n\n\n\n","slug":"树莓派数字识别——openCV方法","date":"2024-07-22T15:50:17.000Z","categories_index":"MashineLearning","tags_index":"python","author_index":"PIKO"},{"id":"714f64421d876371e3d936b45ca967be","title":"Jetson Nano部署Yolo模型笔记","content":"前言本人在大二期间就学习了计算机视觉相关的模型搭建、训练和预测。大致了解目标检测的训练流程，却没有去实际在端设备上进行过模型的部署。在准备电赛期间，拿到 jetson nano 开始玩一玩模型部署，就有了这篇博客。\n\n项目前置\n\n\n\n\n\n \n深度模型YOLOv8(pytorch框架)\n深度模型部署框架infer (基于TensorRT和onnx模型的框架)\n\n\n项目主体\n\n\n\n\n\n\n \n使用YOLOv8对数字进行识别\n\n\n整体流程\n\n\n\n\n\n\nWARNING\n电脑上进行YOLOv8模型训练—&gt; 输出.pt模型—&gt; 转换成onnx模型(通用的模型)—&gt; 在infer框架下转换成.engine—&gt; 运行.engine开始运行\n\n\n模型训练\n\n\n\n\n\nTIP\n在PC端或是GPU服务器上进行会比较顺利。\n\n\n下载YOLOv8YOLOv8_GitHub库\nbash# 使用命令行\ngit clone https://github.com/ultralytics/ultralytics.gitpython安装ultralytics库（推荐在conda上进行）\nbashpip install ultralytics安装其他依赖库\nbash# 在ultralytics的仓库里找到requirements.txt,注意当前工作目录（转移到requirements.txt所在目录）\npip install -r requirements.txt数据集获取\n\n\n\n\n\nTIP\n根据最终需要识别的目标，可以去网上找现成的数据集\n\n\n在这里推荐查找数据集的两个网站\n\nKaggle\n\n\n\nroboflow Universe\n\n\n\n\n数据集采集（可选）\n\n\n\n\n\nTIP\n也可以自己去做数据集，像本项目就是自己去采的数据集。\n\n\n本项目\n总共400张，通过写opencv的脚本去实现采集。\n\n\n\n\n\n\n\n\nWARNING\n这里提供一个可以通过摄像头去获取图片的脚本auto_get_img.py\n\npythonimport cv2 \nimport os\nimport os.path as osp\nimport numpy as np\n# 保存目录需要自己去设置\nscript_dir = osp.dirname(__file__)\nimg_dir = osp.join(script_dir, &#39;image&#39;)\nsave_dir = osp.join(img_dir, &#39;captured_image_dir&#39;)\n\n# 创建保存目录,如果不存在\nif not osp.exists(save_dir):\n    os.makedirs(save_dir)\n    print(f&quot;保存目录&#123;save_dir&#125;已创建&quot;)\nelse:\n    print(f&quot;保存目录&#123;save_dir&#125;已存在&quot;)\n# 创建视频捕获对象\ncap = cv2.VideoCapture(0)\n\nif not cap.isOpened():\n    print(&quot;无法打开摄像头&quot;)\n    exit()\ntimes = int(input(&quot;请输入需要截取几个图片\\n&quot;))\ni = 0\nwhile True:\n    # 读取一帧\n    ret, frame = cap.read()\n    if ret:\n        # 显示图像\n        cv2.imshow(&#39;frame&#39;, frame)\n        \n        # 保存图像\n        if cv2.waitKey(1) &amp; 0xFF == ord(&#39;s&#39;): # 按s保存图片\n            cv2.imwrite(osp.join(save_dir , f&quot;img_&#123;i&#125;.jpg&quot;), frame)\n            print(f&quot;图像已保存为 img_&#123;i&#125;.jpg&quot;)\n            times -= 1\n            i += 1\n            \n        \n    else:\n        print(&quot;无法读取摄像头图像&quot;)\n    if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;) or times == 0:  # 按q退出\n        break\n# 释放摄像头\ncap.release()\n# 关闭所有OpenCV窗口\ncv2.destroyAllWindows()数据集的标定（可选）本项目使用的是精灵标注助手，使用labelme之类的标注工具也是可以的，我们的目的是对数据集图片打标，对我们感兴趣的区域进行标注，画出bondnigbox。我们做的是目标检测detect，只画矩形框即可。\n\n大致如图\n\n\n\nXML转txt(YOLO数据集格式)XML是labelme标注完之后生成的文件格式，详细可以去google看看。Yolo接受的数据集格式是txt，详细去google。\n\n\n\n\n\n\n\nWARNING\n这里提供一个XML转txt的脚本xml_to_txt.py\n\npython\n#########################\n# xml2txt.py\n#########################\n\nimport xml.etree.ElementTree as ET\nimport os, cv2\nimport numpy as np\nfrom os import listdir\nfrom os.path import join\n\nclasses = [&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;,&#39;6&#39;,&#39;7&#39;,&#39;8&#39;] # 写入你做目标检测的物体的分类\n\n\ndef convert(size, box):\n    dw = 1. / (size[0])\n    dh = 1. / (size[1])\n    x = (box[0] + box[1]) / 2.0 - 1\n    y = (box[2] + box[3]) / 2.0 - 1\n    w = box[1] - box[0]\n    h = box[3] - box[2]\n    x = x * dw\n    w = w * dw\n    y = y * dh\n    h = h * dh\n    return (x, y, w, h)\n\n\ndef convert_annotation(xmlpath, xmlname):\n    with open(xmlpath, &quot;r&quot;, encoding=&#39;utf-8&#39;) as in_file:\n        txtname = xmlname[:-4] + &#39;.txt&#39;\n        txtfile = os.path.join(txtpath, txtname)\n        tree = ET.parse(in_file)\n        root = tree.getroot()\n        filename = root.find(&#39;filename&#39;)\n        img = cv2.imdecode(np.fromfile(&#39;&#123;&#125;/&#123;&#125;.&#123;&#125;&#39;.format(imgpath, xmlname[:-4], postfix), np.uint8), cv2.IMREAD_COLOR)\n        h, w = img.shape[:2]\n        res = []\n        for obj in root.iter(&#39;object&#39;):\n            cls = obj.find(&#39;name&#39;).text\n            if cls not in classes:\n                classes.append(cls)\n            cls_id = classes.index(cls)\n            xmlbox = obj.find(&#39;bndbox&#39;)\n            b = (float(xmlbox.find(&#39;xmin&#39;).text), float(xmlbox.find(&#39;xmax&#39;).text), float(xmlbox.find(&#39;ymin&#39;).text),\n                 float(xmlbox.find(&#39;ymax&#39;).text))\n            bb = convert((w, h), b)\n            res.append(str(cls_id) + &quot; &quot; + &quot; &quot;.join([str(a) for a in bb]))\n        if len(res) != 0:\n            with open(txtfile, &#39;w+&#39;) as f:\n                f.write(&#39;\\n&#39;.join(res))\n\n\nif __name__ == &quot;__main__&quot;:\n    postfix = &#39;jpg&#39;\n    imgpath = &#39;xxx&#39; # 图片存储位置\n    xmlpath = &#39;xxx&#39; # xml存储位置\n    txtpath = &#39;xxx&#39; # txt存储位置\n\n    if not os.path.exists(txtpath):\n        os.makedirs(txtpath, exist_ok=True)\n\n    list = os.listdir(xmlpath)\n    error_file_list = []\n    for i in range(0, len(list)):\n        try:\n            path = os.path.join(xmlpath, list[i])\n            if (&#39;.xml&#39; in path) or (&#39;.XML&#39; in path):\n                convert_annotation(path, list[i])\n                print(f&#39;file &#123;list[i]&#125; convert success.&#39;)\n            else:\n                print(f&#39;file &#123;list[i]&#125; is not xml format.&#39;)\n        except Exception as e:\n            print(f&#39;file &#123;list[i]&#125; convert error.&#39;)\n            print(f&#39;error message:\\n&#123;e&#125;&#39;)\n            error_file_list.append(list[i])\n    print(f&#39;this file convert failure\\n&#123;error_file_list&#125;&#39;)\n    print(f&#39;Dataset Classes:&#123;classes&#125;&#39;)转换得到的文件要做成类似如下文件结构\nbash——dataset\n|\n|\n————train—————images\n|       |\n|       ——————labels\n|\n————test——————images\n|       |\n|       ——————labels\n|\n————val———————images\n        |\n        ——————labels\n配置数据集在yolo中通过yaml文件来对数据集进行管理和读取，在训练时调用数据集就是看yaml的。\n一下提供一个简单的yaml文件的范例，建议把yaml文件和dataset放在同一个文件夹\nyaml\npath: F:\\AI\\Yolov8\\Project\\datasets\\number\\dataset #数据集的位置\ntrain: images/train # train图片的文件夹与数据集path的相对位置\nval: images/val # val图片的文件夹与数据集path的相对位置\n\nnc: 8 # 目标检测物体类的数目 \nnames: [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;] # 物体对应的名称\nYOLO模型训练通过yolo官方提供的库进行训练，选取yolov8n.pt作为预训练模型。yolov8n是规模最小的模型，之后部署在jetson nano上运行会相对比较流畅。\n\n\n\n\n\n\n\nWARNING\n这里提供一个训练脚本train.py\n\npythonfrom ultralytics import YOLO\nimport os.path as osp\nroot_dir = osp.dirname(osp.dirname(osp.abspath(__file__))) #&lt;-- absolute dir the script is in\nscript_dir = osp.dirname(__file__) #&lt;-- absolute dir the script is in\nsave_dir = osp.join(root_dir, &#39;exports/runs/train/exp&#39;) # 数据会保存到../exports/runs/train/exp\nif not osp.exists(save_dir):\n    os.makedirs(save_dir)\ndataset_dir = osp.join(root_dir, &#39;datasets&#39;) # 数据集存放文件夹位置\n\nif __name__ == &#39;__main__&#39;: \n    model = YOLO(f&quot;&#123;root_dir&#125;//yolov8n.pt&quot;)  # 改成你的模型的存放地址\n    # YOLO(&quot;model.pt&quot;)  use pre-trained model if available\n    model.info()  # 展示模型信息\n    model.train(task = &#39;detect&#39;, data= dataset_dir +&quot;\\\\number\\\\num_detection.yaml&quot;, epochs=200, workers=2, save_dir=save_dir)  # 进行模型训练\n    # task      -&gt;  表示任务 detect表示做目标检测\n    # data      -&gt;  表示数据集yaml文件的存放位置\n    # epochs    -&gt;  表示训练的轮数\n    # workers   -&gt;  表示装载时CPU的线程数\n    # save_dir  -&gt;  表示训练出的模型的保存路径\n模型验证训练出模型之后我们需要检验一下模型的识别效果，可以去runs里看混淆矩阵或召回率之类的，不过这不是我这篇博客的重点。\n\n\n\n\n\n\n\nWARNING\n我在这提供一个摄像头的验证demo\n\npython\nimport os.path as osp\nimport cv2\nfrom ultralytics import YOLO\nfrom cv2 import getTickCount, getTickFrequency\n# 加载 YOLOv8 模型\nscript_dir = osp.dirname(osp.abspath(__file__)) #&lt;-- absolute dir the script is in\nparent_dir = osp.dirname(script_dir)\ngrandpa_dir = osp.dirname(parent_dir)\nmodel_path = osp.join(grandpa_dir, &#39;runs/detect/train/weights/best.pt&#39;)\nmodel = YOLO(f&quot;&#123;model_path&#125;&quot;)\n\n# 获取摄像头内容，参数 0 表示使用默认的摄像头\ncap = cv2.VideoCapture(0)\n\nwhile cap.isOpened():\n    loop_start = getTickCount()\n    success, frame = cap.read()  # 读取摄像头的一帧图像\n\n    if success:\n        results = model.predict(source=frame) # 对当前帧进行目标检测并显示结果\n    annotated_frame = results[0].plot()\n\n    # 中间放自己的显示程序\n    loop_time = getTickCount() - loop_start\n    total_time = loop_time / (getTickFrequency())\n    FPS = int(1 / total_time)\n    # 在图像左上角添加FPS文本\n    fps_text = f&quot;FPS: &#123;FPS:.2f&#125;&quot;\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    font_scale = 1\n    font_thickness = 2\n    text_color = (0, 0, 255)  # 红色\n    text_position = (10, 30)  # 左上角位置\n\n    cv2.putText(annotated_frame, fps_text, text_position, font, font_scale, text_color, font_thickness)\n    cv2.imshow(&#39;img&#39;, annotated_frame)\n    # 通过按下 &#39;q&#39; 键退出循环\n    if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):\n        break\n\ncap.release()  # 释放摄像头资源\ncv2.destroyAllWindows()  # 关闭OpenCV窗口模型导出yolo是基于pytorch深度学习框架的一种视觉模型，经过训练产生的模型是后缀.pth的pytorch模型，是无法直接在端侧（非pytorch框架下）直接运行的。因此我们需要将其转换成其他类型的模型文件。一般pytorch模型都是导出成onnx模型（一种相对通用的模型）。\n\n\n\n\n\n\n\nWARNING\n这里提供一个pth转onnx的脚本pth_to_onnx.py\n\npython\nimport os.path as osp\nfrom ultralytics import YOLO\n\nscript_dir = osp.dirname(osp.abspath(__file__)) #&lt;-- absolute dir the script is in\nparent_dir = osp.dirname(script_dir)\nroot_dir = osp.dirname(parent_dir)\nmodel_path = osp.join(root_dir, &#39;runs\\\\detect\\\\train\\\\weights\\\\best.pt&#39;) # 更改成你的模型存放位置 \n\nmodel = YOLO(model_path)\n\nsuccess = model.export(format=&quot;onnx&quot;, batch=1, save_dir=osp.join(parent_dir, &#39;exports&#39;))#导出为onnx模型\n\n# 静态batch 输入batch张图片作batch个的推理\n# success = model.export(format=&quot;onnx&quot;, dynamic=Ture)\n# 动态batch 输入多少张图片作多少张推理 server时使用(服务器)\n# 此处导出的模型为静态batch\n将生成的onnx模型文件上传到jetson nano上，在上位机上的模型训练工作我们就完成了。\n模型部署本次模型部署使用的是infer的模型部署框架详细操作可以参考其GitHub库。\n首先我们在github上拉取infer的仓库\nbashgit clone https://github.com/shouxieai/infer.git\ninfer框架的文件结构\n\nsrc 源文件夹，存放最终我们cmake的c++文件，进行检测的代码workspace 存放我们的onnx模型和提供一个节点v8格式到infer格式转换脚本v8trans.pyworkspace&#x2F;inference 存放我们的待检测的图片\n\n需要在配置trtexec工具的环境变量\n打开.bashrc文件\nbashnano ~/.bashrc在最后一行添加如下命令\nbashexport PATH=/usr/src/tensorrt/bin:$PATH按下ctrl+x y Enter保存并退出\n刷新一下环境变量\nbashsource ~/.bashrconnx模型转换将onnx模型转移到infer的workspace文件夹中，将当前工作路径移动到workspace文件夹中，执行infer提供的onnx转换到infer框架下的onnx的脚本v8trans.py\npythonpython v8trans.py xx.onnx运行后会在当前工作目录下生成一个xx.trans.onnx模型\nonnx模型优化onnxoptimizer、onnxsim被誉为onnx的优化利器，其中onnxsim可以优化常量，onnxoptimizer可以对节点进行压缩\n\n\n\n\n\n\n\nWARNING\n这里提供一个onnx模型优化脚本v8onnxsim.py\n\npython\nimport onnx\nfrom onnxsim import simplify\n\nonnx_model = onnx.load(&quot;xx.transd.onnx&quot;)\nmodel_simp, check = simplify(onnx_model)\nassert check, &quot;Simplified ONNX model could not be Validated&quot;\nonnx.save(model_simp, &quot;xx.transd.sim.onnx&quot;)\n执行完成会在当前工作目录生成一个xx.transd.sim.onnx模型，其在端侧运行的能效会高一些。\n生成engineinfer框架可以直接通过trtexec工具直接生成engine文件。比较简单，没有复杂的封装，适合初学者。\n终端使用trtexec工具直接构建engine\nbashtrtexec --onnx=workspace/xx.transd.sim.onnx --saveEngine=workspace/xx.transd.sim.engine\ntrtexec的扩展应用\ntrtexec是NVIDIA TensorRT SDK中的一个实用工具，它允许用户从命令行轻松运行和测试TensorRT引擎。trtexec命令行工具可以使用以下参数：\nbash./trtexec [-h] [--uff model.uff [model.uff ...]] [--onnx model.onnx] [--model=model.plan] [--deploy=&lt;deploy.prototxt&gt;] [--output=&lt;output_name&gt;] [--batch=N] [--device=N] [--workspace=N] [--fp16] [--int8] [--calib=&lt;dir&gt;] [--useDLA=N] [--allowGPUFallback] [--iterations=N] [--avgRuns=N] [--verbose] [--nshapes=N] [--optShapes=NxN ... NxN] [--minShapes=NxN ... NxN] [--maxShapes=NxN ... NxN] [--shapeInput=&#123;0,1,2,...&#125;] [--tacticSources] [--loadEngine=&lt;filename&gt;] [--saveEngine=&lt;filename&gt;] [--plugins=&lt;XML file&gt;] [--dumpOutput=&lt;filename&gt;] [--excludePlugin=&lt;name&gt;] [--start=&lt;tag&gt;] [--streams=N] [--batchTile] [--engine=&lt;filename&gt;] [--uffInput=input_name,input_shape] [--uffNHWC] [--uffNCHW] [--uffHW=&lt;value&gt;] [--workspaceSize=&lt;size&gt;] [--buildOnly] [--engineFormat=&lt;format&gt;] [--refit] [--saveRefinedEngine=&lt;filename&gt;] [--calibrator=&lt;classname&gt;] [--detach] [--check] [--fp16Char] [--int8Calib=&lt;dir&gt;] [--int8IO] [--disableTensorCores] [--useSpinWait] [--shapes=&lt;shape&gt;] [--maxBatch=&lt;size&gt;]\n\n其中一些重要的参数如下：\n\n--uff：指定输入为UFF模型，后面跟上模型文件的路径。\n--onnx：指定输入为ONNX模型，后面跟上模型文件的路径。\n--model：指定输入为序列化的引擎文件，后面跟上文件路径。\n--deploy：指定输入为Caffe deploy文件的路径。\n--output：指定输出Tensor名称。\n--batch：指定执行推理时每个batch的大小，默认为1。\n--device：指定执行推理的设备编号，默认为0。\n--workspace：指定GPU内存的最大使用量，默认为1GB。\n--fp16：启用FP16精度，可提高推理性能和减少内存使用。\n--int8：启用INT8精度，可进一步提高推理性能和减少内存使用。\n--calib：指定INT8校准数据集的路径。\n--useDLA：指定使用哪个DLA，以及在DLA上运行哪些层。\n--allowGPUFallback：如果使用DLA，当某些层无法在DLA上运行时，是否允许将其回退到GPU。\n--iterations：指定测试迭代次数。\n--avgRuns：指定平均运行次数。\n--verbose：打印更详细的输出信息。\n--loadEngine：指定加载的TensorRT引擎文件，后面跟上文件路径\n--saveEngine：指定生成的TensorRT引擎文件，后面跟上文件路径\n\n\nengine构建完成之后就可以开始运行了\n开始检测实现视频流的实时目标检测，需提前给jetson nano装上USB摄像头。\n\n\n\n\n\n\n\nWARNING\nMakefile文件修改\n\nmakefilecc        := g++\nnvcc      = /usr/local/cuda-10.2/bin/nvcc\n\ncpp_srcs  := $(shell find src -name &quot;*.cpp&quot;)\ncpp_objs  := $(cpp_srcs:.cpp=.cpp.o)\ncpp_objs  := $(cpp_objs:src/%=objs/%)\ncpp_mk\t  := $(cpp_objs:.cpp.o=.cpp.mk)\n\ncu_srcs\t  := $(shell find src -name &quot;*.cu&quot;)\ncu_objs   := $(cu_srcs:.cu=.cu.o)\ncu_objs\t  := $(cu_objs:src/%=objs/%)\ncu_mk\t  := $(cu_objs:.cu.o=.cu.mk)\n\ninclude_paths := src        \\\n            /usr/include/opencv4 \\\n            /usr/include/aarch64-linux-gnu \\\n            /usr/local/cuda-10.2/include\n\nlibrary_paths := /usr/lib/aarch64-linux-gnu \\\n            /usr/local/cuda-10.2/lib64\n\nlink_librarys := opencv_core opencv_highgui opencv_imgproc opencv_videoio opencv_imgcodecs \\\n            nvinfer nvinfer_plugin nvonnxparser \\\n            cuda cublas cudart cudnn \\\n            stdc++ dl\n\nempty\t\t  :=\nexport_path   := $(subst $(empty) $(empty),:,$(library_paths))\n\nrun_paths     := $(foreach item,$(library_paths),-Wl,-rpath=$(item))\ninclude_paths := $(foreach item,$(include_paths),-I$(item))\nlibrary_paths := $(foreach item,$(library_paths),-L$(item))\nlink_librarys := $(foreach item,$(link_librarys),-l$(item))\n\ncpp_compile_flags := -std=c++11 -fPIC -w -g -pthread -fopenmp -O0\ncu_compile_flags  := -std=c++11 -g -w -O0 -Xcompiler &quot;$(cpp_compile_flags)&quot;\nlink_flags        := -pthread -fopenmp -Wl,-rpath=&#39;$$ORIGIN&#39;\n\ncpp_compile_flags += $(include_paths)\ncu_compile_flags  += $(include_paths)\nlink_flags        += $(library_paths) $(link_librarys) $(run_paths)\n\nifneq ($(MAKECMDGOALS), clean)\n-include $(cpp_mk) $(cu_mk)\nendif\n\npro\t   := workspace/pro\nexpath := library_path.txt\n\nlibrary_path.txt : \n    @echo LD_LIBRARY_PATH=$(export_path):&quot;$$&quot;LD_LIBRARY_PATH &gt; $@\n\nworkspace/pro : $(cpp_objs) $(cu_objs)\n        @echo Link $@\n        @mkdir -p $(dir $@)\n        @$(cc) $^ -o $@ $(link_flags)\n\nobjs/%.cpp.o : src/%.cpp\n    @echo Compile CXX $&lt;\n    @mkdir -p $(dir $@)\n    @$(cc) -c $&lt; -o $@ $(cpp_compile_flags)\n\nobjs/%.cu.o : src/%.cu\n    @echo Compile CUDA $&lt;\n    @mkdir -p $(dir $@)\n    @$(nvcc) -c $&lt; -o $@ $(cu_compile_flags)\n\nobjs/%.cpp.mk : src/%.cpp\n    @echo Compile depends CXX $&lt;\n    @mkdir -p $(dir $@)\n    @$(cc) -M $&lt; -MF $@ -MT $(@:.cpp.mk=.cpp.o) $(cpp_compile_flags)\n    \nobjs/%.cu.mk : src/%.cu\n    @echo Compile depends CUDA $&lt;\n    @mkdir -p $(dir $@)\n    @$(nvcc) -M $&lt; -MF $@ -MT $(@:.cu.mk=.cu.o) $(cu_compile_flags)\n\nrun   : workspace/pro\n          @cd workspace &amp;&amp; ./pro\n\nclean :\n    @rm -rf objs workspace/pro\n    @rm -rf library_path.txt\n    @rm -rf workspace/Result.jpg\n\n# 导出符号，使得运行时能够链接上\nexport LD_LIBRARY_PATH:=$(export_path):$(LD_LIBRARY_PATH)\n\n\n\n\n\n\nWARNING\n源码修改，在main.cpp中进行添加与改动\n\ncppstatic void yolo_video_demo(const string&amp; engine_file)&#123;\t\t// 修改1 新增函数\n  auto yolo = yolo::load(engine_file, yolo::Type::V8);\n  if (yolo == nullptr)  return;\n  \n  // auto remote_show = create_zmq_remote_show();\n\n  cv::Mat frame;\n  cv::VideoCapture cap(0);\n  if (!cap.isOpened())&#123;\n    printf(&quot;Engine is nullptr&quot;);\n    return;\n  &#125;\n\n  while(true)&#123;\n    cap.read(frame);\n    auto objs = yolo-&gt;forward(cvimg(frame));\n    \n    for(auto &amp;obj : objs) &#123;\n      uint8_t b, g, r;\n      tie(b, g, r) = yolo::random_color(obj.class_label);\n      cv::rectangle(frame, cv::Point(obj.left, obj.top), cv::Point(obj.right, obj.bottom),\n                    cv::Scalar(b, g, r), 5);\n      \n      auto name = mylabels[obj.class_label];\n      auto caption = cv::format(&quot;%s %.2f&quot;, name, obj.confidence);\n      int width = cv::getTextSize(caption, 0, 1, 2, nullptr).width + 10;\n      cv::rectangle(frame, cv::Point(obj.left - 3, obj.top - 33),\n                    cv::Point(obj.left + width, obj.top), cv::Scalar(b, g, r), -1);\n      cv::putText(frame, caption, cv::Point(obj.left, obj.top - 5), 0, 1, cv::Scalar::all(0), 2, 16);\n    &#125;\n      imshow(&quot;frame&quot;, frame);\n      // remote_show-&gt;post(frame);\n      int key = cv::waitKey(1);\n      if (key == 27)\n          break;\n  &#125;\n\n  cap.release();\n  cv::destroyAllWindows();\n  return;\n&#125;\n\nint main() &#123;\t// 修改2 调用该函数\n  // perf();\n  // batch_inference();\n  // single_inference();\n  yolo_video_demo(&quot;best.transd.sim.engine&quot;);\n  return 0;\n&#125;识别效果肉眼估计能有个10几20来帧，对于Jetson Nano这种老硬件来说已经挺不错的了。\n\n\nshow more\n\n\n","slug":"Jetson-Nano部署Yolo模型笔记","date":"2024-07-22T15:34:15.000Z","categories_index":"Electronic","tags_index":"jetson nano,ComputerVision","author_index":"PIKO"},{"id":"4e554f9d434ef086687ddd88dc6d7f86","title":"树莓派搭建MC服务器","content":"树莓派搭建MC服务器硬件配置树莓派4B：树莓派系统\n下载java根据你希望搭建的MC版本来选择java版本\n\n\n\n\n\n\n\nWARNING\njava8 : MC &lt;&#x3D; 1.16.5java17: MC &gt;&#x3D; 1.16.5\n\n搭建MCSmanager面板我们使用MCSmanager面板来对我们的服务器进行管理\nbashwget -qO- https://gitee.com/mcsmanager/script/raw/master/setup.sh | sudo bash启动面板\nbash\n# 先启动面板守护进程。\n# 这是用于进程控制，终端管理的服务进程。\nsystemctl start mcsm-daemon.service\n# 再启动面板 Web 服务。\n# 这是用来实现支持网页访问和用户管理的服务。\nsystemctl start mcsm-web.service\n\n# 重启面板命令\nsystemctl restart mcsm-daemon.service\nsystemctl restart mcsm-web.service\n\n# 停止面板命令\nsystemctl stop mcsm-web.service\nsystemctl stop mcsm-daemon.service\n\n# 面板启用开机自启的命令\nsystemctl enable mcsm-web.service\nsystemctl enable mcsm-daemon.service\n\n# 面板禁用开机自启的命令\nsystemctl disable mcsm-web.service\nsystemctl disable mcsm-daemon.service进入面板在同一局域网的电脑上输入树莓派服务器ip:23333端口就可以进入面板，按照实例教程就可以进行服务器的创建。\n未完待续","slug":"树莓派搭建MC服务器","date":"2024-04-20T01:07:29.000Z","categories_index":"game","tags_index":"raspberrypi","author_index":"PIKO"},{"id":"0c01823a69be2694393fe22da12fd1e6","title":"Zerotier赋能Parsec——低延迟高分辨率远程办公解决方案","content":"前言用过todesk或向日葵免费版的都知道，虽然他俩可以实现远程连接，但是延迟和画质真的不敢恭维，本文提供了一个解决方案，适合愿意折腾的windows用户实现高分辨率低延迟的远程办公，最好手里正好有云服务器。\n准备环境购买云服务器可以选择阿里云或腾讯云，国内这两家最大，1核1G就够，本方案主要图的是公网ip，对服务器带宽没有要求。如果额外有建站需求的可以考虑Amazon等的香港云服务器（不用备案）。为了追求更低的延迟可以选择距离自己比较近的云服务器。尽量选择Ubuntu或Debian镜像，如选其他镜像，命令可能会有所出入。\n安装ZerotierZerotier的基础配置\n\n\n\n\n\nTIP\n参考本博主以前的文章\n\n\n\n\n\n\n\n\n\nWARNING\n在需要远程的设备中都要下载\n\n\nZerotier Moon服务器搭建云服务器终端输入如下命令\n安装Zerotier\nbashcurl -s https://install.zerotier.com | sudo bash 生成 moon.json 签名文件\nbashzerotier-idtool initmoon /var/lib/zerotier-one/identity.public &gt;&gt; /var/lib/zerotier-one/moon.json编辑moon.json\nbashnano /var/lib/zerotier-one/moon.json将你的公网IP添加到”stableEndpoints”: [] 中，如 “stableEndpoints”: [ “132.313.11.313&#x2F;9993”] ，服务器9993端口防火墙也要打开。\n生成 .moon 签名文件(0000xxxxxxxxx.moon)\nbashzerotier-idtool genmoon moon.json创建moons.d存放.moon文件\nbashmkdir /var/lib/zerotier-one/moons.d\nmv /var/lib/zerotier-one/0000xxxxxx.moon /var/lib/zerotier-one/moons.d重启zerotier服务\nbashsystemctl restart zerotier-one.service查看Moon服务器id\nbashzerotier-cli info 找到Moon结尾的那一行，对应的就是Moon服务器id，也可以去zerotier center直接查看member\nmoon服务器搭建完毕，接下来要其他端接入moon服务器。\nwindows端cmd中输入命令\n将设备接入Moon服务器\nbash zerotier-cli.bat orbit &lt;Moon服务器的id&gt; &lt;Moon服务器的id&gt;Moon中转服务器搭建完成\n如果上述方法不行，还可以将.moon文件从云服务器上下载下来，去zerotier安装目录，windows默认C:\\ProgramData\\ZeroTier\\One，创建moons.d文件夹，将.moon文件移动进去再重启zerotier服务也可实现接入moon服务器。\n更换节点控制器zerotier的虚拟局域网实际上的运作如图正常情况下，终端设备A通过Planet根服务器与终端设备B连接，但是节点是有唯一节点ID的，一个虚拟局域网要确认节点ID才能保证A能连接到B。这个时候就需要节点控制器来告诉Planet根服务器，A提供的虚拟ip对应的节点ID是B的，这样A才能连接到B。有时候，你搭建的Moon服务器不一定能连接到节点控制器，此时就算A、B都连接到Moon服务器，Moon中转也是走不通的。为了解决上面这种情况，我们可以将Moon服务器本身变成节点控制器，让它中转设备的同时可以解析节点。\n这里用到一个开源项目ztncui\n在云服务器端输入命令安装deb\nbashcurl -O https://s3-us-west-1.amazonaws.com/ket-networks/deb/ztncui/1/x86_64/ztncui_0.8.14_amd64.deb\nsudo apt install ./ztncui_0.8.14_amd64.deb查看zerotier密钥\nbashsudo cat /var/lib/zerotier-one/authtoken.secret复制cat的内容到下面单引号’ ‘内\nbashsh -c &quot;echo ZT_TOKEN=&#39;sudo cat /var/lib/zerotier-one/authtoken.secret&#39;&gt;/opt/key-networks/ztncui/.env&quot;bashsh -c &quot;echo HTTP_ALL_INTERFACES=yes &gt;&gt; /opt/key-networks/ztncui/.env&quot;bashsh -c &quot;echo NODE_ENV=production &gt;&gt; /opt/key-networks/ztncui/.env&quot;设置.env的权限\nbashchmod 400 /opt/key-networks/ztncui/.env更改.env的所有者\nbashchown ztncui.ztncui /opt/key-networks/ztncui/.env启动ztncui\nbashsystemctl enable ztncui重启ztncui\nbashsystemctl restart ztncui在服务器防火墙处打开3000端口，在浏览器中输入公网ip:3000，打开云服务器的3000端口，显示如图即为成功右上角login，默认账户admin，密码password\n创建新网络\neasy setup 快速配置\n选择网段，submit\n将设备加入网络\n如果需要保证这个管理界面安全性可以去上SSL证书。\n安装parsec\n\n\n\n\n\nTIP\nparsec官网\n\n\n需要在你希望远程办公的设备上都下载客户端(只支持windows用户)\n打开如下界面，如果配完网出现要远程的设备ID就是成功了\n","slug":"Zerotier赋能Parsec-低延迟高分辨率远程办公Solution","date":"2024-03-06T07:12:15.000Z","categories_index":"IT","tags_index":"solution","author_index":"PIKO"},{"id":"913a88b1308a2ce8d0499d200801961e","title":"Zerotier——内网穿透神器 for free","content":"Zerotier简介zerotier是一个异地组网的solution，可以进行内网穿透从而实现远程办公，将公司、学校、家里的设备牢牢控制在自己手心里。\nZerotier安装\n\n\n\n\n\nTIP\nzerotier官网\n\n\nzerotier center配置在zerotier官网打开Get ZeroTier登录账户后会显示如下界面,点击Create A Network新建一个虚拟局域网Base就是一些基础信息，要记住Network ID，这之后其他设备接入该虚拟局域网要用比较重要的是这个member，你之后往zerotier这个虚拟局域网里加入其他设备需要在这里授权如果有中转服务器，需要在这里添加路由，这样才能使你的设备连接通过这个中转服务器\nzerotier 常用命令(Linux)1.安装zerotier\nbashcurl -s https://install.zerotier.com | sudo bash 2.启动Zerotier\nbashsudo systemctl start zerotier-one.service 3.重启Zerotier\nbashsudo systemctl restart zerotier-one.service 4.设置开机自启动Zerotier\nbashsudo systemctl enable zerotier-one.service 5.查看服务状态\nbashsudo zerotier-cli status   or\nbashsudo zerotier-cli info6.加入网络 \nbashzerotier-cli join &lt;16位虚拟网络ID&gt;7.离开网络 \nbashzerotier-cli leave &lt;16位虚拟网络ID&gt;8.查看所有的网络 \nbashzerotier-cli listnetworks 9.显示详细信息\nbashsudo zerotier-cli listpeers10.授权设备\nbashsudo zerotier-cli orbit &lt;16位虚拟网络ID&gt; &lt;16位要授权的设备的唯一标识符&gt;zerotier使用(windows)拥有图形界面，是个人应该都会用。\n简单介绍一下如何添加设备到你的虚拟局域网\n点击join new network,然后输入你的虚拟网络ID(16位码)，在去zerotier center确认授权即可\n\n\n\n\n\n\n\nWARNING\nzerotier windows端下载好了之后会自动缩到任务栏，要在任务栏操作\n\n\nzerotier 效果可以通过zerotier虚拟局域网下提供的虚拟ip地址去进行不同设备之间的通信，从而实现内网穿透。如下我配置了两个设备进入我的虚拟局域网，这两个设备现实中不在同一个局域网内在cmd中ping另个设备可以看到已经可以ping通了，延迟还可以，其实是我配了中转服务器的缘故正常情况如果不能P2P直连的话就会走zerotier的免费planet根服务器，但是zerotier的planet根服务器是部署在国外的，这一来一去延迟就会超级高，大概500ms往上，几乎到了无法使用的地步，因此强烈建议搭一个zerotier的moon中转服务器（我之后会再出教程如何配置zerotier的moon中转服务器，并实现高分辨率低延迟远程办公）。\n","slug":"Zerotier——内网穿透神器-for-free","date":"2024-03-05T15:26:29.000Z","categories_index":"IT","tags_index":"tool","author_index":"PIKO"},{"id":"7ef338d136f909941106f7733815b8c9","title":"基于运放的信号发生电路总结","content":"前言最近在导师那里接了一个做蓄电池检测仪的项目，在测量电池内阻时需要向电池打一个稳定的小正弦交流，因此开始研究如何使用运算放大器实现正弦信号的发生和最终的电流输出。\n正弦信号发生电路正弦信号作为最基本的测试信号、参考信号和载波信号而被广泛使用。尽管正弦波本身十分简单，但是产生正弦波的电路可没有那么简单。使用运算放大器来搭建正弦信号发生电路最适合的就是文氏电桥振荡器和正交振荡器。下面将给出二者的实际电路实现.\n文氏电桥振荡电路基本文氏电桥振荡器\n\n\n\n\n\n\n\n\nTIP\n\n虚短虚断之后就是一个电桥电路\n运放正反馈放大信号经过RC高通滤波和RC低通滤波构成的带通滤波器，达到选频的效果，通过f&#x3D;1&#x2F;2πRC，计算出最后的通频段。\n运放负反馈≈同向放大器，将每次经过选频之后的信号进行放大，最后稳定。\n巴克豪森准则：当电子振荡器系统的信号从输入到输出再到反馈输入的相位差为360度，并且增益为1时，这是振荡器振荡的必要条件。由此可以得到文氏电桥振荡电路的起振条件: R3&#x2F;R4 &#x3D; 2 (有点偏差没问题，&#x3D;2时振荡效果最好)\n\n\n\n仿真结果\n\n\n\n\n\n\n\nWARNING\n通道1为输出Vo通道2为运放同向端Vp大概可以得到Vo&#x2F;Vp &#x3D; 3起振时间大约100ms产生正弦波频率：f&#x3D;1&#x2F;(2π158kΩ1nF)≈1kHz\n\n\n拓展文氏电桥振荡器\n\n\n\n\n\n\nTIP\n采用二级管控制负反馈端的电阻阻值稳定，保证生成正弦波的稳定性\n\n\n\n\n\n\n\n\n\nTIP\n利用限幅器来稳定幅度的文氏电桥振荡电路\n\n\n正交振荡器\n嗨嗨嗨\n待更新~~~\n\n\n","slug":"基于运放的信号发生电路总结","date":"2024-01-18T13:46:56.000Z","categories_index":"Electronic","tags_index":"design","author_index":"PIKO"},{"id":"150ed4b39e17f0239763704a3281f203","title":"基于pytorch的简单BP神经网络搭建","content":"前言神经网络(个人理解)神经网络本质就是权重，就是计算机在那算数，计算你输入的值经过网络迭代后得到另一个值，就很像函数对应一样，但是能进行推理，输入定义域以外的数，它能给你推理出来一个接近的解。喂数据可以提高网络的精确性，本质也是让它改变更新权重（改变对应法则），让网络更倾向去得到你想要的值。\n最基本神经网络结构由三部分构成：输入层、隐藏层、输出层这三部分之间是存在逻辑关系的，人为指定，一般比较简单，无非就是一些简单的函数变换，因为每步的计算简单，所以需要占用大量的算力，适合使用GPU。\nBP算法(个人理解)前向传播：完成从输入到隐藏在到输出的过程，由输入得到输出后向传播：完成从输出到输入的过程，由误差更新每一步的权重\npython代码实现train.pypython############################################\n#               简单训练模块                #\n############################################\n####一定要在train.py目录下创建weight文件夹####\n\nimport os\nfrom sys import path as sys_path\n\nscript_path = os.path.dirname(os.path.abspath(__file__)) # 获取当前脚本的绝对路径\nparent_path = os.path.dirname(os.path.dirname(script_path)) # 获取当前脚本的上级目录\nsys_path.append(parent_path) # 将上级目录加入到系统路径中\n\nfrom torch.utils.data import Dataset, DataLoader # 导入数据集和数据加载器\nimport torch\nimport torch.nn as nn # 导入神经网络模块\nfrom torch.optim import SGD # 导入随机梯度下降优化器\nimport time # 导入时间模块\nimport matplotlib.pyplot as plt # 导入绘图模块\n\n##################################################################################################\n\n# 创建输入数据,手敲数据集\n\nx = [[1,2],[3,4],[5,6],[6,7],[8,9],[10,11],[10,12],[11,13],[12,13],[14,15],[16,17],[18,19]] \ny = [[3],[7],[11],[13],[17],[21],[22],[24],[25],[29],[33],[37]] \n\nX = torch.tensor(x).float() # 将x转换成张量\nY = torch.tensor(y).float() # 将y转换成张量\n\ndevice = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;  # 判断是否有cuda\n\nX = X.to(device)  # 将x放到cuda上\nY = Y.to(device)  # 将y放到cuda上\n\n##################################################################################################\n# 创建神经网络的类\nclass MyNeuralNetwork(nn.Module):\n    def __init__(self):\n        super(MyNeuralNetwork, self).__init__()\n        # 创建神经网络的层就是直接调用一些nn提供的函数来实现的\n        self.input_to_hidden_layer = nn.Linear(2, 10) # 输入层到隐藏层 nn.Linear(2,10)表示输入层有2个神经元，隐藏层有10个神经元\n        self.hidden_to_output_layer = nn.Linear(10, 1) # 隐藏层到输出层\n        self.hidden_layer_activation = nn.ReLU() # 激活函数\n\n    def forward(self, x):\n        # 前向传播 表达层与层之间的逻辑关系\n        x = self.input_to_hidden_layer(x) # 输入层到隐藏层\n        x = self.hidden_layer_activation(x) # 激活函数\n        x = self.hidden_to_output_layer(x) # 隐藏层到输出层\n        return x\n    \n# 创建数据集类\nclass MyDataset(Dataset):\n    def __init__(self, x, y): # 初始化数据集\n        self.x = torch.tensor(x).float() # 将x转换成张量\n        self.y = torch.tensor(y).float() # 将y转换成张量\n        self.len = len(x) #数据集的长度\n    def __getitem__(self, index): #获取数据集中的一条数据\n        return self.x[index], self.y[index] #返回数据\n    def __len__(self): #获取数据集的长度\n        return self.len\n    \n##################################################################################################    \n# 实例化神经网络\n\n\ntorch.manual_seed(0) # 设置随机数种子\n\nmynet = MyNeuralNetwork().to(device) # 创建神经网络\nprint(mynet.input_to_hidden_layer.weight) # 查看输入层到隐藏层的权重\n\nfor par in mynet.parameters(): # 查看神经网络的参数\n    print(par)\n\ndataset = MyDataset(X, Y) # 实例化数据集 \ndataloader = DataLoader(dataset, batch_size=2, shuffle=True) # 实例化数据加载器 \n# 从dataset加载 batch_size=2表示每次加载2条数据，shuffle=True表示打乱数据\nfor data in dataloader: # 从dataloader中循环取出数据\n    print(data) # 打印数据\n\n##################################################################################################\n# 创建损失函数和优化器\n\nloss_func = nn.MSELoss() # 均方误差损失函数 \n\noptimizer = SGD(mynet.parameters(), lr=0.0015) # 随机梯度下降优化器 lr 学习率\n\n# optimizer.zero_grad() # 梯度清零\n# loss_value = loss_func(mynet(X), Y) # 计算损失\n# loss_value.backward() # 反向传播\n# optimizer.step() # 更新权重\n\n##################################################################################################\n\nloss_history = [] # 创建一个空列表，用于保存损失值\ntrain_epoch = 10000 # 训练次数\n\nstart = time.time() # 记录开始时间\nfor epoch in range(train_epoch):\n    for data in dataloader: # 从dataloader中循环取出数据\n        x, y = data\n        optimizer.zero_grad() # 梯度清零\n        loss_value = loss_func(mynet(x), y) # 计算损失\n        loss_value.backward() # 反向传播\n        optimizer.step() # 更新权重\n        loss_history.append(loss_value.item()) # 将损失值添加到列表中\n    if epoch % 100 == 0 and epoch &lt; train_epoch - 1: \n            plt.ion() # 打开交互模式\n            plt.plot(loss_history) # 绘制损失曲线\n            plt.title(&#39;Loss over the increasing number of epoch&#39;) # 设置标题\n            plt.xlabel(&#39;Epoch&#39;) # 设置x轴标签\n            plt.ylabel(&#39;Loss&#39;) # 设置y轴标签\n            plt.pause(0.1) # 暂停0.1秒\n    elif epoch == train_epoch - 1:\n            plt.ioff() # 关闭交互模式\n            plt.show() # 显示图像\nend = time.time() # 记录结束时间\nprint(&quot;训练时间：&quot;, end - start) # 打印训练时间\n\nval_x = [[13,19]] # 1*2的list\nval_x = torch.tensor(val_x).float().to(device) # 将val_x转换成张量\nprediction = mynet(val_x) # 预测val_x的值\nprediction_int = round(prediction.item()) # 将预测值转换成整数\nprint(&quot;&#123;&#125; + &#123;&#125; = &quot;.format(val_x[0][0],val_x[0][1]) + str(prediction_int)) # 打印预测值\n\n# 获取train.py的路径\nscript_dir = os.path.dirname(os.path.abspath(__file__))\n\n# 构造模型文件的路径\ndirectory = os.path.join(script_dir, &#39;weight&#39;)\n\n# 如果目录不存在，创建它\nif not os.path.exists(directory):\n    os.makedirs(directory)\n\n# 保存模型参数\ntorch.save(mynet.state_dict(), os.path.join(directory, &#39;BP_Pytorch.pth&#39;)) # 保存模型参数是以当前路径为参考路径的\n\n\nval.pypython\nimport torch\nimport torch.nn as nn\nfrom torchsummary import summary\n\nclass MyNeuralNetwork(nn.Module):\n    def __init__(self):\n        super(MyNeuralNetwork, self).__init__()\n        # 创建神经网络的层就是直接调用一些nn提供的函数来实现的\n        self.input_to_hidden_layer = nn.Linear(2, 10) # 输入层到隐藏层 nn.Linear(2,10)表示输入层有2个神经元，隐藏层有10个神经元\n        self.hidden_to_output_layer = nn.Linear(10, 1) # 隐藏层到输出层\n        self.hidden_layer_activation = nn.ReLU() # 激活函数\n\n    def forward(self, x):\n        # 前向传播 表达层与层之间的逻辑关系\n        x = self.input_to_hidden_layer(x) # 输入层到隐藏层\n        x = self.hidden_layer_activation(x) # 激活函数\n        x = self.hidden_to_output_layer(x) # 隐藏层到输出层\n        return x\n\nmodel = MyNeuralNetwork().to(&#39;cuda&#39;) # 创建神经网络\n\nstatic_dict = torch.load(&#39;BP_Pytorch.pth&#39;)\n\nmodel.load_state_dict(static_dict) # 加载模型参数 保证模型参数一致即可，不需要保证模型结构一致\n#类创建的神经网络和nn.Squential（序贯）创建的神经网络的参数保存和加载方式不同，二者不能混用\n\nsummary(model, input_data=(1,2)) # 打印模型结构\n\nprint(&quot;请输入两个数，我来帮你计算它们的和：&quot;)\nx1 = float(input(&#39;请输入第一个数：&#39;))\nx2 = float(input(&#39;请输入第二个数：&#39;))\n\nval_x = [[x1,x2]] #创建列表\nval_x = torch.tensor(val_x).float().to(&#39;cuda&#39;)\nprediction = model(val_x) # 预测val_x的值\nprediction_int = round(prediction.item()) # 将预测值转换成整数\nprint(&quot;&#123;&#125; + &#123;&#125; = &quot;.format(val_x[0][0],val_x[0][1]) + str(prediction.item())) # 打印预测值\n\n","slug":"基于pytorch的简单BP神经网络搭建","date":"2023-11-21T15:56:50.000Z","categories_index":"AI","tags_index":"ANN","author_index":"PIKO"},{"id":"c7f2a0b48c2c1da08799955658fe960e","title":"计算方法——一些算法的python实现","content":"前言本文是基于python实现的计算方法课程的算法，仅实数域内计算。部分算法提供迭代曲线的绘制（基于matplotlib），可以在控制台打印每一步的迭代结果\nGitHub库\nSolution_for_Nonlinear_algebra_equation二分法bash\nimport math\nimport matplotlib.pyplot as plt\n\n##############################################################################################################\n# name: Dichotomy_method\n# function: 二分法\n# parameter: f: 函数\n#            a: 区间左端点\n#            b: 区间右端点\n#            tol: 精度要求\n#            maxiter: 最大迭代次数\n#            info: 是否输出迭代信息\n#            plot: 是否输出迭代曲线\n# return: x1: 迭代结果，如果无法判断是否有根则返回None\n#         epoch: 迭代次数，如果无法判断是否有根则返回None\n# date: 2023/11/21\n# author: Yesord\n# note 需要确定区间内有且仅有一个根\n##############################################################################################################\n\ndef Dichotomy_method(f, a, b, tol, maxiter, info=False, plot=False): \n    x0 = a\n    x1 = b\n    if f(a) * f(b) &gt; 0:\n        print(&quot;Error: f(&#123;&#125;) * f(&#123;&#125;) &gt; 0, we can&#39;t ensure that there is root in the interval [&#123;&#125;, &#123;&#125;]&quot;.format(a, b, a, b))\n        # %操作符用于格式化字符串\n        return None, None\n    if plot:\n        x_history = []\n        epoch_history = []\n    for epoch in range(maxiter):\n        x1 = (a + b) / 2\n        if f(x1) * f(a) &lt; 0:\n            b = x1\n        else:\n            a = x1\n        if info:\n            print(&quot;epoch = %d, x1 = %f&quot; % (epoch, x1))\n        if plot:# 绘制迭代过程\n            plt.ion()\n            plt.xlabel(&quot;epoch&quot;)\n            plt.ylabel(&quot;x&quot;)\n            plt.title(&quot;Dichotomy method&quot;)\n            plt.plot(epoch_history, x_history, color=&#39;r&#39;, linestyle=&#39;-&#39;)\n            plt.pause(0.1)\n        if abs(x1 - x0) &lt; tol:\n            plt.ioff()\n            plt.show()\n            return x1, epoch\n        epoch_history.append(epoch)\n        x_history.append(x1)\n        x0 = x1\n\ndef Dichotomy_method_convergence_judgment(f, a, b):\n    if f(a) * f(b) &gt; 0:\n        print(&quot;Error: f(%f) * f(%f) &gt; 0, we can&#39;t ensure that there is root in the interval [%f, %f]&quot; % (a, b, a, b))\n        # format()函数用于格式化字符串\n        print(&quot;Dichotomy method isn&#39;t recommended.&quot;)\n\n# 测试\nif __name__ == &quot;__main__&quot;: # __name__是当前模块名，当模块被直接运行时模块名为__main__。没有被直接运行时模块名为文件名\n    print(&quot;\\n*****************************************************************&quot;)\n    print(&quot;Dichotomy method test start...&quot;)\n    print(&quot;*****************************************************************\\n&quot;)\n    def f(x):\n        y = x**2 - 0.2\n        return y\n    a = -1\n    b = 1\n    tol = 1e-5\n    maxiter = 1000\n    x, epoch = Dichotomy_method(f, a, b, tol, maxiter, info=True, plot=True)\n    if x is None or epoch is None:\n        print(&quot;Dichotomy_method did not return valid values.&quot;)\n    else:\n        print(&quot;\\nx = %f, epoch = %d\\n&quot; % (x, epoch))\n    print(&quot;\\n*****************************************************************&quot;)\n    print(&quot;Dichotomy method test end...&quot;)\n    print(&quot;*****************************************************************\\n&quot;)\n迭代法bash\nimport math\nimport matplotlib.pyplot as plt\n\n##############################################################################################################\n# name: iterative_method\n# function: 迭代法\n# parameter: g: 迭代函数\n#            x0: 初始值\n#            tol: 精度要求\n#            maxiter: 最大迭代次数\n#            info: 是否输出迭代信息\n#            plot: 是否输出迭代曲线\n# return: x1: 迭代结果\n#         epoch: 迭代次数\n# date: 2023/11/21\n# author: Yesord\n# note 需要人工计算得到迭代函数以及验证是否收敛\n##############################################################################################################\n\n# 迭代法\ndef iterative_method(g, x0, tol, maxiter, info=False, plot=False):\n    if plot:\n        x_history = []\n        epoch_history = []\n    for epoch in range(maxiter):\n        x1 = g(x0)\n        if info:\n            print(&quot;epoch = %d, x1 = %f&quot; % (epoch, x1))\n        if plot:# 绘制迭代过程\n            plt.ion()\n            plt.xlabel(&quot;epoch&quot;)\n            plt.ylabel(&quot;x&quot;)\n            plt.title(&quot;Iterative method&quot;)\n            plt.plot(epoch_history, x_history, color=&#39;r&#39;, linestyle=&#39;-&#39;)\n            plt.pause(0.1)\n        if abs(x1 - x0) &lt; tol:\n            plt.ioff()\n            plt.show()\n            return x1, epoch\n        epoch_history.append(epoch)\n        x_history.append(x1)\n        x0 = x1\n\n\n\n\n# 测试\n\n\n\nif __name__ == &quot;__main__&quot;: \n    print(&quot;\\n*****************************************************************&quot;)\n    print(&quot;Iterative method test start...&quot;)\n    print(&quot;*****************************************************************\\n&quot;)\n    def g(x): # 迭代函数\n        y = x - (x**3 - 2*x**2 + 3*x - 1)/(3*x**2 - 4*x + 3)\n        return y\n    x0 = float(input(&quot;Please input the initial value x0: &quot;))\n    tol = 1e-5\n    maxiter = 1000\n    x, epoch = iterative_method(g, x0, tol, maxiter, info=True, plot=True)\n    print(&quot;\\nx = %f, epoch = %d\\n&quot; % (x, epoch))\n    print(&quot;*****************************************************************&quot;)\n    print(&quot;Iterative method test end...&quot;)\n    print(&quot;*****************************************************************\\n&quot;)\n    牛顿法bash\nimport math\nimport matplotlib.pyplot as plt\n\n##############################################################################################################\n# name: Newton_method\n# function: 牛顿法\n# parameter: f: 函数\n#            df: f的导函数\n#            x0: 初始值\n#            tol: 精度要求\n#            maxiter: 最大迭代次数\n#            info: 是否输出迭代信息\n#            plot: 是否输出迭代曲线\n# return: x1: 迭代结果\n#         epoch: 迭代次数\n# date: 2023/11/21\n# author: Yesord\n# note 需要人工计算得到导函数以及验证是否收敛\n##############################################################################################################\n\ndef Newton_method(f, df, x0, tol, maxiter, info=False, plot=False):\n    if df == None:# 如果df没有定义，则使用数值微分\n        df = lambda f, x: (f(x+1e-6) - f(x-1e-6)) / (2e-6) # lambda x: df(f, x)是一个匿名函数，等价于def df(x): return df(f, x)\n    if plot:\n        x_history = []\n        epoch_history = []\n    for epoch in range(maxiter):\n        x1 = x0 - f(x0)/df(f, x0)\n        if info:\n            print(&quot;epoch = %d, x1 = %f&quot; % (epoch, x1))\n        if plot:# 绘制迭代过程\n            plt.ion()\n            plt.xlabel(&quot;epoch&quot;)\n            plt.ylabel(&quot;x&quot;)\n            plt.title(&quot;Newton method&quot;)\n            plt.plot(epoch_history, x_history, color=&#39;r&#39;, linestyle=&#39;-&#39;)\n            plt.pause(0.1)\n        if abs(x1 - x0) &lt; tol: # 精度达到要求输出结果\n            plt.ioff()\n            plt.show()\n            return x1, epoch\n        epoch_history.append(epoch) \n        x_history.append(x1)\n        x0 = x1\n\n# 收敛性判断\ndef Newton_method_convergence_judgment(f, df, x0):\n    if df(f, x0) == 0:\n        print(&quot;Error: df(%f) = 0, Newton method isn&#39;t recommended.&quot; % x0)\n\n\n# 测试\nif __name__ == &quot;__main__&quot;: \n    f = lambda x: math.pow(x,3) - 2*math.pow(x,2) + 3*x - 4 # sample function\n    df = lambda f, x: (f(x+1e-6) - f(x-1e-6)) / (2e-6) # lambda f,x :用于定义匿名函数，冒号前面的x表示函数参数，冒号后面的x表示函数返回值\n    print(&quot;\\n*****************************************************************&quot;)\n    print(&quot;Newton method test start...&quot;)\n    print(&quot;*****************************************************************\\n&quot;)\n    print(&quot;test function: f(x) = x**3 - 2*x**2 + 3*x - 4&quot;)\n    #输出x0初值并将其转换成浮点数\n    x0 = float(input(&quot;Please input the initial value x0: &quot;)) \n    tol = 1e-6 # 精度要求\n    maxiter = 100 # 最大迭代次数\n    x, epoch = Newton_method(f, df, x0, tol, maxiter, info=True, plot=True) # Netwon method\n    print(&quot;x = %f, epoch = %d&quot; % (x, epoch))\n    Newton_method_convergence_judgment(f, df, x0) # 收敛性判断\n    print(&quot;\\n*****************************************************************&quot;)\n    print(&quot;Newton method test done!&quot;)\n    print(&quot;*****************************************************************\\n&quot;)\n\nIterative_method_for_Linear_algebra_equations雅可比迭代法bash\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# jacobi迭代法\n##############################################################################################################\n# name: jacobi_iterative_method\n# function: jacobi迭代法\n# parameter: A: 系数矩阵\n#            b: 常数项\n#            x0: 初始值\n#            tol: 精度要求\n#            max_iter: 最大迭代次数\n#            info: 是否输出迭代信息\n#            plot: 是否输出迭代曲线\n# return: x1: 迭代结果\n#         epoch: 迭代次数\n# date: 2023/11/20\n# author: Yesord\n##############################################################################################################\n\ndef jacobi_iterative_method(A, b, x0, tol, max_iter, info=False, plot=False):\n    A = np.array(A) # 转换为numpy数组\n    b = np.array(b) # 转换为numpy数组\n    x0 = np.array(x0) # 转换为numpy数组\n    n = len(A) # 方程组的阶数\n    x1 = np.zeros(n) # 初始化x1\n    epoch = 0 # 迭代次数\n    if plot:\n        x1_history = []\n        x2_history = []\n        x3_history = []\n        epoch_value = []\n    for epoch in range(max_iter): # 迭代\n        for i in range(n):\n            x1[i] = b[i]\n            for j in range(n):\n                if i != j: # 不是对角元素\n                    x1[i] -= A[i][j] * x0[j]\n            x1[i] /= A[i][i] \n\n        if info: # 输出迭代信息\n            print(&quot;x^= &quot;, x1) # 输出结果\n            print(&quot;epoch= &quot;, epoch) # 输出迭代次数\n\n        if plot: # 输出迭代曲线\n            \n            x1_history.append(x1[0])\n            x2_history.append(x1[1])\n            x3_history.append(x1[2])\n            epoch_value.append(epoch)\n\n            plt.ion() # 打开交互模式\n            plt.xlabel(&quot;epoch&quot;) # 设置x轴标签\n            plt.ylabel(&quot;x_iterative&quot;) # 设置y轴标签\n            plt.title(&quot;jacobi iterative method&quot;) # 设置标题\n\n            line_x1, = plt.plot(epoch_value, x1_history, &#39;r.-&#39;) # 画x1图 r.-表示红色点线图\n            line_x2, = plt.plot(epoch_value, x2_history, &#39;g.-&#39;) # 画x2图 g.-表示绿色点线图\n            line_x3, = plt.plot(epoch_value, x3_history, &#39;b.-&#39;) # 画x3图 b.-表示蓝色点线图\n            \n            plt.pause(0.1)   \n        if np.linalg.norm(x1 - x0) &lt; tol: # 判断是否满足精度要求\n            if plot:\n                line_x1.set_label(&quot;x1&quot;) # 设置图例\n                line_x2.set_label(&quot;x2&quot;) \n                line_x3.set_label(&quot;x3&quot;) \n                plt.legend() # 显示图例\n                plt.ioff() # 关闭交互模式\n                plt.show() # 显示图像\n            break\n        x0 = x1.copy() # 更新x0\n        \n        \n    \n    return x1, epoch # 返回结果\n\n\n# 测试\ndef test():\n    x0 = [0, 0, 0] # 初始值\n    A = [[5, -2, 1], [1, 5, -3], [2, 1, -5]]\n    B = [4, 2, -11] # 常数项\n    tol = 0.5e-3 # 精度要求\n    max_iter = 100 # 最大迭代次数\n    x, epoch = jacobi_iterative_method(A, B, x0, tol, max_iter, info=True, plot=True) # jacobi迭代法\n    return x, epoch # 返回结果\n\n        高斯赛德尔迭代法bash\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# 高斯-赛德尔迭代法\n##############################################################################################################\n# name: GS_iterative_method\n# function: 高斯-赛德尔迭代法\n# parameter: A: 系数矩阵\n#            b: 常数项\n#            x0: 初始值\n#            acurrate: 精度要求\n#            max_iter: 最大迭代次数\n#            info: 是否输出迭代信息\n#            plot: 是否输出迭代曲线\n# return: x1: 迭代结果\n#         epoch: 迭代次数\n# date: 2023/11/20\n# author: Yesord\n##############################################################################################################\ndef GS_iterative_method(A, b, x0, acurrate, max_iter, info=False, plot=False):\n    A = np.array(A) # 转换为numpy数组\n    b = np.array(b) # 转换为numpy数组\n    x0 = np.array(x0) # 转换为numpy数组\n    n = len(A) # 方程组的阶数\n    epoch = 0 # 迭代次数\n    if plot:\n        x1_history = []\n        x2_history = []\n        x3_history = []\n        epoch_value = []\n    x1 = np.zeros(n) # 初始化x1\n    for epoch in range(max_iter):\n        for i in range(n):\n            x1[i] = b[i]\n            for j in range(n):\n                if i != j:\n                    x1[i] -= A[i][j] * x1[j]\n            x1[i] /= A[i][i]\n\n        if info: # 输出迭代信息\n            print(&quot;x^= &quot;, x1) # 输出结果\n            print(&quot;epoch= &quot;, epoch) # 输出迭代次数\n\n        if plot: # 输出迭代曲线\n            \n            x1_history.append(x1[0])\n            x2_history.append(x1[1])\n            x3_history.append(x1[2])\n            epoch_value.append(epoch)\n\n            plt.ion() # 打开交互模式\n            plt.xlabel(&quot;epoch&quot;) # 设置x轴标签\n            plt.ylabel(&quot;x_iterative&quot;) # 设置y轴标签\n            plt.title(&quot;GS iterative method&quot;) # 设置标题\n\n            line_x1, = plt.plot(epoch_value, x1_history, &#39;r.-&#39;) # 画x1图 r.-表示红色点线图\n            line_x2, = plt.plot(epoch_value, x2_history, &#39;g.-&#39;) # 画x2图 g.-表示绿色点线图\n            line_x3, = plt.plot(epoch_value, x3_history, &#39;b.-&#39;) # 画x3图 b.-表示蓝色点线图\n            \n            plt.pause(0.1)  \n        if np.linalg.norm(x1 - x0) &lt; acurrate: # 判断是否满足精度要求\n            if plot:\n                line_x1.set_label(&quot;x1&quot;) # 设置图例\n                line_x2.set_label(&quot;x2&quot;) \n                line_x3.set_label(&quot;x3&quot;) \n                plt.legend() # 显示图例\n                plt.ioff() # 关闭交互模式\n                plt.show() # 显示图像\n            break\n        x0 = x1.copy() # 更新x0\n    return x1, epoch # 返回结果\n\n# 测试\ndef test():\n    x0 = [0, 0, 0] # 初始值\n    A = [[5, -2, 1], [1, 5, -3], [2, 1, -5]] # 系数矩阵\n    B = [4, 2, -11] # 常数项\n    tol = 0.5e-3 # 精度要求\n    max_iter = 100 # 最大迭代次数\n    x, epoch = GS_iterative_method(A, B, x0, tol, max_iter,info=True, plot=True) # GS迭代法\n    return x, epoch # 返回结果\n\nSOC迭代法bash\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# SOC迭代法\n##############################################################################################################\n# name: SOC_iterative_method\n# function: SOC迭代法\n# parameter: A: 系数矩阵\n#            b: 常数项\n#            x0: 初始值\n#            w: 松弛因子\n#            tol: 精度要求\n#            max_iter: 最大迭代次数\n#            info: 是否输出迭代信息\n#            plot: 是否输出迭代曲线,仅支持三元方程组\n# return: x1: 迭代结果\n#         epoch: 迭代次数\n# date: 2023/11/20\n# author: Yesord\n##############################################################################################################\n\ndef SOC_iterative_method(A, b, x0, w, tol, max_iter, info=False, plot=False):\n    A = np.array(A) # 转换为numpy数组\n    b = np.array(b) # 转换为numpy数组\n    x0 = np.array(x0) # 转换为numpy数组\n    n = len(A) # 方程组的阶数\n    x1 = np.zeros(n) # 初始化x1\n    epoch = 0 # 迭代次数\n    if plot and n == 3:\n        x1_history = []\n        x2_history = []\n        x3_history = []\n        epoch_value = []\n\n    for epoch in range(max_iter): # 迭代\n        for i in range(n):\n            x1[i] = b[i]\n            for j in range(n):\n                if i != j: # 不是对角元素\n                    x1[i] -= A[i][j] * x1[j]\n            x1[i] /= A[i][i]\n            x1[i] = (1 - w) * x0[i] + w * x1[i]\n        \n        if info: # 输出迭代信息\n            print(&quot;x^= &quot;, x1) # 输出结果\n            print(&quot;epoch= &quot;, epoch) # 输出迭代次数\n\n        if plot and n == 3: # 输出迭代曲线\n            \n            x1_history.append(x1[0])\n            x2_history.append(x1[1])\n            x3_history.append(x1[2])\n            epoch_value.append(epoch)\n\n            plt.ion() # 打开交互模式\n            plt.xlabel(&quot;epoch&quot;) # 设置x轴标签\n            plt.ylabel(&quot;x_iterative&quot;) # 设置y轴标签\n            plt.title(&quot;SOC iterative method&quot;) # 设置标题\n\n            line_x1, = plt.plot(epoch_value, x1_history, &#39;r.-&#39;) # 画x1图 r.-表示红色点线图\n            line_x2, = plt.plot(epoch_value, x2_history, &#39;g.-&#39;) # 画x2图 g.-表示绿色点线图\n            line_x3, = plt.plot(epoch_value, x3_history, &#39;b.-&#39;) # 画x3图 b.-表示蓝色点线图\n            \n            plt.pause(0.1)  \n                \n        if np.linalg.norm(x1 - x0) &lt; tol: # 判断是否满足精度要求\n            if plot:\n                line_x1.set_label(&quot;x1&quot;) # 设置图例\n                line_x2.set_label(&quot;x2&quot;) \n                line_x3.set_label(&quot;x3&quot;) \n                plt.legend() # 显示图例\n                plt.ioff() # 关闭交互模式\n                plt.show() # 显示图像\n            break\n        x0 = x1.copy() # 更新x0\n        \n    \n    return x1, epoch # 返回结果\n\n# 测试\ndef test():\n    x0 = [0, 0, 0] # 初始值\n    A = [[4,3,0], [3,4,-1], [0,-1,4]] # 系数矩阵\n    B = [16,20,-12] # 常数项\n    w = 1.24 # 松弛因子\n    tol = 1e-3 # 精度要求\n    max_iter = 100 # 最大迭代次数\n    x, epoch = SOC_iterative_method(A, B, x0, w, tol, max_iter,info=True, plot=True) # jacobi迭代法\n    return x, epoch\n","slug":"计算方法——一些算法的python实现","date":"2023-11-20T16:14:56.000Z","categories_index":"","tags_index":"python","author_index":"PIKO"},{"id":"a6d09d63bbb6828b08c842fdfa1f3e69","title":"pytorch环境搭建下遇到的问题","content":"前言总所周知，ai的发展是极度迅速的，ai相关的软件与库的更新速度极快，很容易出现库依赖之间的不兼容，因而基于anaconda下的pytorch环境搭建总是会遇到问题，本博文就总结了我在搭建pytorch环境时所遇到的问题\nVscode下的环境搭建\n\n\n\n\n\nconda报错\n解决Anaconda关联VsCode使用时powershell终端conda报错的问题\n\n\n\n\n\n\n\n\npytorch、cuda、python版本对应\npytorch官网\n\n\n","slug":"pytorch环境搭建下遇到的问题","date":"2023-11-15T13:53:53.000Z","categories_index":"AI","tags_index":"ComputerVision","author_index":"PIKO"},{"id":"7654cddd589d5117806bb3f535f9e67b","title":"滤波电路设计","content":"前言滤波电路是一种用于过滤信号的电路，它将仅传递所需的信号并滤去不需要的信号。通常，滤波电路由无源元件或有源元件设计。\n· 无源元件包括电阻器、电感器和电容器。\n· 有源元件包括BJT、MOSFET和运算放大器。\n\nClick to see more\n低通滤波电路:一种允许频率低于一个确定频率阈值的信号通过，并且阻止高于该确定频率阈值的信号通过的电子电路。低通滤波电路允许低频信号通过，并抑制高频信号。其核心思想是在频率域上通过移除高频成分来平滑信号。这在去噪、平滑和提取基本频率成分时非常有用。\n高通滤波电路:一种允许频率高于一个确定频率阈值的信号通过，并且阻止低于该确定频率阈值的信号通过的电子电路。高通滤波电路允许高频信号通过，并抑制低频信号。它的工作原理与低通滤波电路相反，通过移除低频成分来突出高频特征。\n\n\n\n无源高通滤波电路仅由无源器件构成的通高频阻低频的滤波电路。\n最简单的无源高通滤波电路就是一个电阻+一个电容\n截止频率 $f_L &#x3D; \\frac{1}{2πRC}$ f &#x3D; 1&#x2F;(2πRC),高于 $f_L$ f可通，低于$f_L$ f衰减\n\n\n\n\n\n\nTIP\n此处的通高频阻低频都是相对的，本人习惯认为k量级以下的为低频，G量级以上的为高频。这不唯一，实际电路应根据实际需要的信号进行分析。\n\n\n无源低通滤波电路仅由无源器件构成的通低频阻高频的滤波电路。\n最简单的无源低通滤波电路也就是一个电阻+一个电容\n截止频率 $ f_L &#x3D; \\frac{1}{2πRC} $ f &#x3D; 1&#x2F;(2πRC) ，低于$ f_L $ f可通，高于$ f_L $ f衰减\n感兴趣的读者可以去Multisim上验证\n\n\n\n\n\n\n\nWARNING\n无源滤波通常放大倍数及其截止频率随负载变化，在实际电路中负载会变化会导致无源滤波的参数改变而可能导致信号的失真。\n\n\n有源低通滤波电路由有源器件构成的通低频阻高频的滤波电路。\n介绍基础的有源一阶低通滤波电路\n在无源低通滤波电路中加入高输入阻抗，低输出阻抗的隔离电路（后级电路输入低阻抗）-&gt;电压跟随器\n\n\n\n\n\n\nTIP\n高输入电抗可以保证原先无源低通滤波电路的参数的稳定，负载都趋于无穷了。。。低输出电抗可以保证信号的纯净度，减小干扰。\n\n\n将无源滤波电路与运放组合可以做到滤波+信号放大\n有源高通滤波电路由有源器件构成的通高频阻低频的滤波电路。\n带通滤波电路只保留某一段频率带，抑制或衰减其他频率带的滤波电路\n懂点电路原理知道了LRC电路就可以知道实际上这有个通频带的概念。\n带阻滤波电路只抑制或衰减某一段频率带，对其他频率无影响的滤波电路，可以认为是与带通滤波电路互补的。\n实际电路中的滤波电路分析电容的实际等效模型\n\n\n\n\n\n\n\n阻抗计算公式\n阻抗$ Z &#x3D; ESR + jωL + &#x2F;frac{1}{jωC} $Z &#x3D; ESR + jωL + 1&#x2F;jωC其中$ω &#x3D; 2πf$ ω &#x3D; 2πf\n\n\n\n\n\n\n\n\n\n性质\n当 f &#x3D; f 时，Z&#x3D;ESR  此时电容相当于一个纯电阻， f 为其谐振频率当 f &gt; f 时， 电容呈感性(像电感一样阻高频)当 f &lt; f 时， 电容呈容性(像电容一样阻低频)\n\n\n\n\n\n\n\n\n\n举例\n因此，当电容作高频滤波时，应工作在容性即f &lt; f 的条件下\n一般电容厂商会提供电容的数据手册上会有电容的阻抗-频率曲线，可以对应到谐振频率\n\n\n芯片电源引脚为什么加100nF电容？该电容常常称为旁路电容\n该电容需要尽量靠近电源引脚，因为如果摆放很远的话，电容滤除噪声后的电源会在这个段路径上又串扰进新的噪声，那么这个电容的作用就没有太大的意义。。\n\n\n\n\n\n\n作用\n\n滤除电源上的高频噪声–显然的\n储能–芯片电源端进行供电时可以就近供能，减小电源平面干扰，不然拉长从电源平面易产生噪声（电压波动）\n减小高频信号的回流路径–高频回路路径大会导致对其他器件的电磁干扰\n\n\n\n常用数字芯片信号频率基本都在10MHz以下，电源上的干扰或其自身干扰大概100nF的谐振频率之下，因此电源引脚旁路电容经验值100nF。\n\nClick to see more\n芯片信号频率比较高可以选择小容值\n容值与频率的对应参考（懒人不想看数据手册）\n\n\n\n频率范围\n推荐容值\n\n\n\nDC-100KHz\n10uF\n\n\n100KHz-10MHz\n100nF\n\n\n10MHz-100MHz\n10nF\n\n\n&gt;100M\n1nF\n\n\n可以去Multisim上进行电路仿真感受以下滤波的工作机理\n\n\n","slug":"滤波电路设计","date":"2023-11-07T15:33:17.000Z","categories_index":"Electronic","tags_index":"design","author_index":"PIKO"},{"id":"e3a2cd7dc7fecebd46e5f83ebb105431","title":"PCB设计中遇到的疑问","content":"疑问\n在设计四层板时，为什么标准的叠层设计是SIN1+PWR+GND+SIN2，能否不要GND，搞双电源层？\n\n\n\n\n\n\n\nTIP\n开始时不理解为啥要在内电层花一整层作为地层，认为只要在铺铜时铺GND的网络就可以做到和内电层作地层同样的效果，实际上GND仅铺铜还是会造成信号的回路变长，显然的，跨过半个板子到地和原地打个孔到地传输速度还是没法比。因此在设计四层板时，一般还是得搞个地平面保证信号质量。\n\n\n\n内层作电源分割和地层分割\n\n\n\n\n\n\n\nTIP\n注意电源输入，不要作截断，在分割电源区时，这又得在元件布局的时候注意、\n\n\n","slug":"PCB设计中遇到的疑问","date":"2023-10-31T14:02:21.000Z","categories_index":"Electronic","tags_index":"PCB","author_index":"PIKO"},{"id":"d50032c68beed9718f289c6681698c9b","title":"PCB设计原则","content":"前言\n\n\n\n\n\nTIP\nPCB设计是一个很吃经验的活，很多原则要在多次实践中慢慢摸索与牢记。\n\n资料\n\n\n\n\n\n\n\n\n\nPCB设计时如何选择合适的叠层方案\n丝印注意事项\n内电层电源平面分割\n\n","slug":"PCB设计原则","date":"2023-10-31T13:52:00.000Z","categories_index":"Electronic","tags_index":"PCB","author_index":"PIKO"},{"id":"e51893df1f169b9a3df8a540ead30580","title":"Docker资料汇总","content":"前言本文是基于Windows的Docker资料汇总\nDocker 官网https://www.docker.com/\n下载链接https://www.docker.com/get-started/\n改非C盘安装\nhttps://blog.csdn.net/weixin_41166529/article/details/128597650\n\n按1资料的方法不要用命令行启动installer，本人亲测出了问题，直接点击installer图标下载即可\n下载wsl管理员身份打开cmd运行如下命令\nbash\ndism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\n\ndism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart\nMicrosoft官网下载Linux更新包https://learn.microsoft.com/zh-cn/windows/wsl/install-manual#step-4---download-the-linux-kernel-update-package\n设置wsl2为默认版本在命令行里执行如下命令\nbashwsl –set-default-version2Microsoft store 下载linux内核\ndocker 简单创建命令行使用\nbash\ndocker version \n查找docker版本\ndocker pull ubuntu:20.04 \n拉取docker数据库里的ubuntu镜像\ndocker run -it ubuntu:20.04 /bin/ bash \n创建并运行容器（相当于一个空白ubuntu环境）\ndocker exer -it 容器名      \n再次进入时输入，记得要在docker中打开容器\n可能问题wsl打开Ubuntu20.04时,出现0x8007a1bc原因：没有内核升级解决方案：Microsoft官网下载Linux更新包https://learn.microsoft.com/zh-cn/windows/wsl/install-manual#step-4---download-the-linux-kernel-update-package\nwsl创建用户时，出现参考对象类型不支持尝试解决方案：改注册表记事本新建文档写下如下信息：\nbash\nWindows Registry Editor Version 5.00\n \n[HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\WinSock2\\Parameters\\AppId_Catalog\\0408F7A3]\n&quot;AppFullPath&quot;=&quot;C:\\\\Windows\\\\System32\\\\wsl.exe&quot;\n&quot;PermittedLspCategories&quot;=dword:80000000\n保存加后缀.reg为注册表文件然后执行即可解决问题\n","slug":"Docker资料汇总","date":"2023-10-26T03:22:11.000Z","categories_index":"","tags_index":"Linux","author_index":"PIKO"},{"id":"9cc33b411dfac4ea702c9c37d469d71e","title":"Yolov5初次尝试","content":"前言本人刚入门学习计算机视觉，完全看不懂理论算法，就想着先搭建一个环境去跑一个目标识别，这样能在大体上对该领域有个粗略的认知\n正文简述Yolov5的检测原理\n\nResize image改变图像大小，目标检测一般要统一输入图像的大小方便后面神经网络的预测和推理\n\n缩放图像-&gt;设定锚框\n\nRun convolutional network在神经网络中处理图像（很复杂，这才是yolo的精髓）\n\n多尺度融合：最后得到大小不同的含有检测信息的特征图，将其融合计算以便于小目标的检测\n\nNom-maximum suppression把多个预选框融合成一个预选框，得到最后的bounding box，输出就是最后的检测结果了\n\n参考文档\nhttps://blog.csdn.net/qq_45104603/article/details/121783848\nhttps://blog.csdn.net/m0_51261924/article/details/131650365\nhttps://blog.csdn.net/HUASHUDEYANJING/article/details/126023842\n\n","slug":"YOLOv5初次尝试","date":"2023-10-25T15:07:58.000Z","categories_index":"AI","tags_index":"","author_index":"PIKO"},{"id":"26be7765875c7794e84f5b7513278f2a","title":"Hexo + GitHub Pages 自定义域名的设置","content":"背景我在创建个人博客之后就有打算改域名了，只是碍于空闲时间不够充裕没有去实现。到了周末，我抽了大段空闲时间来搞这个，在其中花费了大量时间去配置DNS和SSL，走了许多弯路，我本人不是IT专业的，希望本文能帮到那些和我一样遇到问题（想搞个自己的域名装13）的人吧。\n具体操作本文只演示如何将Hexo + GitHub Pages 创建的静态网站域名从 xxx.github.io 改成 自定义域名\n域名获取可以在万网上购买自己想要的域名https://wanwang.aliyun.com/\n要先注册账号，购买域名是要实名认证的，一般流程是要等待身份审核通过，但这就要花2~3天去等待。可以先购买域名（使用阿里云的公共云账号），之后的操作会与一般流程不同，但可以不需要等待，短时间完成。\n配置本地博客文件夹在public文件夹中创建CNAME文件（注意不要有后缀）\n在CNAME中添加如下内容\nbash自定义域名（不要https://）再将本地文件上传到GitHub仓库\nbashhexo g &amp;&amp; hexo d完成上述步骤后，在仓库的Setting的Page下就可以看到域名指向你自己的域名啦\n如果是在GitHub仓库里创建CNAME的话就会导致之后你上传blog时会导致CNAME文件被删除（因为本地没有），就要重新写一份CNAME。\n配置DNS在阿里云的控制台里可以进行域名解析。使用Cloudflare可以不需要实名认证进行域名解析，详细操作可以去参考其他博客。\n\n工作台进入云解析，快速实名认证，认证完之后要进行账户转移，把账户从阿里云转到你的账户中，不然无法执行域名解析\n完成上述操作之后就可以执行域名解析了，可以在解析设置中添加记录，详细配置如下，配置完成之后等待几分钟之后DNS就配置好了\n\nbash\n记录类型：CNAME\n\n主机记录：填你的域名\n\n\n解析请求来源：默认\n\n记录值： xxx.github.io\n\nTTL：10分钟\n域名解析之后就可以以你自定义的域名去打开GitHub Pages创建的静态网站了。注：输入之前的xxx.github.io也会被指向你自定义的域名到这一步实际上就已经基本完成了，github.io域名已经指向你自定义的域名了。但是还是会有一点点小瑕疵，那就是这个域名没有配置SSL加密，它的前缀是http会导致你在浏览器上打开时会一直显示不安全，yysy，个人博客一直挂着一个不安全就是真没事看着也难受。\n配置SSL可以在阿里云上申请免费SSL证书。（一年为期限）通过配置免费证书可以使你的域名加密，从http变成https\n\n打开阿里云控制台数字证书管理服务-&gt;SSL证书-&gt;免费证书 提交申请会获得20个网站的SSL证书\n创建证书-&gt;证书申请-&gt;提交审核\n\nbash证书绑定域名： 你自定义的域名\n域名验证方式： 手工DNS验证\nCSR生成方式： 系统生成系统会帮你配置好DNS不用管，只要在域名解析里把新生成的SSL的域名解析重启就行。\n配置GitHub仓库在Pages界面Custom domain下勾选Enforce HTTPS 即可。前头不重启SSL的域名解析会导致这里的Enforce HTTPS是不可选取的\n结语这次配置二级域名是真的折磨，基本把能踩的坑都踩了一遍。\n参考网站：\n\nhttps://blog.csdn.net/weixin_44129672/article/details/104763893\n\nhttps://www.moerats.com/archives/616/\n\n\n","slug":"Hexo-GitHub-Pages-二级域名的设置","date":"2023-10-22T13:02:08.000Z","categories_index":"IT","tags_index":"blog","author_index":"PIKO"},{"id":"bd7f3e90fea1abdc8b58a7bfc4c3d088","title":"颜色对应的代码","content":"颜色代码6位码可直接复制bash\n红色        #FF0000 \n深紫色      #871F78 \n褐红色      #8E236B \n石英色      #D9D9F3\n绿色        #00FF00 \n深石板蓝    #6B238E \n中海蓝色    #32CD99 \n艳蓝色      #5959AB\n蓝色        #0000FF \n深铅灰色    #2F4F4F \n中蓝色      #3232CD \n鲑鱼色      #6F4242\n牡丹红      #FF00FF \n深棕褐色    #97694F \n中森林绿    #6B8E23 \n猩红色      #BC1717\n青色        #00FFFF \n深绿松石色  #7093DB \n中鲜黄色    #EAEAAE \n海绿色      #238E68\n黄色        #FFFF00 \n暗木色      #855E42 \n中兰花色    #9370DB \n半甜巧克力色 #6B4226\n黑色        #000000 \n淡灰色      #545454 \n中海绿色    #426F42 \n赭色        #8E6B23\n海蓝        #70DB93 \n土灰玫瑰红色 #856363 \n中石板蓝色  #7F00FF \n银色        #E6E8FA\n巧克力色    #5C3317 \n长石色      #D19275 \n中春绿色    #7FFF00 \n天蓝        #3299CC\n蓝紫色      #9F5F9F \n火砖色      #8E2323 \n中绿松石色  #70DBDB \n石板蓝      #007FFF\n黄铜色      #B5A642 \n森林绿      #238E23 \n中紫红色    #DB7093 \n艳粉红色    #FF1CAE\n亮金色      #D9D919 \n金色        #CD7F32 \n中木色      #A68064 \n春绿色      #00FF7F\n棕色        #A67D3D \n鲜黄色      #DBDB70 \n深藏青色    #2F2F4F \n钢蓝色      #236B8E\n青铜色      #8C7853 \n灰色        #C0C0C0 \n海军蓝      #23238E \n亮天蓝色    #38B0DE\n2号青铜色   #A67D3D \n铜绿色      #527F76 \n霓虹篮      #4D4DFF \n棕褐色      #DB9370\n士官服蓝色  #5F9F9F \n青黄色      #93DB70 \n霓虹粉红    #FF6EC7 \n紫红色      #D8BFD8\n冷铜色      #D98719 \n猎人绿      #215E21 \n新深藏青色  #00009C \n石板蓝色    #ADEAEA\n铜色        #B87333 \n印度红      #4E2F2F \n新棕褐色    #EBC79E \n浓深棕色    #5C4033\n珊瑚红      #FF7F00 \n土黄色      #9F9F5F \n暗金黄色    #CFB53B \n淡浅灰色    #CDCDCD\n紫蓝色      #42426F\n浅蓝色      #C0D9D9\n橙色        #FF7F00 \n紫罗兰色    #4F2F4F\n深棕        #5C4033 \n浅灰色      #A8A8A8 \n橙红色      #FF2400 \n紫罗兰红色  #CC3299\n深绿        #2F4F2F \n浅钢蓝色    #8F8FBD \n淡紫色      #DB70DB \n麦黄色      #D8D8BF\n深铜绿色    #4A766E \n浅木色      #E9C2A6 \n浅绿色      #8FBC8F \n黄绿色      #99CC32\n深橄榄绿    #4F4F2F \n石灰绿色    #32CD32 \n粉红色      #BC8F8F\n深兰花色    #9932CD \n桔黄色      #E47833 \n李子色      #EAADEA\n图解\n\n\n\n\n\n\n\n\n\n\n\n\n\n在线6位色彩码转换器https://www.rapidtables.org/zh-CN/convert/color/index.html\n8位码未完待续\nEND","slug":"颜色对应的代码","date":"2023-10-21T08:59:35.000Z","categories_index":"","tags_index":"undefined","author_index":"PIKO"},{"id":"0efe1085ac0265965eb4a094f23bbe8f","title":"Keil编译常见Error及其解决方案","content":"Keil编译常见Error及其解决方案这里总结了本人使用Keil所遇到过的所有编译报错，并附上解决方案可供参考。\nError: L6218E: Undefined symbol __aeabi_assert (referred from xxx.o).原因：引用 #include &lt;assert.h&gt; 断言功能缺失方案一未定义的符号__aeabi_assert,原因是keil没有添加依赖项，请在 Manage Run-time Evironment 中添加即可。Compiler–&gt; I&#x2F;O –&gt; STDERR\n方案二bash\n__attribute__((weak,noreturn))\nvoid __aeabi_assert (const char *expr, const char *file, int line) &#123;\n  char str[12], *p;\n\n  fputs(&quot;*** assertion failed: &quot;, stderr);\n  fputs(expr, stderr);\n  fputs(&quot;, file &quot;, stderr);\n  fputs(file, stderr);\n  fputs(&quot;, line &quot;, stderr);\n\n  p = str + sizeof(str);\n  *--p = &#39;\\0&#39;;\n  *--p = &#39;\\n&#39;;\n  while (line &gt; 0) &#123;\n    *--p = &#39;0&#39; + (line % 10);\n    line /= 10;\n  &#125;\n  fputs(p, stderr);\n\n  abort();\n&#125;\n\n__attribute__((weak))\nvoid abort(void) &#123;\n  for (;;);\n&#125;\n在使用Arm MicroLIB C库时，实现一个自定义的__aeabi_assert()函数。使用上面的代码作为模板。\n禁用assert(): On Project -&gt; Options For Target -&gt; On C&#x2F; c++选项卡，用于定义类型“NDEBUG”。-这会导致对assert()函数的调用不起作用。\n禁用MicroLIB:在Target对话框右上方的Project -&gt; Options For Target -&gt;下，取消选中Use MicroLIB以禁用Arm MicroLIB C库。\n修改完成后，重新构建项目使其生效。————————————————可参考链接：https://blog.csdn.net/qq_29246181/article/details/128777718\nError: L6218E: Undefined symbol xxx (referred from xxx.o)原因：函数未定义解决方案：确定该函数的调用在什么地方出现问题1.检查头文件路径2.探查函数是否有定义 ctrl+F\n","slug":"Keil编译常见Error及其解决方案","date":"2023-10-20T14:28:16.000Z","categories_index":"Embedded","tags_index":"IDE","author_index":"PIKO"},{"id":"7742e69e813648d5c85a45d015f995f0","title":"STM32F103RCT6核心板设计","content":"原理图总览\n功能bash1. 四按键模块（共阴极）\n2. ST-LINK接口\n3. TypeC通信和XH座子通信\n4. 内置CH340可以串口ISP\n5. 外部FLASH和EEPROM\n6. 2个引脚LED\n7. 7线0.96寸OLED屏接口\n8. 拉出STM32F103RCT6基本所有引脚\n9. 蜂鸣器\n10. 可使用跳线和按键控制外部设备的启动应该加个电源开关的，搞忘了。\n电源模块采用AMS1117-3.3线性稳压器性能一般般，主要是便宜，设计的电路也不复杂，相比其他什么开关电源（MP2315之类的），发热比较严重吧，纹波也比较大。\n蜂鸣器可使用有源蜂鸣器or无源蜂鸣器\nPCB设计\n\n\n\n\n\nTIP\n使用四层板\n\n\n\n图层名\n作用\n\n\n\ntoplayer\n信号层1\n\n\ninnerlayer1\n5V内电层\n\n\ninnerlayer2\n3V3内电层\n\n\nbottomlayer\n信号层2\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING\n\n在设计晶振时需要保证晶振电路和其他电路相隔离（通过GND），保证晶振的高频信号不会对其他电路产生电磁干扰。\n芯片的电源输入一定要有旁路电容接地（经验值100nF），而且该电容一定要放在芯片电源引脚边上，保证信号的回路尽量短，该电容其实很有讲究的，功能挺多的，感兴趣之后可以再讨论。\n高频信号尽量不过两次孔，低频信号不过三次孔，减少过孔寄生效应的不利影响，高频信号线之间尽量铺GND隔离，需要严格同步时序的高频信号要保证信号线长度相等。\n焊接时候先焊电源，不然电源一炸之后的单片机也跟着炸，直接输光光。\n\n\n\n物理尺寸整板大小：7cm x 6cm定位孔：M3OLED定位孔：M2\n后记本板子还未经验证，等验证完之后我会开源在JLC。之后还会设计一个扩展板用于心电检测。\nPS: \n\n本来想着把板子做小，做成6*6，奈何layout水平不够。。。\n在最后布地线是真的折磨，我本来以为最后铺铜就可以保证大部分GND相连了。。最后还是改了半个多小时。。。（还好这个板子可以全部设备共地，不用分什么模拟地、数字地、直流地、交流地的）\n\n参考资料数据手册\n\n\n\n\n\n\n\n\n\nCH340\nBY25Q64ASSIG\nAT24C02\nASM1117\nSTM32F1x\n\nPCB布线原则https://blog.csdn.net/zerokingwang/article/details/127651767\n","slug":"STM32F103RCT6核心板设计","date":"2023-10-20T13:50:03.000Z","categories_index":"Electronic","tags_index":"STM32","author_index":"PIKO"},{"id":"de085bbc27b1fb479554dba220957700","title":"关于学习模式的讨论","content":"今天与项目导师交流立项意见，受益良多。\nAI的崛起是必然的作为大二的鼠鼠很少还去关注研究生的专业，从导师处了解到本来应该只是计软院开的ai专业逐渐渗透到别的院里，不管是电信还是机电都开设了研究生ai相关专业。而ai在各个领域的表现是显而易见的。原先的理念逐渐被打破，以测试员为例：原先都是说老练的测试员更吃香，究其源是干过几个项目后手里都有一些项目数据和项目模板可供搭建，在之后的项目中可以沿用之前的项目而在此基础之上作修改以适配现有项目。再资深的测试员经过再多的项目积累手里也不可能会有几百份项目报告，而ai在深度学习之后能在几分钟内给出成百上千份项目报告。而在底层架构上，ai是完爆人类的，你不会指望着你一个才学了2、3年的C语言编写出来的程序架构能和ai写出来的掰手腕的。\nAI时代下的学习模式当下绝大部分课本的学习模式都是从最底层开始搭建，先教会如何搭砖头在学如何砌墙。模电从二极管开始学、数电以芯片为主导……这些都太细了。一本书要学一个学期的时间(当然有些是基础是必然要这么学的，凡事不能片面化~~)，而新时代的变革是飞快的，一本书学个一年半载的效率实在是太低了，你可能学完了，但是ai能比你实现的更好。现在需要的是快速学习的能力，需要的是从上而下的学习，先学会快速搭建，之后在慢慢搞懂底层逻辑。从理论开始一点一点的学不仅难而且实现功能的周期太长。导师提出在以后工作中应该也会遇到如此情况：老板今天看到一个好的实例，给你一点相关资料(甚至不给)，让你在一周之内搭建实现这个实例，你要在这一周之内搞懂其实现原理并复现出来把老板讲懂(有些情况下甚至不需要你搞懂，可能会有相关技术人员去实现)，你要给出方案，在这一周之内。。。快速学习的能力不言而喻了吧。\n具体实现我们可以这样认为，我们手里拿的书都是只有翻一页才会展示下一页的知识，下一个要用的知识永远是未知的，因而需要保持长久的好奇心与适应力去接受新事物。实际一点来说，我们拿到一本书首先是去翻看目录，花去一小段时间去建立整个知识体系，能快速检索到或实现想要达成的目标，回过头来在好好琢磨一些核心底层问题。总而言之，学习是永无止境的，重要是能弄明白你在学什么，别当无头苍蝇。\n","slug":"关于学习模式的讨论","date":"2023-10-19T11:52:41.000Z","categories_index":"Life","tags_index":"tittle-tattle","author_index":"PIKO"},{"id":"507d422f151c505ace4132bc7d4f4fff","title":"LVGL的移植","content":"前言LVGL是一个免费好用的嵌入式GUI库，其控件扁平化，符合当今大众审美，主要应用在一些像智能手表之类的产品之上。前几天刚搞到一块STM32F103RCT6的最小系统板+ST7735S的TFT-LCD（可惜了不是触摸屏QAQ），今咱就来尝试移植一下LVGL到STM32上，整点炫的。\nLVGL的移植流程以STM32F103RCT6为例\nLVGL源码下载Github仓库地址 https://github.com/lvgl/lvgl\n准备工作准备LVGL源码修改lv_conf.h文件，修改条件编译\n删除不必要文件\n添加工程文件需要添加文件需要添加头文件路径特殊配置(可选)屏蔽warning\n修改工程文件(裸机开发)总体配置修改lv_conf.h\nbash\n/* clang-format off */\n#if 1 /*Set it to &quot;1&quot; to enable content*/\n// 原先这里是#if 0 需改成 #if 1\n添加output设备修改lv_port_disp_template.h为lv_port_disp.h对应的头文件名在对应c文件中也要更改\nbash\n/*Copy this file as &quot;lv_port_disp.h&quot; and set this value to &quot;1&quot; to enable content*/\n#if 1 //使能lv_port_disp.h\n\n修改lv_port_disp_template.c\nbash\n// 添加自己的屏幕尺寸\n\n #define MY_DISP_HOR_RES    128 //水平\n #define MY_DISP_VER_RES    160 //垂直\n\n// 修改使用的buffer算法（三选一）\n// 有详细介绍，可以认为从上到下性能递增，占用内存也递增\n/* Example for 1) */\n    static lv_disp_draw_buf_t draw_buf_dsc_1;\n    static lv_color_t buf_1[MY_DISP_HOR_RES * 10];                          /*A buffer for 10 rows*/\n    lv_disp_draw_buf_init(&amp;draw_buf_dsc_1, buf_1, NULL, MY_DISP_HOR_RES * 10);   /*Initialize the display buffer*/\n\n//    /* Example for 2) */\n//    static lv_disp_draw_buf_t draw_buf_dsc_2;\n//    static lv_color_t buf_2_1[MY_DISP_HOR_RES * 10];                        /*A buffer for 10 rows*/\n//    static lv_color_t buf_2_2[MY_DISP_HOR_RES * 10];                        /*An other buffer for 10 rows*/\n//    lv_disp_draw_buf_init(&amp;draw_buf_dsc_2, buf_2_1, buf_2_2, MY_DISP_HOR_RES * 10);   /*Initialize the display buffer*/\n\n//    /* Example for 3) also set disp_drv.full_refresh = 1 below*/\n//    static lv_disp_draw_buf_t draw_buf_dsc_3;\n//    static lv_color_t buf_3_1[MY_DISP_HOR_RES * MY_DISP_VER_RES];            /*A screen sized buffer*/\n//    static lv_color_t buf_3_2[MY_DISP_HOR_RES * MY_DISP_VER_RES];            /*Another screen sized buffer*/\n//    lv_disp_draw_buf_init(&amp;draw_buf_dsc_3, buf_3_1, buf_3_2,\n//                          MY_DISP_VER_RES * LV_VER_RES_MAX);   /*Initialize the display buffer*/\n\n// 修改 disp_init 放入自己的屏幕驱动\n\n/*Initialize your display and the required peripherals.*/\nstatic void disp_init(void)\n&#123;\n    /*You code here*/\n  LCD_Init();\n  \n&#125;\n\n// 修改 disp_flush 放入自己屏幕的色块填充程序\n\n/*Flush the content of the internal buffer the specific area on the display\n *You can use DMA or any hardware acceleration to do this operation in the background but\n *&#39;lv_disp_flush_ready()&#39; has to be called when finished.*/\nstatic void disp_flush(lv_disp_drv_t * disp_drv, const lv_area_t * area, lv_color_t * color_p)\n&#123;\n//    if(disp_flush_enabled) &#123;\n//        /*The most simple case (but also the slowest) to put all pixels to the screen one-by-one*/\n\n//        int32_t x;\n//        int32_t y;\n//        for(y = area-&gt;y1; y &lt;= area-&gt;y2; y++) &#123;\n//            for(x = area-&gt;x1; x &lt;= area-&gt;x2; x++) &#123;\n//                /*Put a pixel to the display. For example:*/\n//                /*put_px(x, y, *color_p)*/\n//                color_p++;\n//            &#125;\n//        &#125;\n//    &#125;\n    LCD_Fill(area-&gt;x1,area-&gt;y1,area-&gt;x2,area-&gt;y2,(uint16_t)color_p);\n    /*IMPORTANT!!!\n     *Inform the graphics library that you are ready with the flushing*/\n    lv_disp_flush_ready(disp_drv);\n&#125;\n添加input设备LVGL支持编码器、按键、触摸屏等输入\nbash\n// 还没写捏~~\n// 产能不足惹~~\n添加lvgl时基定时器驱动文件加入 #include”lvgl.h”定时器中断设置1ms中断，放入lv_tick_inc\nbash\nvoid TIM3_Handler(void)&#123; //定时器配置1ms中断\n    lv_tick_inc(1);\n&#125;\n\n测试程序可能无法显示需要去改startup文件的堆栈大小 \nFreeRTOS下的移植基本同裸机的配置相差不大，也就需要改个时基，使用FreeRTOS的\n时基配置lv_conf.h中添加自定义时基\nbash\n/*Use a custom tick source that tells the elapsed time in milliseconds.\n *It removes the need to manually update the tick with `lv_tick_inc()`)*/\n#define LV_TICK_CUSTOM 1 //使能1，默认是0\n#if LV_TICK_CUSTOM\n    #define LV_TICK_CUSTOM_INCLUDE &quot;FreeRTOS.h&quot;         /*Header for the system time function*/\n    #define LV_TICK_CUSTOM_SYS_TIME_EXPR (xTaskGetTickCount())    /*Expression evaluating to current system time in ms*/\n注意该配置与裸机移植的配置不能兼容。\n后记刚学LVGL，移个植都费劲，库函数也还没理清，路漫漫其修远兮啊。。。\n","slug":"LVGL的移植","date":"2023-10-18T14:26:45.000Z","categories_index":"Embedded","tags_index":"TPSW","author_index":"PIKO"},{"id":"11caf7d9cf7a688f2728c9360ab7d311","title":"图片转链接--SM.MS图床","content":"SM.SM图床网站推荐引言平时大家编写.md文件时肯定有对图片有所苦恼。正常从本机引用的话，跨设备图片就会无法显示，如果调到github仓库里又会比较麻烦。\n大家肯定会想要一个方法既能解决跨设备图片显示，操作起来又没那么困难的方法。\n正文在这里给大家推荐一个网站 https://smms.app/\n其界面UI如下：\n可以支持五种链接格式\n结语阿巴阿巴阿巴阿巴阿巴\n","slug":"图片转链接-SM-MS图床","date":"2023-10-17T14:01:58.000Z","categories_index":"IT","tags_index":"tool","author_index":"PIKO"},{"id":"7306721a27e9ec7e2372287f94d883c8","title":"Cmake基础语法","content":"Cmake基础语法bash# 设置cmake最低版本\ncmake_minimum_required(VERSION 3.2)\n\n# project命令用于指定cmake工程的名称，实际上，它还可以指定cmake工程的版本号（VERSION关键字）、简短的描述（DESCRIPTION关键字）、主页URL（HOMEPAGE_URL关键字）和编译工程使用的语言（LANGUAGES关键字）\n# project(&lt;PROJECT-NAME&gt; [&lt;language-name&gt;...])\n# project(&lt;PROJECT-NAME&gt; [VERSION &lt;major&gt;[.&lt;minor&gt;[.&lt;patch&gt;[.&lt;tweak&gt;]]]] [DESCRIPTION &lt;project-description-string&gt;][HOMEPAGE_URL &lt;url-string&gt;] [LANGUAGES &lt;language-name&gt;...])\n# $&#123;PROJECT_SOURCE_DIR&#125; 和 &lt;PROJECT-NAME&gt;_SOURCE_DIR：本CMakeLists.txt所在的文件夹路径\n# $&#123;PROJECT_NAME&#125;：本CMakeLists.txt的project名称\nproject(xxx)\nproject(mytest VERSION 1.2.3.4)\nproject (mytest HOMEPAGE_URL “https://www.XXX(示例).com”)\n\n# 获取路径下所有的.cpp/.c/.cc文件（不包括子目录），并赋值给变量中\naux_source_directory(路径 变量)\n\n# GLOB_RECURSE 获取目录下的所有cpp文件（不包括子目录），并赋值给SOURCES\nfile(\n        GLOB SOURCES\n        $&#123;PROJECT_SOURCE_DIR&#125;/*.c\n\n)\n# GLOB_RECURSE 获取目录下的所有cpp文件（包括子目录），并赋值给NATIVE_SRC\nfile(\n      GLOB_RECURSE NATIVE_SRC \n      $&#123;PROJECT_SOURCE_DIR&#125;/lib/*.cpp\n)\n\n# 给文件名/路径名或其他字符串起别名，用$&#123;变量&#125;获取变量内容\nset(变量 文件名/路径/...)\n\n# 添加编译选项FOO BAR\n# add_definitions定义宏，但是这种定义方式无法给宏具体值 等价C语言中的#define  MG_ENABLE_OPENSSL\nadd_definitions(-DFOO -DBAR ...)\n\n# add_compile_definitions定义宏，这种方式可以给宏具体值，但是这个指令只要高版本的cmake支持 等价C语言中 #define  MG_ENABLE_OPENSSL   1 \nadd_compile_definitions(MG_ENABLE_OPENSSL=1)\n\n# 打印消息\nmessage(消息)\n\n# 编译子文件夹的CMakeLists.txt\nadd_subdirectory(子文件夹名称)\n\n# 将.cpp/.c/.cc文件生成.a静态库\n# 注意，库文件名称通常为libxxx.so，在这里只要写xxx即可\nadd_library(库文件名称 STATIC 文件)\n\n# 将.cpp/.c/.cc文件生成可执行文件\nadd_executable(可执行文件名称 文件)\n\n# 规定.h头文件路径\ninclude_directories(路径)\n\n# 规定.so/.a库文件路径\nlink_directories(路径)\n\n# 设置编译选项及默认值\noption(TEST_DEBUG &quot;option for debug&quot; OFF)\n\n# 对add_library或add_executable生成的文件进行链接操作\n# 注意，库文件名称通常为libxxx.so，在这里只要写xxx即可\ntarget_link_libraries(库文件名称/可执行文件名称 链接的库文件名称)\n\n \n基础配置流程\nproject(xxx)                                          #必须\n\nadd_subdirectory(子文件夹名称)                         #父目录必须，子目录不必\n\nadd_library(库文件名称 STATIC 文件)                    #通常子目录(二选一)\nadd_executable(可执行文件名称 文件)                     #通常父目录(二选一)\n\ninclude_directories(路径)                              #必须\nlink_directories(路径)                                 #必须\n\ntarget_link_libraries(库文件名称/可执行文件名称 链接的库文件名称)       #必须\n     ","slug":"Cmake基础语法","date":"2023-10-17T11:57:43.000Z","categories_index":"Embedded","tags_index":"Linux,Cmake","author_index":"PIKO"},{"id":"92468dc8f04e07f9e105708cbbdde9c2","title":"FreeRTOS的移植","content":"FreeRTOS的移植介绍下FreeRTOS源码内容\n\n\n名称\n描述\n\n\n\nFreeRTOS\nFreeRTOS内核\n\n\nFreeRTOS-Plus\nFreeRTOS组件\n\n\ntools\n工具\n\n\nGitHub-FreeRTOS-Home\nFreeRTOS的GitHub仓库链接\n\n\nQuick_Start_Guide\n快速入门指南官方文档链接\n\n\nUpgrading-to-FreeRTOS-xxx\n升级到指定FreeRTOS版本官方文档链接\n\n\nHistory.txt\nFreeRTOS历史更新记录\n\n\n其他\n其他\n\n\nFreeRTOS内核\n\n\n名称\n描述\n\n\n\nDemo\nFreeRTOS演示例程\n\n\nLicense\nFreeRTOS相关许可\n\n\nSource\nFreeRTOS源码（主要拷贝）\n\n\nTest\n公用以及移植层测试代码\n\n\nSource文件夹freeRTOS源码\n\n\n\n名称\n描述\n\n\n\ninclude\n内包含了FreeRTOS的头文件\n\n\nportable\n内包含了FreeRTOS的移植文件\n\n\ncroutine.c\n协程相关文件\n\n\nevent_groups.c\n事件相关文件\n\n\nlist.c\n列表相关文件（状态相关，必要）\n\n\nqueue.c\n队列相关文件（状态相关，必要）\n\n\nstream_buffer.c\n流式缓冲区相关文件\n\n\ntasks.c\n任务相关文件（必要）\n\n\ntimers.c\n软件定时器相关文件\n\n\nportable文件夹freeRTOS与硬件交互的桥梁\n\n\n\n名称\n描述\n\n\n\nKeil\n指向RVDS文件夹\n\n\nRVDS\n不同内核芯片的移植文件\n\n\nMemMang\n内存管理文件（5种算法）\n\n\n其他的文件用不着可以删掉\nFreeRTOS移植步骤以STM32F103为例\n1．添加FreeRTOS源码添加入工程\nFreeRTOS源码导入工程\nportable里这些文件导入工程(其他的删掉!!)\n在Keil中建立如下组\n注意配置头文件地址\n2．添加FreeRTOSConfig.h作用：对FreeRTOS进行配置和裁剪，以及API函数的使能操作系统配置文件:获取途径\na.自己手写FreeRTOS官网有详细说明（https://www.freertos.org/a00110.html）\nb.Demo例程或许有官方支持（但也不全，还没很多的注释，建议还是自己写）\nDemo这个里的FreeRTOSConfig.h可使用\nc.参考我写哒\nbash\n#ifndef FREERTOS_CONFIG_H\n#define FREERTOS_CONFIG_H\n\n/*-----------------------------------------------------------\n * Application specific definitions.\n *\n * These definitions should be adjusted for your particular hardware and\n * application requirements.\n *\n * THESE PARAMETERS ARE DESCRIBED WITHIN THE &#39;CONFIGURATION&#39; SECTION OF THE\n * FreeRTOS API DOCUMENTATION AVAILABLE ON THE FreeRTOS.org WEB SITE.\n *\n * See http://www.freertos.org/a00110.html\n *----------------------------------------------------------*/\n\n#define configUSE_PREEMPTION\t\t1   //使用抢占式内核\n//#define configUSE_TIME_SLICING  1   //使用时间片调度（默认是使能的）\n#define configUSE_IDLE_HOOK\t\t\t0   //使用模式的钩子函数\n#define configUSE_TICK_HOOK\t\t\t0   //使用TICK的钩子函数\n#define configUSS_PORT_OPTIMISED_SELECTION  1  //1使用硬件选择下一个执行任务 （需要架构拥有计算前导零[CLZ]的指令）\n#define configUSE_TICKLESS_IDLE 0   //1使用低功耗模式\n#define configUSE_QUEUE_SETS    1   //1使用队列集\n#define configCPU_CLOCK_HZ\t\t\t( ( unsigned long ) 72000000 )  //CPU主频\n//#define configSYSTICK_CLOCK_HZ  (configCPU_CLOCK_HZ)  //定义systick的时钟频率 默认与系统时钟相同。\n#define configTICK_RATE_HZ\t\t\t( ( TickType_t ) 1000 )\n#define configMAX_PRIORITIES\t\t( 32 )  //可使用最大优先级\n#define configMINIMAL_STACK_SIZE\t( ( unsigned short ) 128 )\n\n#define configMAX_TASK_NAME_LEN\t\t( 16 )\n\n#define configUSE_16_BIT_TICKS\t\t0   //设置系统节拍计数器的数据类型 0为32位\n#define configIDLE_SHOULD_YIELD\t\t1   //1同优先级的任务可以抢占空闲任务\n#define configUSE_TASK_NOTIFCATIONS 1 //1 使能任务间的消息传递\n#define configENABLE_BACKWARD_COMPATIBILITY 0 //1 兼容老版本API函数\n\n/*Memory Allocation*/\n#define configTOTAL_HEAP_SIZE\t\t( ( size_t ) ( 17 * 1024 ) )  //17k总堆栈\n#define configSUPPORT_STATIC_ALLOCATION   0   //1 支持静态申请分配内存，默认0\n#define configSUPPORT_DYNAMIC_ALLOCATION  1   //1 支持动态申请分配内存，默认1\n\n/*Debug*/\n#define configUSE_TRACE_FACILITY\t1   //使能可视化追踪调试\n#define configUSE_STATS_FORMATTING_FUNCTION 1\n\n/*Software Timer*/\n#define configUSE_TIMERS  1 //1 使能软件定时器\n#define configTIMER_TASK_PRIORITY (configMAX_PRIORITIES - 1)\n#define configTIMER_QUEUE_LENGTH  5\n#define configTIMER_TASK_STACK_DEPTH  (configMINIMAL_STACK_SIZE * 2)\n\n\n/* Set the following definitions to 1 to include the API function, or zero\nto exclude the API function. */\n\n#define INCLUDE_vTaskPrioritySet\t\t    1   //设置任务优先级 \n#define INCLUDE_uxTaskPriorityGet\t\t    1   //获取任务优先级\n#define INCLUDE_vTaskDelete\t\t\t\t      1   //删除任务\n#define INCLUDE_vTaskCleanUpResources\t  1   //\n#define INCLUDE_vTaskSuspend\t\t\t      1   //挂起任务\n#define INCLUDE_xResumeFromISR          1   //恢复在中断中挂起的任务\n#define INCLUDE_vTaskDelayUntil\t\t\t    1   //任务绝对延时\n#define INCLUDE_vTaskDelay\t\t\t\t      1   //任务延时\n#define INCLUDE_xTaskGetSchedulerState  1   //\n#define INCLUDE_xTaskGetCurrentTaskHandle 1 //\n//#define INCLUDE_xTimerPendFunctionCall  1   //\n#define INCLUDE_eTaskGetState           1   //\n\n/* This is the raw value as per the Cortex-M3 NVIC.  Values can be 255\n(lowest) to 0 (1?) (highest). */\n#define configKERNEL_INTERRUPT_PRIORITY \t\t255\n/* !!!! configMAX_SYSCALL_INTERRUPT_PRIORITY must not be set to zero !!!!\nSee http://www.FreeRTOS.org/RTOS-Cortex-M3-M4.html. */\n#define configMAX_SYSCALL_INTERRUPT_PRIORITY \t191 /* equivalent to 0xb0, or priority 11. */\n\n/*FreeRTOS interrupt relative define*/\n#define xPortPendSVHandler    PendSV_Handler\n#define vPortSVCHandler       SVC_Handler\n\n/* This is the value being used as per the ST library which permits 16\npriority values, 0 to 15.  This must correspond to the\nconfigKERNEL_INTERRUPT_PRIORITY setting.  Here 15 corresponds to the lowest\nNVIC value of 255. */\n#define configLIBRARY_KERNEL_INTERRUPT_PRIORITY\t15\n\n/*Assert*/\n//#define vAssertCalled(char, int) printf(&quot;Error: %s, %d\\r\\n&quot;, char, int)\n//#define configASSERT(x) if((x) == 0) vAssertCalled(__FILE__,__LINE__)\n\n\n#endif /* FREERTOS_CONFIG_H */\n3．修改系统相关配置Systick 系统滴答时钟需要与FreeRTOSConfig.h里的configTICK_RATE_HZ对应滴答时钟中断需要加上任务切换\n4．修改中断相关配置Systick中断（自己配置）、SVC中断、PendSV中断（这俩在freeRTOS中有定义）因而在stm32f1xx_it.c中需要将相关中断定义注释掉\n一般要在SysTick_Handler中写\nbashvoid SysTick_Handler(void)&#123;\n  if(xTaskGetSchedulerState() != taskSCHEDULER_NOT_STARTED)&#123;//任务切换相关\n    xPortSysTickHandler(); //SVC和PendSV中断入口\n  &#125;\n&#125;没有注释编译就会出现这种情况\nbash\n.\\Objects\\Project.axf: Error: L6200E: Symbol SVC_Handler multiply defined (by port.o and stm32f10x_it.o).\n.\\Objects\\Project.axf: Error: L6200E: Symbol PendSV_Handler multiply defined (by port.o and stm32f10x_it.o).5．添加应用程序因人而异啦~\n源码参考基于STM32F103C8T6的FreeRTOS移植与简单配置（没有测试程序捏~~）https://github.com/Yesord/FreeROTS_STM32F103C8T6\nFreeRTOS移植可能出现的问题1.在学习FreeRTOS移植后，编译出现bash…\\OBJ\\LED.axf: Error: L6218E: Undefined symbol xTaskGetSchedulerState (referred from delay.o).出现这个错误的原因是xTaskGetSchedulerState的值没有改，需要在FreeRTOS.h中将其宏定义的值改为1即可。也可以去FreeRTOSConfig.h中把INCLUDE_xTaskGetScheduler 赋1\n后记配图还待更新，小编比较懒，不定时更新 ^.^\n","slug":"FreeRTOS的移植","date":"2023-10-17T09:33:25.000Z","categories_index":"Embedded","tags_index":"RTOS,STM32","author_index":"PIKO"},{"id":"5fb0b2232f7b008486299018a9a2ee11","title":"markdown-example","content":"@TOC\n欢迎使用Markdown编辑器你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。\n新的改变我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客：\n\n全新的界面设计 ，将会带来全新的写作体验；\n在创作中心设置你喜爱的代码高亮样式，Markdown 将代码片显示选择的高亮样式 进行展示；\n增加了 图片拖拽 功能，你可以将本地的图片直接拖拽到编辑区域直接展示；\n全新的 KaTeX数学公式 语法；\n增加了支持甘特图的mermaid语法^1 功能；\n增加了 多屏幕编辑 Markdown文章功能；\n增加了 焦点写作模式、预览模式、简洁写作模式、左右区域同步滚轮设置 等功能，功能按钮位于编辑区域与预览区域中间；\n增加了 检查列表 功能。\n\n功能快捷键撤销：Ctrl/Command + Z重做：Ctrl/Command + Y加粗：Ctrl/Command + B斜体：Ctrl/Command + I标题：Ctrl/Command + Shift + H无序列表：Ctrl/Command + Shift + U有序列表：Ctrl/Command + Shift + O检查列表：Ctrl/Command + Shift + C插入代码：Ctrl/Command + Shift + K插入链接：Ctrl/Command + Shift + L插入图片：Ctrl/Command + Shift + G查找：Ctrl/Command + F替换：Ctrl/Command + G\n合理的创建标题，有助于目录的生成直接输入1次#，并按下space后，将生成1级标题。输入2次#，并按下space后，将生成2级标题。以此类推，我们支持6级标题。有助于使用TOC语法后生成一个完美的目录。\n如何改变文本的样式强调文本 强调文本\n加粗文本 加粗文本\n&#x3D;&#x3D;标记文本&#x3D;&#x3D;\n删除文本\n\n\n\n\n\n\n\n\n\n引用文本\nH2O is是液体。\n2^10^ 运算结果是 1024.\n插入链接与图片链接: link.\n图片: \n带尺寸的图片: ![Alt](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hdmF0YXIuY3Nkbi5uZXQvNy83L0IvMV9yYWxmX2h4MTYzY29tLmpwZw &#x3D;30x30)\n居中的图片: \n居中并且带尺寸的图片: ![Alt](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hdmF0YXIuY3Nkbi5uZXQvNy83L0IvMV9yYWxmX2h4MTYzY29tLmpwZw#pic_center &#x3D;30x30)\n当然，我们为了让用户更加便捷，我们增加了图片拖拽功能。\n如何插入一段漂亮的代码片去博客设置页面，选择一款你喜欢的代码片高亮样式，下面展示同样高亮的 代码片.\n​javascript // An highlighted block var foo = &#39;bar&#39;; ​\n生成一个适合你的列表\n项目\n项目\n项目\n\n\n\n\n\n\n项目1\n项目2\n项目3\n\n\n 计划任务\n 完成任务\n\n创建一个表格一个简单的表格是这么创建的：\n\n\n\n项目\nValue\n\n\n\n电脑\n$1600\n\n\n手机\n$12\n\n\n导管\n$1\n\n\n设定内容居中、居左、居右使用:---------:居中使用:----------居左使用----------:居右\n\n\n\n第一列\n第二列\n第三列\n\n\n\n第一列文本居中\n第二列文本居右\n第三列文本居左\n\n\nSmartyPantsSmartyPants将ASCII标点字符转换为“智能”印刷标点HTML实体。例如：\n\n\n\nTYPE\nASCII\nHTML\n\n\n\nSingle backticks\n&#39;Isn&#39;t this fun?&#39;\n‘Isn’t this fun?’\n\n\nQuotes\n&quot;Isn&#39;t this fun?&quot;\n“Isn’t this fun?”\n\n\nDashes\n-- is en-dash, --- is em-dash\n– is en-dash, — is em-dash\n\n\n创建一个自定义列表MarkdownText-to-HTML conversion toolAuthors:  JohnLuke如何创建一个注脚一个具有注脚的文本。^2\n注释也是必不可少的Markdown将文本转换为 HTML。\n*[HTML]:   超文本标记语言\nKaTeX数学公式您可以使用渲染LaTeX数学表达式 KaTeX:\nGamma公式展示 $\\Gamma(n) &#x3D; (n-1)!\\quad\\foralln\\in\\mathbb N$ 是通过欧拉积分\n$$\\Gamma(z) &#x3D; \\int_0^\\infty t^{z-1}e^{-t}dt,.$$\n\n\n\n\n\n\n\n\n\n你可以找到更多关于的信息 LaTeX 数学表达式here.\n新的甘特图功能，丰富你的文章​mermaid gantt         dateFormat  YYYY-MM-DD         title Adding GANTT diagram functionality to mermaid         section 现有任务         已完成               :done,    des1, 2014-01-06,2014-01-08         进行中               :active,  des2, 2014-01-09, 3d         计划一               :         des3, after des2, 5d         计划二               :         des4, after des3, 5d ​\n\n关于 甘特图 语法，参考 这儿,\n\nUML 图表可以使用UML图表进行渲染。 Mermaid. 例如下面产生的一个序列图：\n​&#96;&#96;&#96;mermaidsequenceDiagram张三 -&gt;&gt; 李四: 你好！李四, 最近怎么样?李四–&gt;&gt;王五: 你最近怎么样，王五？李四–x 张三: 我很好，谢谢!李四-x 王五: 我很好，谢谢!Note right of 王五: 李四想了很长时间, 文字太长了不适合放在一行.\n李四–&gt;&gt;张三: 打量着王五…张三-&gt;&gt;王五: 很好… 王五, 你怎么样?​&#96;&#96;&#96;\n这将产生一个流程图。:\n​mermaid graph LR A[长方形] -- 链接 --&gt; B((圆)) A --&gt; C(圆角长方形) B --&gt; D&#123;菱形&#125; C --&gt; D ​\n\n关于 Mermaid 语法，参考 这儿,\n\nFLowchart流程图我们依旧会支持flowchart的流程图：\n​&#96;&#96;&#96;mermaidflowchatst&#x3D;&gt;start: 开始e&#x3D;&gt;end: 结束op&#x3D;&gt;operation: 我的操作cond&#x3D;&gt;condition: 确认？\nst-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op​&#96;&#96;&#96;\n\n关于 Flowchart流程图 语法，参考 这儿.\n\n导出与导入导出如果你想尝试使用此编辑器, 你可以在此篇文章任意编辑。当你完成了一篇文章的写作, 在上方工具栏找到 文章导出 ，生成一个.md文件或者.html文件进行本地保存。\n导入如果你想加载一篇你写过的.md文件，在上方工具栏可以选择导入功能进行对应扩展名的文件导入，继续你的创作。\n自己的理解\nmarkdown可以兼容html语法\n\n","slug":"markdown-example","date":"2023-10-17T09:01:15.000Z","categories_index":"","tags_index":"blog","author_index":"PIKO"},{"id":"b9663f58f18133b35bfe243f3e916a80","title":"Hello World","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.My own theme: aurora.(https://aurora.tridiamond.tech/)\nQuick StartCreate a new postbash$ hexo new &quot;My New Post&quot;More info: Writing\nRun serverbash$ hexo serverMore info: Server\nGenerate static filesbash$ hexo generateMore info: Generating\nDeploy to remote sitesbash$ hexo deployBy the Wayactually you can combine Generate with Deploy\nbash$ hexo g &amp;&amp; hexo dthe new markdown file will be restored in yourblog&#x2F;source&#x2F;_post&#x2F;\nOf course SSH deployment is much slower than the local deployment, you can use the command as follow to deploy in local so that you can check your new blog quickly.\nbash$ hexo sMore info: Deployment\n","slug":"hello-world","date":"2023-10-17T01:55:02.000Z","categories_index":"","tags_index":"blog","author_index":"PIKO"}]