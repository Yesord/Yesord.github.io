[{"id":"60d1e2155fb0daf3f82164b48bebcc61","title":"科研工具软件推荐","content":"前言作为准研究生（博0 GAP仔），现在要开始逐渐养成科研习惯。大二大三当时没有系统性的用工具进行管理，当时并不觉得我会直接攻读博士，也就是玩玩，就瞎子拉琴——瞎扯。做事没有工具辅助吧就好像近视出门不带眼镜，说能走路吧能走，但就是膈应。\nZotero——文献管理Zotero 是一款免费开源的文献管理软件，可以很方便的从浏览器上下载文献，在中文社区拥有很多的插件。\n下载\n    \n\n\n官网\n插件推荐Zotero中文社区\n小绿鲸一款功能集成度高、对中文用户友好、且AI特色鲜明的综合性科研工具，功能更集成不用自己折腾插件，但是页面就有一股中国软件特有的那种推销感，就搁那种很明显的地方放VIP功能这样，但总的来说毕竟是国人做的，翻译质量与实际体验更加符合中国人习惯。\n下载\n    \n\n\n官网\n","slug":"科研工具软件推荐","date":"2025-11-19T02:37:43.000Z","categories_index":"","tags_index":"tool","author_index":"Yesord"},{"id":"54bcbd803e2c6237373d5a7bedf51a81","title":"四非保研资料整理与分享","content":"保研经历本人已在其他网站分享过，可以直通我的主站或者深大飞跃手册\n保研简历与PPT百度网盘：提取码: 2025 \n\n\n\n\n\n\nTIP\n如有更多想和博主聊的欢迎致信邮箱 or QQ or WeChat\n\n\ntxt还在更新ing","slug":"四非保研简历PPT模板分享","date":"2025-11-18T08:45:47.000Z","categories_index":"","tags_index":"solution","author_index":"Yesord"},{"id":"bce753e6b1d9700e434fd870f7d4ef6a","title":"计算成像发展研究","content":"前言本篇博文会记录我在计算成像领域所读过的相关综述，是对于该领域一些进展的汇总。\n端到端光学算法联合设计传统的成像链路是前期进行光设设计镜头，与后期图像处理算法设计是分离的。因而可以寻找一种方法去对光学设计和图像处理算法进行同时优化，不单单在意镜头的成像质量与后期图像算法的处理质量，而是在乎全链路最后的成像质量，此即为端到端光学算法联合设计。总的来说就是训练一套成像系统，他的最终成像质量最优，允许硬（软）件“偷懒”，但要求软（硬）件更“聪明”，最终以最低的成本实现最好的整体效果。\n高动态范围成像即HDR(high Dynamic Range Imaging)，所谓动态范围是通过曝光值EV的差来描述的。HDR是用来解决大光比问题的，动态范围越大，就能获取更亮或者更暗的图像细节信息。\n低光照成像不同手机的相机在白天的强光环境下拍照差异并不明显，然而在夜晚弱光情况下则差距明显。其原因是成像依赖于镜头收集物体发出的光子，且传感器在光电转换、增益和模数转换等一系列过程中会有不可避免的噪声；白天光线充足，信号的信噪比高，成像质量很高；晚上光线微弱，信号的信噪比下降数个数量级，成像质量低。\n无透镜成像空间编码代替传统的几何光学成像方法，CMOS上得到的图像类似散斑，通过后期解码编码片的图案类似傅里叶的解法复原图像。方法有CDI，CMI\n参考文献https://blog.csdn.net/dguopenmvp/article/details/108142912\n光场成像即LFI(Light Field Imaging)，7D光场模型P(x, y, z, θ, Φ, λ, t)，而传统的图像传感器只能得到P(x, y, λ, t)，丢失了深度z、水平角度θ、垂直角度Φ，实际无法以低成本的方式取得7D数据，因而现在主要采用的是4D光场模型(u,v,x,y)，(u,v)、(x,y)分别是两个不共面的平面，可以通过这两个平面去计算光的角度这些轴向信息。\nLytro相机，可以后期对焦，但是空间分辨率受限，商业化受阻。\n参考文献https://blog.csdn.net/weixin_44690935/article/details/119176556https://zhuanlan.zhihu.com/p/645534366\n光谱成像由传统彩色成像技术发展而来,能够获取目标物体的光谱信息。每个物体都有自己独特的光谱特征,就像每个人拥有不同的 指纹一样,光谱也因此被视为目标识别的“指纹”信息。通过获取目标物体在连续窄波段内的光谱图像,组成空间维度和光谱维度的数据立方体信息,可以极大地增强目标识别和分析能力。\n主动3维成像简单来说就是通过传感器本身去主动发射光（为了不影响人眼视觉和能量角度思考，一般打的光是红外的）然后通过接收反射光实现深度测量进而实现3维成像，做出来的产品就是深度相机，intel和orbbec两家公司在深度相机产品上算龙头了。主流方案有iTOF和dTOF，简单来说\n计算摄影学传统摄影学主要着眼于使用光学器件更好地进行成像，如佳能、索尼等相机厂商对于镜头的研究；与之相比，计算摄影学则更侧重于使用数字计算的方式进行图像拍摄。在光学镜片的物理尺寸、成像质量受限的情况下，如何使用合理的计算资源，绘制出用户最满意的图像。\n","slug":"计算成像发展研究","date":"2025-11-02T03:10:06.000Z","categories_index":"","tags_index":"research","author_index":"Yesord"},{"id":"8ef1d34961ad10ae08a2490597961436","title":"Office破解方法","content":"前言今天重装了家里的电脑的系统，遇到挺多屁事，例如网卡驱动没装上不了网但上不了网没法下载网卡驱动之类的，但正好也重装了 Office 因此我在这里想记录一下我的破解Office的方法以供下次在破解的时候有一个方法论。\nOffice Tool Plus 一键式破解Office Tool Plus\n密钥 kms.03k.org\n","slug":"Office破解方法","date":"2025-10-09T02:04:26.000Z","categories_index":"","tags_index":"solution","author_index":"Yesord"},{"id":"7334b3419d5caf6520825251532abbd2","title":"深度学习细胞虚拟染色研究","content":"前言这是我\n目标利用所提供的不成对的数据集（即输入和输出图像并不是完全对称）以及深度学习技术对血细胞虚拟染色。实现从黑白图像（灰度图像）到RGB彩色图像的迁移。\n\n数据集\n数据集下载（百度网盘）：链接: https://pan.baidu.com/s/1WUaljE6WreB_6TvWlPrzGQ提取码: m9ek\n\n\n实现过程前期调研项目需要去完成风格迁移的任务去完成细胞染色，而项目已经提供现成的使用CycleGAN的方案，风格迁移可以使用很多模型例如 UGATIT StarGANv2之类考虑到数据集不对称等最终还是选择了最成熟的CycleGAN。完成去噪通过传统的open CV方案是最省事最快速的，但是后续的调参过程很繁琐且扩展性不够，因此选择使用去噪模型进行图像数据的预处理。以前使用过DeblurNetv2做过去噪，但是效果一般部署起来也麻烦。通过在GitHub的寻找也找到了合适的模型NAFNet——2022年的SOTA。\n本次实验我采用NAFNet与CycleGAN复合的方法来进行细胞虚拟染色的处理，流程分为4个部分1）数据集生成2）数据预处理3）CycleGAN模型训练4）细胞上色\n项目依赖详见CycleGAN GitHub仓库详见NAFNet GitHub仓库\n数据集生成首先，提供的实验数据总共约720张（350灰度+350RGB），对于模型训练来说数据集相对偏小，最好进行数据集的扩增。将原始包含350张灰度图像的训练集通过数据增强技术扩增至1750张（实现5倍扩增）。在数据集构建思路上，针对不同图像类型设置差异化处理策略：仅对灰度图像开展噪声处理时，高斯噪声的归一化标准差范围设定为0.02 - 0.05，散粒噪声（泊松噪声）作用于15% - 30%的像素值；对于灰度与RGB图像的同步处理场景，采用幅度为-2%2%的缩放、-6%6%的旋转以及最高1px的模糊等变换操作，且上述处理过程以50%的随机概率执行。数据集划分阶段，按照8:1:1的比例分别确定训练集、验证集与测试集的样本量。此外，基于CycleGAN数据集不成对的特性，将数据集划分为trainA和trainB两部分，以满足模型训练过程中对不同域数据的适配需求\n\n    \n\n部分CycleGAN的增强数据集展示\n\n\n虽然没有时间进行去噪模型的训练，但是我也准备好了去噪模型的数据集针对NAFNet模型开展数据集构建工作，将初始包含350张灰度图的训练集通过数据增广技术扩展为1750张。数据增广的构建思路涵盖多种噪声与模糊扰动：引入归一化标准差范围为0.02 - 0.05的高斯噪声，添加占比15% - 30%像素值的散粒噪声（泊松噪声），施加模糊半径在1 - 3px间的运动模糊，且上述扰动以50%的随机概率应用；数据集按照7:2:1的比例划分为训练集、验证集与测试集，同时遵循NAFNet“数据集必须一一对应”的特性，将生成的含噪声数据作为模型输入（input），原始无噪声数据作为目标输出（target），以此完成适配NAFNet训练需求的数据集构建。\n\n    \n\n部分NAFNet的增强数据集展示\n\n数据预处理采用NAFNet对含噪或模糊的输入图像进行处理。先在Google Colab上进行简单测试，看模型的性能。从可视化结果来看，输入图像存在的噪声与模糊问题在经NAFNet处理后得到明显改善，输出图像在细节保留与清晰度提升方面表现优异；同时，通过重建质量分析可知，该模型处理后图像的平均结构相似性（SSIM）达到0.867，平均峰值信噪比（PSNR）为29.09 dB，定量指标也验证了NAFNet在图像去噪去模糊任务中能够有效提升图像质量，展现出良好的重建能力与去噪去模糊性能。\n\n    \n\n自制数据集去噪效果\n\n\n    \n\n原数据集去噪效果\n\n\n模型训练在CycleGAN的训练实验中，采用细胞染色数据集（Cell Staining Dataset）开展训练，设置默认参数并训练200个epoch。从训练损失可视化结果来看，模型收敛性良好，循环一致性约束在训练过程中效果显著；不过生成器与判别器的博弈平衡性存在不足，其中A→B方向的判别器Dₐ损失过低，疑似判别器过强，而对应的生成器Gₐ损失偏高，该情况可能对生成质量造成限制。\n\n    \n\nCycleGAN训练效果\n\n\n回归预测在CycleGAN训练效果的实验中，针对血细胞灰度图虚拟染色的任务，模型呈现出特定的性能表现。从生成结果的可视化来看，输入的血细胞灰度图经CycleGAN处理后，可转换为具有染色特征的图像；其中不同样本的重建质量存在差异，对应“重建质量分析”指标显示，部分案例平均SSIM分别为0.561、0.749、0.987，平均PSNR分别达到17.43 dB、18.05 dB、39.41 dB，这些量化结果反映了生成图像与目标染色图像在结构相似性与峰值信噪比层面的匹配程度。整体而言，该模型能够实现血细胞灰度图的虚拟染色，但在泛化能力方面存在不足，当输入为含噪声的血细胞灰度图时，其生成染色图像的质量较差。\n\n    \n\ndegraded 图像染色\n\n\n    \n\nnoisy 图像染色\n\n\n    \n\nnormal图像染色\n\n参考网址https://github.com/nightrome/really-awesome-ganhttps://github.com/subeeshvasu/Awesome-Deblurringhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pixhttps://github.com/megvii-research/NAFNethttps://blog.csdn.net/weixin_46235765&#x2F;article&#x2F;details&#x2F;119026209https://blog.csdn.net/weixin_44523062&#x2F;article&#x2F;details&#x2F;105631328https://blog.csdn.net/frothmoon/article/details/90341693https://blog.csdn.net/qq_40280673&#x2F;article&#x2F;details&#x2F;125921937https://blog.csdn.net/qq_24224067&#x2F;article&#x2F;details&#x2F;104538137https://zhuanlan.zhihu.com/p/521264492https://blog.csdn.net/qq_45368632&#x2F;article&#x2F;details&#x2F;131769856\n","slug":"深度学习细胞虚拟染色研究","date":"2025-10-02T09:06:57.000Z","categories_index":"DeepLearning","tags_index":"ComputerVision,python","author_index":"Yesord"},{"id":"4e143a87dfacf185ad49d19d39f24dfc","title":"ROS2 桥接 MQTT 服务器与 Web 通信","content":"前言最近在前海实习的过程，初学 ROS2，项目经理给到业务要做产品的展示，需要将在综保区的ROS自动驾驶车的数据回传到网页进行显示，通过调研可以使用 MQTT 进行简单的数据回传与处理。正好公司也要新进一台服务器，本身要用来部署 DeepSeek，但可以留点空间来做 MQTT 服务器。因此开始钻研如何实现 ROS2-MQTT-WEB 的通信。顺便也记录一下一些简单的 ROS2 命令调试。\n\n\n\n\n\n\nTIP\n代码已上传 GitHub \n\n\nDemo实现思路简单实现的一个 DEMO，就是我 ROS2 开一个 HelloWorld 节点向外广播每 2 秒发一个 Hello，然后写一个 ROS2 与 MQTT bridge 的节点，订阅 HelloWorld 节点的 Hello string 并将 string 传给 MQTT Server 然后在网页端通过 WebSocket 将数据从 MQTT Server 上拿下来显示在网页上，因此也只是ROS2 -&gt; MQTT -&gt; WEB 的单向传输。\nMQTT 相关依赖bash# 安装 MQTT 服务器\nsudo apt update &amp;&amp; sudo apt install -y mosquitto\n\n# 安装 MQTT 客户端\nsudo apt update &amp;&amp; sudo apt install -y mosquitto-clients\n# 安装 Python MQTT 客户端库（兼容版本）\npip3 install paho-mqtt==1.6.1\nHelloWorld节点本身就是一个每 2s 发送 helloworld 字符串消息的发布者节点，没什么好说的。详细实现在GitHub上（hello_world_publisher.py）上，网上也有很多相关的代码。\nbash\n# 编译\ncd [dev_ws] #你的工作空间位置\ncolcon build --packages-select hello_world_node\n\n# source\ncd [dev_ws] #你的工作空间位置\nsource install/setup.bash\n\n# 运行 hello_world_node 包中的 hello_world_publisher 节点\nros2 run hello_world_node hello_world_publisher\n\n# 查看 ROS2 上正在运行的所有节点\nros2 node list \n\n# 查看 ROS2 正在运行的所有的话题\nros2 topic list\n\n# 查看 helloworld 发布话题信息 可以看到话题发出的 ROS 信息类型与变量名称，这很重要，后面写转接需要一一对应上\nros2 topic info hello_world\n\n# 详细看 helloworld 话题发送了什么信息\nros2 topic echo hello_world\n简单的桥接节点通过 rclpy 创建将 ROS2 信息上传 MQTT 的节点，调用 MQTT 相关类实现 MQTT 协议的相关逻辑。具体实现在 GitHub上 （ros_mqtt_bridge_node）。\nbash# 编译\ncd [dev_ws] #你的工作空间位置\ncolcon build --packages-select ros_mqtt_bridge_node\n\n# source\ncd [dev_ws] #你的工作空间位置\nsource install/setup.bash\n\n# 运行 ros_mqtt_bridge 节点的发布 hello_world_mqtt_bridge 话题\nros2 run ros_mqtt_bridge_node hello_world_mqtt_bridge桥接到 MQTT 上的相关参数在 config 里的 yaml文件\nyaml\n# MQTT Bridge Configuration for Hello World Node\nmqtt_broker_host: &quot;localhost&quot;\nmqtt_broker_port: 1883\nmqtt_topic_prefix: &quot;ros2&quot;\nmqtt_client_id: &quot;hello_world_bridge&quot;\nmqtt_username: &quot;&quot;\nmqtt_password: &quot;&quot;\n\n简单网页搭建通过 WebSocket 打通 MQTT 服务器与 Web 之间的通信。因此需要配置好 MQTT的 WebSocket 相关设置。\nbash# 创建正确格式的 Mosquitto 配置文件\nsudo tee /etc/mosquitto/conf.d/websocket.conf &gt; /dev/null &lt;&lt; &#39;EOF&#39;\n# WebSocket 支持配置\nlistener 1883\nprotocol mqtt\n\n# WebSocket 监听器 设置为 9001 端口，注意防火墙\nlistener 9001\nprotocol websockets\n\n# 允许匿名连接（用于开发测试）\nallow_anonymous true\n\n# 日志级别\nlog_type all\nEOF\n\n# 重启 Mosquitto 服务\n\nsudo systemctl restart mosquitto\nsudo systemctl status mosquitto\n\n# 检查端口监听状态 验证 MQTT的 WebSocket 服务正常运行 能看到 mqtt 相关的就行\nnetstat -tlnp | grep 9001\n接下来就是写相关的网页前端代码，详细代码在 GitHub上，就是简单的 HTML + JS + CSS。\nbash# 可以通过 start_web_panel.sh 脚本运行\ncd /web # 存储网页代码的文件夹\n\n# 给权限\nchmod +x ./start_web_panel.sh\n\n# 运行脚本\n./start_web_panel.sh链路验证ROS2 内部节点 -&gt; MQTT 服务器 -&gt; Web 显示\n验证节点连接bash\n# 另起终端 通过 rqt 勘察节点之间的关系\nros2 run rqt_graph rqt_graph\n\n\n\n\n\n\n节点连接关系\n\n    \n\n\n\n\n通过这个可以看到我们成功将 helloworld 信息从发布者节点通过 hello_world 话题传给了桥接节点。\n验证 MQTT 服务器bash# 订阅所有 ros2 相关话题，这里是用本机作为 MQTT 服务器，显示话题名和内容，提前保证 1883 端口的可用性\nmosquitto_sub -h localhost -p 1883 -t &quot;ros2/#&quot; -v\n能在命令行 echo 到话题的 json格式的数据就是证明能成功上传到 MQTT 服务器\n验证 Web 接收在浏览器开启端口8080打开监控界面\ntxthttps://localhost:8080/index.html\n    \n\n\n结果证明可以跑通，能在 Web 上接收到消息\ntxt总体链路可以跑通v1优化思路将只对 helloworld 节点的硬编码桥接改成对任意 ROS2 节点可以进行编码改变的版本，通过 config 的 yaml 实现对不同节点进行配置。目前主要需要对图像传输和字符传输这两种不同形式的数据进行传输，这两种数据需要差异化编写。代码已上传GitHub v1\n","slug":"ROS2桥接MQTT服务器与Web通信","date":"2025-07-17T08:43:02.000Z","categories_index":"","tags_index":"ROS","author_index":"Yesord"},{"id":"222425fd00a7157700d67225b13528c8","title":"上传 GitHub 的若干方法","content":"前言突发奇想简单记录\nGitHub Desktop (APP)下载GitHub Desktop\n\n    \n\n\n\n\n\n\n\n\n优点\n操作简单易懂，不用敲任何一行命令就可以上传下载仓库，而且似乎自带翻墙\n\n\n\n\n\n\n\n\n\n缺点\n只有 Windows 和 Mac 端的，Linux 的官方没有，只有民间的开源项目。\n\n\nGit (CMD)git 命令行上传仓库的流程\nbash# 0. 第一次使用需要配置用户和邮箱\ngit config --global user.email &quot;user@email.com&quot;\ngit config --global user.name &quot;username&quot;\n\n# 1. 进入目录并初始化\ncd /home/xx/Project/\ngit init\n\n# 2. 创建.gitignore\ncat &gt; .gitignore &lt;&lt; &#39;EOF&#39;\nbuild/\ninstall/\nlog/\ndevel/\n__pycache__/\n*.pyc\n*.pyo\n.Python\n.vscode/\n.idea/\n*.swp\n*~\n.DS_Store\n*.tmp\n*.log\nnode_modules/\n.env\nEOF\n\n# 3. 创建README（使用上面的内容）\nnano README.md\n\n# 4. 添加文件\ngit add .\n\n# 5. 提交\ngit commit -m &quot;Initial commit: xxxx&quot;\n\n# 6. 添加远程仓库（记得替换URL）\ngit remote add origin https://github.com/YOUR_USERNAME/REPO_NAME.git\n\n# 7. 推送\ngit branch -M main\ngit push -u origin main\n\n# 8. 报错网络连接可以接代理\ngit config --global http.proxy 127.0.0.1:(vpn代理端口号）\n","slug":"上传GitHub的若干方法","date":"2025-07-17T07:55:50.000Z","categories_index":"Skill","tags_index":"GitHub","author_index":"Yesord"},{"id":"6a333487c46440dcde85a062ca57e2b8","title":"LabView 双通道电压采样系统设计","content":"前言邻近期末，选修的 LabView 的课程也要开始做期末课设，在此记录本次 LabView 课设的设计思路与具体实现。\n要求设计一个智能双通道电压监测系统，要求实现电压自动采集、显示、存储功能。同时，用户可以设置电压值上限，当电压超限时，计数器能够显示超限的个数，并具有报警功能（LED闪烁+蜂鸣提示）。\n设计思路本设计以LabVIEW模块化架构为核心，构建从用户登录到数据闭环存储的全流程智能监测系统。系统初始化阶段通过用户身份双因子验证确保操作权限，登录后采用VI引用动态加载技术无缝切换至电压监测界面；硬件层通过DAQmx多通道采样链实现双路电压同步采集，软件层结合实时极值计算与动态阈值比对，触发四路独立计数器（CH1&#x2F;CH2超上&#x2F;下限）精确统计超限采样数，并联动多模态报警（LED闪烁+蜂鸣脉冲）；数据存储端利用时间戳唯一性编码生成带通道标识的文件名，通过异步写入机制将电压值保存为标准化.xlsx格式，形成”采集→判断→报警→存储” 的闭环数据流，在确保工业级实时性的同时满足数据可追溯性需求。\n功能拆解拆解出的功能主要如下\n\n\n\n\n\n\n登录界面\n\n用户输入账户密码判断\n界面切换\n\n\n\n\n\n\n\n\n\n监测界面\n\nDAQ采样\n超限判断\nLED和蜂鸣器报警\n超限采样计数器\n保存文件\n\n\n\n具体实现用户输入判断模块设计\n    \n\n\n通过双路字符串比对实现用户登录验证：程序将用户输入的用户名和密码分别与数据库中的用户密码数据进行独立匹配，系统根据输入的用户名检索出对应的密码，再通过相等运算和用户输入的密码进行比对得出最终认证结果——仅当条件满足（即用户名和密码一一对应数据库）时输出”1”（认证通过），否则输出”0”（认证失败），其核心逻辑构成身份鉴权的双重校验机制。\n切换页面模块设计\n    \n\n\n通过FP.Open打开目标VI引用，由Run VI节点运行该VI并激活Activate模式；Wait Until Done确保目标VI完成初始化后，依次设置其前面板属性——包括隐藏状态（FP.State:3→Hidden）、自定义标题（FP.CustomTitle）和最小化尺寸（FP.Minimum Size），最终通过Auto Dispose Ref自动释放VI引用，完成整个页面切换流程。\nDAQ采样模块设计\n    \n\n\n首先通过CH1&#x2F;CH2通道选择模块定义采集源（如”Dev1&#x2F;scopeCH0”），其次配置输入终端参数（含±10V量程保护），再设置采样模式与速率（10-1000Hz可调，采样点数10-5000），最终由硬件上的DA模块按既定参数启动同步采集，完成数据从硬件到软件的标准传输链路。\n超限判断模块设计\n    \n\n\n通过并行双通道实时判断电压状态：独立计算CH1&#x2F;CH2的最大值与最小值，并动态比对预设上下限阈值（用户可设）；当任一通道电压超越阈值时，触发”超限判断与闪烁检测”模块，联动LED闪烁报警，同时更新超限计数，实现双通道电压状态的同步监测与实时响应。\n超限采样计数模块设计\n    \n\n\n通过双通道独立阈值判断实现电压超限统计：CH1和CH2通道分别实时与用户设定的最大值（上限）、最小值（下限）比对，当CH1或CH2电压超出阈值时，对应通道的超上限采样数或超下限采样数计数器（共4个独立计数器）执行增量计数。系统持续监测两路信号，精确记录各通道的双向越界事件次数，为电压异常提供量化统计依据。\n文件处理模块设计\n    \n\n\n实现文件存储的自动化管理：用户设置保存路径后，系统通过时间格式化模块（%Y-%m-%d-%H-%M-）和（%S）生成精确到分和秒的时间戳，结合”10”等参数运算结果实现10秒计数一次的时间戳，最终调用Create File和Read&#x2F;Write模块创建文件（格式为xlsx），将时间戳与数据异步写入指定路径，完成数据持久化存储的闭环流程。\n功能展示txt登录界面前面板\n    \n\n\ntxt监测界面前面板\n    \n\n   \n\n\n\n\n\n\n系统功能介绍\n登录区：输入正确凭证（如用户名”许若伦”，密码”123456”）后成功跳转至监测界面，错误输入时弹出警示框（”用户名或密码错误”提示）核心功能区：a)\t双通道波形显示：CH1（蓝色）与CH2（红色）电压波形实时刷新b)\t阈值设置面板：用户可独立设置各通道上下限（如CH1上限&#x3D;5.0V）c)\t超限计数器：四组独立数据显示框（CH1超上&#x2F;下限 + CH2超上&#x2F;下限）控制区：a)\t启动&#x2F;停止采集按钮b)\t数据保存路径选择器c)\tCH1&#x2F;CH2采样启停开关\n\n\ntxt测试面板设置产生正弦波\n    \n\n\ntxt数字波形发生器产生三角波\n    \n\n\n正弦波信号源：NI MAX 测试面板输出 1Hz 正弦波（幅值 3V ），通过杜邦线接入采集卡 ai0 通道三角波参数：频率 5Hz ，幅值 5V ，偏置 1V 的三角波信号接入采集卡 ai1 通道\ntxt电压监测界面正常运行\n    \n\n\ntxt电压监测界面超限运行\n    \n    \n\n\n","slug":"LabView-双通道电压采样系统设计","date":"2025-06-19T02:43:52.000Z","categories_index":"Electronic","tags_index":"LabView","author_index":"Yesord"},{"id":"a699e46203152d47e994e421bf47ac6e","title":"iOS回退版本记录—iPad Mini 2","content":"今天兴起，用老兵 iPad Mini 2 来当电子书但是就开 iBooks 都差点被干碎，忍不了，直接开回退版本。\n前置工作通过谷歌了解到对于 2016年以前的老 Apple 来说 iOS 10.3.3 版本是最适合的，耗电量少而且不卡。我的回退目标于是就定在了 iOS 10.3.3。\n查看当前版本 iOS 12.5.5 能不能直接刷到 iOS 10.3.3。\n\n    \n\n\n\nemmm~~看来是不行的，现在已经是2025年了 apple 官方已经不给我这 2014 年的老兵留后路了。再通过谷歌了解到有三个工具可以用来回退版本（Only to iOS 10.3.3） Vieux、1033-OTA-Downgrader、LeetDown 。这些工具只能在 Mac 上用。这三个工具在 GitHub 上都可以找到，我最后选择的是使用 LeetDown。\n进行回退将 LeetDown 工具打开，并将 iPad Mini 2 连接到 Mac 上（我的 Mac 是 MacBook Pro 2020 M1 ， 网上有大佬说的 M1 Mac 不行的，但现在 LeetDown 版本更迭之后可以支持 Apple Silicon 了）\n接着需要将 iPad Mini 2 开启DFU模式\n\n如何开启 DFU 模式\n按住关机键2秒，然后同时按住关机键和Home键8-10秒；最后，只要按住Home键15秒。看到黑屏状态后 基本就是进入了。\n\n    \n\n\n\n\n成功打开 DFU 模式则 LeetDown 会显示如下的信息\n\n    \n\n\n接着就按下 Select 10.3.3 iPSW 按钮， LeetDown 会自行下载，只需要等待它下载完就可以按下 Downgrade 进行降级操作。\n\n接下来就是漫长的等待时间。。。。\n\n降级完成之后 iPad就会正常亮出 apple 的经典标志意味着 ok 啦。\n\n    \n\n\n回退完毕这样就完成啦！总体流程还是挺顺利的。\n\n    \n\n\n\n    \n\n\n\n    \n\n\n\n    \n\n","slug":"iOS回退版本记录—iPad-Mini-2","date":"2025-03-28T10:35:27.000Z","categories_index":"Electronic","tags_index":"iOS","author_index":"Yesord"},{"id":"2507a275debe86f5cee26058dc7ad298","title":"从零开始搭建 HTML 静态个人展示主页","content":"最近有意愿要找实习和准备后期的研究生面试，决定写一个静态网站来展示我的一些技能和履历。\n搭建 HTML 网页最终效果展示\n    \n        \n    \n\n\n来看看嘛\n搭建步骤txt1. 创建[web]文件夹 （[web]为你自己命的名）\n2. 创建[web]/CN/index.html\n将以下内容输入 index.html\nhtml&lt;!DOCTYPE html&gt;\n&lt;html lang=&quot;en&quot;&gt;\n&lt;head&gt;\n    &lt;meta charset=&quot;UTF-8&quot;&gt;\n    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;\n    &lt;title&gt;Nolen的个人介绍页&lt;/title&gt;\n    &lt;link rel=&quot;icon&quot; href=&quot;../static/img/favicon.ico&quot;&gt;\n    &lt;link rel=&quot;stylesheet&quot; href=&quot;../static/css/style.css&quot;&gt;\n    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css&quot;&gt;\n    &lt;link rel=&quot;stylesheet&quot; href=&quot;../static/css/color-1.css&quot; class=&quot;alternate-style&quot; title=&quot;color-1&quot; &gt;\n    &lt;link rel=&quot;stylesheet&quot; href=&quot;../static/css/color-2.css&quot; class=&quot;alternate-style&quot; title=&quot;color-2&quot; &gt;\n    &lt;link rel=&quot;stylesheet&quot; href=&quot;../static/css/color-3.css&quot; class=&quot;alternate-style&quot; title=&quot;color-3&quot; &gt;\n    &lt;link rel=&quot;stylesheet&quot; href=&quot;../static/css/color-4.css&quot; class=&quot;alternate-style&quot; title=&quot;color-4&quot; &gt;\n    &lt;link rel=&quot;stylesheet&quot; href=&quot;../static/css/color-5.css&quot; class=&quot;alternate-style&quot; title=&quot;color-5&quot; &gt;\n\n&lt;/head&gt;\n&lt;body class=&quot;dark&quot;&gt;\n\n    &lt;div class=&quot;main-container&quot;&gt;\n        \n        &lt;div class=&quot;aside&quot;&gt;\n            &lt;div class=&quot;logo&quot;&gt;\n                &lt;a href=&quot;https://academic.yesord.top&quot;&gt;&lt;span&gt;Nolen&lt;/span&gt;&lt;/a&gt;\n            &lt;/div&gt;\n\n            &lt;div class=&quot;nav-toggler&quot;&gt;\n                &lt;span&gt;&lt;/span&gt;\n            &lt;/div&gt;\n\n            &lt;ul class=&quot;nav&quot;&gt;\n                &lt;li&gt;&lt;a href=&quot;#home&quot; class=&quot;active&quot;&gt;&lt;i class=&quot;fa fa-home&quot;&gt;&lt;/i&gt;首页&lt;/a&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;a href=&quot;#about&quot; &gt;&lt;i class=&quot;fa fa-user&quot;&gt;&lt;/i&gt;关于我&lt;/a&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;a href=&quot;#services&quot; &gt;&lt;i class=&quot;fa fa-list&quot;&gt;&lt;/i&gt;技能&lt;/a&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;a href=&quot;#portfolio&quot; &gt;&lt;i class=&quot;fa fa-briefcase&quot;&gt;&lt;/i&gt;作品集&lt;/a&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;a href=&quot;#contact&quot; &gt;&lt;i class=&quot;fa fa-comments&quot;&gt;&lt;/i&gt;联系我&lt;/a&gt;&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/div&gt;\n\n        &lt;div class=&quot;main-content&quot;&gt;\n            &lt;section class=&quot;home section active&quot; id=&quot;home&quot;&gt;\n                &lt;div class=&quot;container&quot;&gt;\n                    &lt;div class=&quot;row&quot;&gt;\n                        &lt;div class=&quot;home-info padd-15&quot;&gt;\n                            &lt;h3 class=&quot;hello&quot;&gt;我的名字是 &lt;span class=&quot;name&quot;&gt;许若伦&lt;/span&gt;&lt;/h3&gt;\n                            &lt;!-- Nolen Hsu --&gt;\n                            &lt;h3 class=&quot;profession&quot;&gt;我是一名 &lt;span class=&quot;typing&quot;&gt;大学生&lt;/span&gt;&lt;/h3&gt;\n                            &lt;p&gt;                            &lt;p&gt;我是一个拥有超过10年丰富经验的学生。我的经验包括进行电子设计、网页设计等...&lt;/p&gt;&lt;/p&gt;\n                            &lt;a href=&quot;../static/resource/resume.pdf&quot; class=&quot;btn &quot;&gt;查看我的简历&lt;/a&gt;\n                        &lt;/div&gt;\n\n                        &lt;div class=&quot;home-img padd-15&quot;&gt;\n                            &lt;img src=&quot;../static/img/me.png&quot; alt=&quot;hero image&quot;&gt;\n                        &lt;/div&gt;\n                    &lt;/div&gt;\n                &lt;/div&gt;\n            &lt;/section&gt;\n\n            &lt;section class=&quot;about section &quot; id=&quot;about&quot;&gt;\n                &lt;div class=&quot;container&quot;&gt;\n                    &lt;div class=&quot;row&quot;&gt;\n                        &lt;div class=&quot;section-title padd-15&quot;&gt;\n                            &lt;h2&gt;关于我&lt;/h2&gt;\n                        &lt;/div&gt;\n                    &lt;/div&gt;\n\n                    &lt;div class=&quot;row&quot;&gt;\n                        &lt;div class=&quot;about-content padd-15&quot;&gt;\n                            &lt;div class=&quot;row&quot;&gt;\n                                &lt;div class=&quot;about-text padd-15&quot;&gt;\n                                    &lt;h3&gt;我是许若伦，一名&lt;span&gt;大学生&lt;/span&gt;&lt;/h3&gt;\n                                        &lt;p&gt;我的研究兴趣集中在光学测量领域，特别是事件相机技术的应用和发展。在这一领域，我积极参与相关的研究项目，探索事件相机在动态场景中的高精度成像技术。\n                                        在技术技能方面，我在嵌入式开发方面有扎实的经验，并且熟悉各种开发平台和编程语言，使我能够有效地进行硬件和软件的协同设计。此外，我对硬件系统设计和实现有深入的理解，能够独立完成从概念设计到原型制作的所有阶段。\n                                        我热衷于将理论知识应用于实际问题，并希望通过进一步的研究和实践，推动光学测量技术的创新和发展。同时，我渴望与同行和专家进行深入的学术交流，以拓宽我的视野并提高我的专业能力。&lt;/p&gt;\n                                &lt;/div&gt;\n                            &lt;/div&gt;\n\n                            &lt;div class=&quot;row&quot;&gt;\n                                &lt;div class=&quot;personal-info padd-15&quot;&gt;\n                                    &lt;div class=&quot;row&quot;&gt;\n                                        &lt;div class=&quot;info-item padd-15&quot;&gt;\n                                            &lt;p&gt;生日: &lt;span&gt;2004年12月16日&lt;/span&gt;&lt;/p&gt;\n                                        &lt;/div&gt;\n\n                                        &lt;div class=&quot;info-item padd-15&quot;&gt;\n                                            &lt;p&gt;年龄: &lt;span&gt;20&lt;/span&gt;&lt;/p&gt;\n                                        &lt;/div&gt;\n\n                                        &lt;div class=&quot;info-item padd-15&quot;&gt;\n                                            &lt;p&gt;网站: &lt;span&gt;guide.yesord.top&lt;/span&gt;&lt;/p&gt;\n                                        &lt;/div&gt;\n\n                                        &lt;div class=&quot;info-item padd-15&quot;&gt;\n                                            &lt;p&gt;邮件: &lt;span&gt;xuruolun666@gmail.com&lt;/span&gt;&lt;/p&gt;\n                                        &lt;/div&gt;\n\n                                        &lt;div class=&quot;info-item padd-15&quot;&gt;\n                                            &lt;p&gt;专业: &lt;span&gt;光电信息科学与工程&lt;/span&gt;&lt;/p&gt;\n                                        &lt;/div&gt;\n\n                                        &lt;div class=&quot;info-item padd-15&quot;&gt;\n                                            &lt;p&gt;电话: &lt;span&gt;+86 18923434625&lt;/span&gt;&lt;/p&gt;\n                                        &lt;/div&gt;\n\n                                        &lt;div class=&quot;info-item padd-15&quot;&gt;\n                                            &lt;p&gt;城市: &lt;span&gt;深圳&lt;/span&gt;&lt;/p&gt;\n                                        &lt;/div&gt;\n\n                                        &lt;div class=&quot;info-item padd-15&quot;&gt;\n                                            &lt;p&gt;状态: &lt;span&gt;学生&lt;/span&gt;&lt;/p&gt;\n                                        &lt;/div&gt;\n                                    &lt;/div&gt;\n\n                                    &lt;div class=&quot;row&quot;&gt;\n                                        &lt;div class=&quot;buttons padd-15&quot;&gt;\n                                            &lt;a href=&quot;#contact&quot; class=&quot;btn hire-me&quot; data-section-index=&quot;1&quot;&gt;联系我&lt;/a&gt;\n                                        &lt;/div&gt;\n                                    &lt;/div&gt;\n                                &lt;/div&gt;\n                                &lt;div class=&quot;skills padd-15&quot;&gt;\n                                    &lt;div class=&quot;row&quot;&gt;\n                                        &lt;div class=&quot;skill-item padd-15&quot;&gt;\n                                            &lt;h5&gt;Python&lt;/h5&gt;\n                                            &lt;div class=&quot;progress&quot;&gt;\n                                                &lt;div class=&quot;progress-in&quot; style=&quot;width: 85%;&quot;&gt;&lt;/div&gt;\n                                                &lt;div class=&quot;skill-percent&quot;&gt;85%&lt;/div&gt;\n                                            &lt;/div&gt;\n                                        &lt;/div&gt;\n\n                                        &lt;div class=&quot;skill-item padd-15&quot;&gt;\n                                            &lt;h5&gt;HTML&lt;/HTml&gt;&lt;/h5&gt;\n                                            &lt;div class=&quot;progress&quot;&gt;\n                                                &lt;div class=&quot;progress-in&quot; style=&quot;width: 76%;&quot;&gt;&lt;/div&gt;\n                                                &lt;div class=&quot;skill-percent&quot;&gt;76%&lt;/div&gt;\n                                            &lt;/div&gt;\n                                        &lt;/div&gt;\n\n                                        &lt;div class=&quot;skill-item padd-15&quot;&gt;\n                                            &lt;h5&gt;C/C++&lt;/h5&gt;\n                                            &lt;div class=&quot;progress&quot;&gt;\n                                                &lt;div class=&quot;progress-in&quot; style=&quot;width: 70%;&quot;&gt;&lt;/div&gt;\n                                                &lt;div class=&quot;skill-percent&quot;&gt;70%&lt;/div&gt;\n                                            &lt;/div&gt;\n                                        &lt;/div&gt;\n\n                                        &lt;div class=&quot;skill-item padd-15&quot;&gt;\n                                            &lt;h5&gt;C#&lt;/h5&gt;\n                                            &lt;div class=&quot;progress&quot;&gt;\n                                                &lt;div class=&quot;progress-in&quot; style=&quot;width: 61%;&quot;&gt;&lt;/div&gt;\n                                                &lt;div class=&quot;skill-percent&quot;&gt;61%&lt;/div&gt;\n                                            &lt;/div&gt;\n                                        &lt;/div&gt;\n\n                                        &lt;div class=&quot;skill-item padd-15&quot;&gt;\n                                            &lt;h5&gt;MatLab&lt;/h5&gt;\n                                            &lt;div class=&quot;progress&quot;&gt;\n                                                &lt;div class=&quot;progress-in&quot; style=&quot;width: 74%;&quot;&gt;&lt;/div&gt;\n                                                &lt;div class=&quot;skill-percent&quot;&gt;74%&lt;/div&gt;\n                                            &lt;/div&gt;\n                                        &lt;/div&gt;\n                                    &lt;/div&gt;\n                                &lt;/div&gt;\n                            &lt;/div&gt;\n\n                            &lt;div class=&quot;row&quot;&gt;\n                                &lt;div class=&quot;education padd-15&quot;&gt;\n                                    &lt;h3 class=&quot;title&quot;&gt;教育经历&lt;/h3&gt;\n                                    &lt;div class=&quot;row&quot;&gt;\n                                        &lt;div class=&quot;timeline-box padd-15&quot;&gt;\n                                            &lt;div class=&quot;timeline shadow-dark&quot;&gt;\n                                                &lt;div class=&quot;timeline-item&quot;&gt;\n                                                    &lt;div class=&quot;circle-dot&quot;&gt;&lt;/div&gt;\n                                                    &lt;h3 class=&quot;timeline-date&quot;&gt;\n                                                        &lt;i class=&quot;fa fa-calendar&quot;&gt;&lt;/i&gt; 2019 - 2022\n                                                    &lt;/h3&gt;\n                                                    &lt;h4 class=&quot;timeline-title&quot;&gt;深圳市第二高级中学取得高中文凭&lt;/h4&gt;\n                                                    &lt;p class=&quot;timeline-text&quot;&gt;在深圳市第二高级中学的三年里，通过严格的课程和竞赛培养了我的学术韧性，通过协作解决问题培养了批判性思维，并在学校“阳光进取，平实包容”的校训下灌输了坚持和包容的价值观。老师的指导不仅增强了我的求知欲和应对挑战的适应能力，同时在文化活动中的团队合作也加深了我的同理心和领导力。&lt;/p&gt;\n                                                &lt;/div&gt;\n\n                                                &lt;div class=&quot;timeline-item&quot;&gt;\n                                                    &lt;div class=&quot;circle-dot&quot;&gt;&lt;/div&gt;\n                                                    &lt;h3 class=&quot;timeline-date&quot;&gt;\n                                                        &lt;i class=&quot;fa fa-calendar&quot;&gt;&lt;/i&gt; 2022 - 今日\n                                                    &lt;/h3&gt;\n                                                    &lt;h4 class=&quot;timeline-title&quot;&gt;深圳大学本科学位在读&lt;/h4&gt;\n                                                    &lt;p class=&quot;timeline-text&quot;&gt;我学习了很多的领域的知识，这些知识对于理解计算的理论和实践方面至关重要。这包括编程基础、计算机体系结构、操作系统、光学、电路基础、问题解决、协作和沟通软技能。&lt;/p&gt;\n                                                &lt;/div&gt;\n\n                                                &lt;!-- &lt;div class=&quot;timeline-item&quot;&gt;\n                                                    &lt;div class=&quot;circle-dot&quot;&gt;&lt;/div&gt;\n                                                    &lt;h3 class=&quot;timeline-date&quot;&gt;\n                                                        &lt;i class=&quot;fa fa-calendar&quot;&gt;&lt;/i&gt; 2007 - 2010\n                                                    &lt;/h3&gt;\n                                                    &lt;h4 class=&quot;timeline-title&quot;&gt;Bachelor Degree&lt;/h4&gt;\n                                                    &lt;p class=&quot;timeline-text&quot;&gt;There I learnt foundational courses and computer sciences fundamentals. In the institution, I chose my specialization in web-development that involves front-end and back-end technologies, user interface designs and content management systems.&lt;/p&gt;\n                                                &lt;/div&gt; --&gt;\n                                            &lt;/div&gt;\n                                        &lt;/div&gt;\n                                    &lt;/div&gt;\n                                &lt;/div&gt;\n\n                                &lt;div class=&quot;experience padd-15&quot;&gt;\n                                    &lt;h3 class=&quot;title&quot;&gt;工作经历&lt;/h3&gt;\n                                    &lt;div class=&quot;row&quot;&gt;\n                                        &lt;div class=&quot;timeline-box padd-15&quot;&gt;\n                                            &lt;div class=&quot;timeline shadow-dark&quot;&gt;\n                                                &lt;div class=&quot;timeline-item&quot;&gt;\n                                                    &lt;div class=&quot;circle-dot&quot;&gt;&lt;/div&gt;\n                                                    &lt;h3 class=&quot;timeline-date&quot;&gt;\n                                                        &lt;i class=&quot;fa fa-calendar&quot;&gt;&lt;/i&gt; 2010 - 今日\n                                                    &lt;/h3&gt;\n                                                    &lt;h4 class=&quot;timeline-title&quot;&gt;学生&lt;/h4&gt;\n                                                    &lt;p class=&quot;timeline-text&quot;&gt;我现在资历尚浅，还没有在任何一家公司有过业务上的长期实习，最近有意愿开始动员了&lt;/p&gt;\n                                                &lt;/div&gt;\n\n                                                &lt;!-- &lt;div class=&quot;timeline-item&quot;&gt;\n                                                    &lt;div class=&quot;circle-dot&quot;&gt;&lt;/div&gt;\n                                                    &lt;h3 class=&quot;timeline-date&quot;&gt;\n                                                        &lt;i class=&quot;fa fa-calendar&quot;&gt;&lt;/i&gt; 2017 - 2019\n                                                    &lt;/h3&gt;\n                                                    &lt;h4 class=&quot;timeline-title&quot;&gt;Junior Front-end Designer&lt;/h4&gt;\n                                                    &lt;p class=&quot;timeline-text&quot;&gt;I learnt to code in an existing codebase, dive into the project and understanding its structure. Also I worked closely with senior software engineers that guided me, answered my questions and helped me grow.&lt;/p&gt;\n                                                &lt;/div&gt;\n\n                                                &lt;div class=&quot;timeline-item&quot;&gt;\n                                                    &lt;div class=&quot;circle-dot&quot;&gt;&lt;/div&gt;\n                                                    &lt;h3 class=&quot;timeline-date&quot;&gt;\n                                                        &lt;i class=&quot;fa fa-calendar&quot;&gt;&lt;/i&gt; 2014 - 2016\n                                                    &lt;/h3&gt;\n                                                    &lt;h4 class=&quot;timeline-title&quot;&gt;Graphic Designer&lt;/h4&gt;\n                                                    &lt;p class=&quot;timeline-text&quot;&gt;I can create logos, color schemes and typography for a brand&#39;s identity. Also I develop graphics for websites, social media and digital ads with applications that enhance user experience.&lt;/p&gt;\n                                                &lt;/div&gt; --&gt;\n                                            &lt;/div&gt;\n                                        &lt;/div&gt;\n                                    &lt;/div&gt;\n                                &lt;/div&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt;\n                    &lt;/div&gt;\n                &lt;/div&gt;\n            &lt;/section&gt;\n\n            &lt;section class=&quot;service section &quot; id=&quot;services&quot;&gt;\n                &lt;div class=&quot;container&quot;&gt;\n                    &lt;div class=&quot;row&quot;&gt;\n                        &lt;div class=&quot;section-title padd-15&quot;&gt;\n                            &lt;h2&gt;技能&lt;/h2&gt;\n                        &lt;/div&gt;\n                    &lt;/div&gt;\n\n                    &lt;div class=&quot;row&quot;&gt;\n                        &lt;div class=&quot;service-item padd-15&quot;&gt;\n                            &lt;div class=&quot;service-item-inner&quot;&gt;\n                                &lt;div class=&quot;icon&quot;&gt;\n                                    &lt;i class=&quot;fa fa-microchip&quot;&gt;&lt;/i&gt;\n                                &lt;/div&gt;\n                                &lt;h4&gt;硬件设计&lt;/h4&gt;\n                                &lt;p&gt;我可以熟练嘉立创和AD进行原理图设计和PCB走线&lt;/p&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt;\n\n                        &lt;div class=&quot;service-item padd-15&quot;&gt;\n                            &lt;div class=&quot;service-item-inner&quot;&gt;\n                                &lt;div class=&quot;icon&quot;&gt;\n                                    &lt;i class=&quot;fa fa-laptop-code&quot;&gt;&lt;/i&gt;\n                                &lt;/div&gt;\n                                &lt;h4&gt;计算机视觉研究&lt;/h4&gt;\n                                &lt;p&gt;我了解 pytorch 和 Tensorflow 的深度学习框架，了解机器学习和多视图几何，可以熟练使用TensorRT进行模型部署&lt;/p&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt;\n\n                        &lt;div class=&quot;service-item padd-15&quot;&gt;\n                            &lt;div class=&quot;service-item-inner&quot;&gt;\n                                &lt;div class=&quot;icon&quot;&gt;\n                                    &lt;i class=&quot;fa fa-ring&quot;&gt;&lt;/i&gt;\n                                &lt;/div&gt;\n                                &lt;h4&gt;光学设计&lt;/h4&gt;\n                                &lt;p&gt;可以熟练使用Zemax设计猫眼、望远镜等镜头&lt;/p&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt;\n\n                        &lt;div class=&quot;service-item padd-15&quot;&gt;\n                            &lt;div class=&quot;service-item-inner&quot;&gt;\n                                &lt;div class=&quot;icon&quot;&gt;\n                                    &lt;i class=&quot;fa fa-code&quot;&gt;&lt;/i&gt;\n                                &lt;/div&gt;\n                                &lt;h4&gt;嵌入式开发&lt;/h4&gt;\n                                &lt;p&gt;我可以熟练使用常见的微控制器，例如：STM32 及其国产替代、ESP32/ESP8266 等，也熟悉常见的嵌入式Linux开发板：Jetson nano、树莓派及其国产变体&lt;/p&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt;\n\n                        &lt;!-- &lt;div class=&quot;service-item padd-15&quot;&gt;\n                            &lt;div class=&quot;service-item-inner&quot;&gt;\n                                &lt;div class=&quot;icon&quot;&gt;\n                                    &lt;i class=&quot;fa fa-search&quot;&gt;&lt;/i&gt;\n                                &lt;/div&gt;\n                                &lt;h4&gt;Brand Consultancy&lt;/h4&gt;\n                                &lt;p&gt;I build brands through cultural insights &amp; strategic vision. Custom crafted business solution&lt;/p&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt; --&gt;\n\n                        &lt;!-- &lt;div class=&quot;service-item padd-15&quot;&gt;\n                            &lt;div class=&quot;service-item-inner&quot;&gt;\n                                &lt;div class=&quot;icon&quot;&gt;\n                                    &lt;i class=&quot;fa fa-bullhorn&quot;&gt;&lt;/i&gt;\n                                &lt;/div&gt;\n                                &lt;h4&gt;Photography&lt;/h4&gt;\n                                &lt;p&gt;I make high-quality photos of any category at a professional level.&lt;/p&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt; --&gt;\n                    &lt;/div&gt;\n                &lt;/div&gt;\n            &lt;/section&gt;\n\n            &lt;section class=&quot;portfolio section &quot; id=&quot;portfolio&quot;&gt;\n                &lt;div class=&quot;container&quot;&gt;\n                    &lt;div class=&quot;row&quot;&gt;\n                        &lt;div class=&quot;section-title padd-15&quot;&gt;\n                            &lt;h2&gt;作品集&lt;/h2&gt;\n                        &lt;/div&gt;\n                    &lt;/div&gt;\n\n                    &lt;div class=&quot;row&quot;&gt;\n                        &lt;div class=&quot;portfolio-heading padd-15&quot;&gt;\n                            &lt;h2&gt;我之前的项目: &lt;/h2&gt;\n                        &lt;/div&gt;\n                    &lt;/div&gt;\n\n                    &lt;div class=&quot;row&quot;&gt;\n                        &lt;div class=&quot;portfolio-item padd-15&quot;&gt;\n                            &lt;div class=&quot;portfolio-item-inner shadow-dark&quot;&gt;\n                                &lt;div class=&quot;portfolio-img&quot;&gt;\n                                    &lt;img src=&quot;https://i.postimg.cc/NGkQB1Y0/portfolio-1.jpg&quot; alt=&quot;portfolio image&quot;&gt;\n                                &lt;/div&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt;\n\n                        &lt;div class=&quot;portfolio-item padd-15&quot;&gt;\n                            &lt;div class=&quot;portfolio-item-inner shadow-dark&quot;&gt;\n                                &lt;div class=&quot;portfolio-img&quot;&gt;\n                                    &lt;img src=&quot;https://i.postimg.cc/ydg49Z3w/portfolio-2.jpg&quot; alt=&quot;portfolio image&quot;&gt;\n                                &lt;/div&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt;\n\n                        &lt;div class=&quot;portfolio-item padd-15&quot;&gt;\n                            &lt;div class=&quot;portfolio-item-inner shadow-dark&quot;&gt;\n                                &lt;div class=&quot;portfolio-img&quot;&gt;\n                                    &lt;img src=&quot;https://i.postimg.cc/RVPdrbDL/portfolio-3.jpg&quot; alt=&quot;portfolio image&quot;&gt;\n                                &lt;/div&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt;\n\n                        &lt;div class=&quot;portfolio-item padd-15&quot;&gt;\n                            &lt;div class=&quot;portfolio-item-inner shadow-dark&quot;&gt;\n                                &lt;div class=&quot;portfolio-img&quot;&gt;\n                                    &lt;img src=&quot;https://i.postimg.cc/25dBY15N/portfolio-4.jpg&quot; alt=&quot;portfolio image&quot;&gt;\n                                &lt;/div&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt;\n\n                        &lt;div class=&quot;portfolio-item padd-15&quot;&gt;\n                            &lt;div class=&quot;portfolio-item-inner shadow-dark&quot;&gt;\n                                &lt;div class=&quot;portfolio-img&quot;&gt;\n                                    &lt;img src=&quot;https://i.postimg.cc/RZBHD1ML/portfolio-5.jpg&quot; alt=&quot;portfolio image&quot;&gt;\n                                &lt;/div&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt;\n\n                        &lt;div class=&quot;portfolio-item padd-15&quot;&gt;\n                            &lt;div class=&quot;portfolio-item-inner shadow-dark&quot;&gt;\n                                &lt;div class=&quot;portfolio-img&quot;&gt;\n                                    &lt;img src=&quot;https://i.postimg.cc/3wP7t1hK/portfolio-6.jpg&quot; alt=&quot;portfolio image&quot;&gt;\n                                &lt;/div&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt;\n                    &lt;/div&gt;\n                &lt;/div&gt;\n            &lt;/section&gt;\n\n            &lt;section class=&quot;contact section&quot; id=&quot;contact&quot;&gt;\n                &lt;div class=&quot;container&quot;&gt;\n                    &lt;div class=&quot;row&quot;&gt;\n                        &lt;div class=&quot;section-title padd-15&quot;&gt;\n                            &lt;h2&gt;联系我&lt;/h2&gt;\n                        &lt;/div&gt;\n                    &lt;/div&gt;\n\n                    &lt;h3 class=&quot;contact-title padd-15&quot;&gt;有任何问题都可以来联系我&lt;/h3&gt;\n                    &lt;h4 class=&quot;contact-subtitle padd-15&quot;&gt;我随时准备着&lt;/h4&gt;\n\n                    &lt;div class=&quot;row&quot;&gt;\n                        &lt;div class=&quot;contact-info-item padd-15&quot;&gt;\n                            &lt;div class=&quot;icon&quot;&gt;&lt;i class=&quot;fa fa-phone&quot;&gt;&lt;/i&gt;&lt;/div&gt;\n                            &lt;h4&gt;我的电话&lt;/h4&gt;\n                            &lt;p&gt;+86 18923434625&lt;/p&gt;\n                        &lt;/div&gt;\n\n\n                        &lt;div class=&quot;contact-info-item padd-15&quot;&gt;\n                            &lt;div class=&quot;icon&quot;&gt;&lt;i class=&quot;fa fa-map-marker-alt&quot;&gt;&lt;/i&gt;&lt;/div&gt;\n                            &lt;h4&gt;所在城市&lt;/h4&gt;\n                            &lt;p&gt;深圳&lt;/p&gt;\n                        &lt;/div&gt;\n\n                        &lt;div class=&quot;contact-info-item padd-15&quot;&gt;\n                            &lt;div class=&quot;icon&quot;&gt;&lt;i class=&quot;fa fa-envelope&quot;&gt;&lt;/i&gt;&lt;/div&gt;\n                            &lt;h4&gt;我的邮箱&lt;/h4&gt;\n                            &lt;p&gt;xuruolun666@gmail.com&lt;/p&gt;\n                        &lt;/div&gt;\n\n                        &lt;div class=&quot;contact-info-item padd-15&quot;&gt;\n                            &lt;div class=&quot;icon&quot;&gt;&lt;i class=&quot;fa fa-globe&quot;&gt;&lt;/i&gt;&lt;/div&gt;\n                            &lt;h4&gt;我的网站&lt;/h4&gt;\n                            &lt;p&gt;guide.yesord.top&lt;/p&gt;\n                        &lt;/div&gt;\n                    &lt;/div&gt;\n                    &lt;h3 class=&quot;contact-title padd-15&quot;&gt;向我发送邮件&lt;/h3&gt;\n                    &lt;h4 class=&quot;contact-subtitle padd-15&quot;&gt;我将会尽我所能尽快回复！&lt;/h4&gt;\n\n                    &lt;div class=&quot;row&quot;&gt;\n                        &lt;div class=&quot;contact-form padd-15&quot;&gt;\n                            &lt;div class=&quot;row&quot;&gt;\n                                &lt;div class=&quot;form-item col-6 padd-15&quot;&gt;\n                                    &lt;div class=&quot;form-group&quot;&gt;\n                                        &lt;input type=&quot;text&quot; class=&quot;form-control&quot; placeholder=&quot;Name&quot;&gt;\n                                    &lt;/div&gt;\n                                &lt;/div&gt;\n\n                                &lt;div class=&quot;form-item col-6 padd-15&quot;&gt;\n                                    &lt;div class=&quot;form-group&quot;&gt;\n                                        &lt;input type=&quot;email&quot; class=&quot;form-control&quot; placeholder=&quot;Email&quot;&gt;\n                                    &lt;/div&gt;\n                                &lt;/div&gt;\n                            &lt;/div&gt;\n\n                            &lt;div class=&quot;row&quot;&gt;\n                                &lt;div class=&quot;form-item col-12 padd-15&quot;&gt;\n                                    &lt;div class=&quot;form-group&quot;&gt;\n                                        &lt;input type=&quot;text&quot; class=&quot;form-control&quot; placeholder=&quot;Subject&quot;&gt;\n                                    &lt;/div&gt;\n                                &lt;/div&gt;\n                            &lt;/div&gt;\n\n                            &lt;div class=&quot;row&quot;&gt;\n                                &lt;div class=&quot;form-item col-12 padd-15&quot;&gt;\n                                    &lt;div class=&quot;form-group&quot;&gt;\n                                        &lt;textarea name=&quot;message&quot; id=&quot;message&quot; class=&quot;form-control&quot; placeholder=&quot;Message&quot;&gt;&lt;/textarea&gt;\n                                    &lt;/div&gt;\n                                &lt;/div&gt;\n                            &lt;/div&gt;\n\n                            &lt;div class=&quot;row&quot;&gt;\n                                &lt;div class=&quot;form-item col-12 padd-15&quot;&gt;\n                                    &lt;div class=&quot;form-group&quot;&gt;\n                                        &lt;button type=&quot;submit&quot; class=&quot;btn&quot;&gt;发送消息&lt;/button&gt;\n                                    &lt;/div&gt;\n                                &lt;/div&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt;\n                    &lt;/div&gt;\n                &lt;/div&gt;\n            &lt;/section&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;div class=&quot;style-switcher&quot;&gt;\n        &lt;div class=&quot;style-switcher-toggler s-icon&quot;&gt;\n            &lt;i class=&quot;fas fa-cog fa-spin&quot;&gt;&lt;/i&gt;\n        &lt;/div&gt;\n        \n\n        &lt;div class=&quot;day-night s-icon&quot;&gt;\n            &lt;i class=&quot;fas &quot;&gt;&lt;/i&gt;\n        &lt;/div&gt;\n\n\n        \n        &lt;h4&gt;主题颜色&lt;/h4&gt;\n\n        &lt;div class=&quot;colors&quot;&gt;\n            &lt;span class=&quot;color-1&quot; onclick=&quot;setActiveStyle(&#39;color-1&#39;)&quot;&gt;&lt;/span&gt;\n            &lt;span class=&quot;color-2&quot; onclick=&quot;setActiveStyle(&#39;color-2&#39;)&quot;&gt;&lt;/span&gt;\n            &lt;span class=&quot;color-3&quot; onclick=&quot;setActiveStyle(&#39;color-3&#39;)&quot;&gt;&lt;/span&gt;\n            &lt;span class=&quot;color-4&quot; onclick=&quot;setActiveStyle(&#39;color-4&#39;)&quot;&gt;&lt;/span&gt;\n            &lt;span class=&quot;color-5&quot; onclick=&quot;setActiveStyle(&#39;color-5&#39;)&quot;&gt;&lt;/span&gt;\n        &lt;/div&gt;\n        \n\n    &lt;/div&gt;\n\n    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/typed.js/2.0.10/typed.min.js&quot;&gt;&lt;/script&gt;\n    &lt;script src=&quot;../static/js/app.js&quot;&gt;&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\ntxt3. 创建[web]/static/css/style.css\n将以下内容输入 style.css\ncss@import url(&#39;https://fonts.googleapis.com/css2?family=Clicker+Script&amp;family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&amp;display=swap&#39;);\n@import url(&#39;https://fonts.googleapis.com/css2?family=ZCOOL+KuaiLe&amp;display=swap&#39;);\n@import url(&#39;https://fonts.googleapis.com/css2?family=Liu+Jian+Mao+Cao&amp;family=ZCOOL+KuaiLe&amp;display=swap&#39;);\n:root &#123;\n    --bg-black900: #F2F2FC;\n    --bg-black100: #FDF9FF;\n    --bg-black50: #E8DFEC;\n    --text-black900: #302E4D;\n    --text-black700: #504E70;\n\n    --skin-color: #EC1839;\n\n&#125;\n\nbody.dark &#123;\n    --bg-black900: #151515;\n    --bg-black100: #222222;\n    --bg-black50: #393939;\n    --text-black900: #FFFFFF;\n    --text-black700: #E9E9E9;\n&#125;\n\n*&#123; margin: 0; padding: 0; outline: none; text-decoration: none; box-sizing: border-box; &#125;\n::before, ::after &#123; box-sizing: border-box; &#125;\nul &#123; list-style: none; &#125;\nbody &#123; line-height: 1.5; font-size: 16px; font-family: &#39;ZCOOL KuaiLe&#39;, sans-serif; &#125;\n.section &#123; \n    background: var(--bg-black900); \n    min-height: 100vh; \n    display: block; \n    opacity: 1; \n    padding: 0 30px; \n    position: fixed; \n    left: 270px; \n    top: 0; \n    right: 0; \n    bottom: 0; \n    z-index: 0; \n    overflow-x: hidden; \n    overflow-y: auto;\n    transition: all .3s ease; \n&#125;\n    .section.back-section &#123; z-index: 1; &#125;\n    .section.active &#123; z-index: 2; opacity: 1; animation: slideSection 1s ease; &#125;\n        @keyframes slideSection &#123;\n            0% &#123; transform: translateX(100%); &#125;\n            100% &#123; transform: translateX(0%); &#125;\n        &#125;\n.hidden &#123; display: none !important; &#125;\n/*.main-content &#123; padding-left: 270px; &#125;*/\n.padd-15 &#123; padding-left: 15px; padding-right: 15px; &#125;\n\n.container &#123; max-width: 1100px; width: 100%; margin: auto; &#125;\n    .section .container &#123; padding-top: 60px; padding-bottom: 70px; &#125;\n.section-title &#123; flex: 0 0 100%; max-width: 100%; margin-bottom: 60px; &#125;\n    .section-title h2 &#123; font-size: 40px; color: var(--text-black900); font-weight: 700; position: relative; &#125;\n        .section-title h2::before &#123;\n            content: &#39;&#39;;\n            height: 4px;\n            width: 50px;\n            background-color: var(--skin-color);\n            position: absolute;\n            top: 100%;\n            left: 0;\n        &#125;\n        .section-title h2::after &#123;\n            content: &#39;&#39;;\n            height: 4px;\n            width: 25px;\n            background-color: var(--skin-color);\n            position: absolute;\n            top: 100%;\n            left: 0;\n            margin-top: 8px;\n        &#125;\n.row &#123; display: flex; flex-wrap: wrap; margin-left: -15px; margin-right: -15px; position: relative; &#125;\n.btn &#123;\n    font-size: 16px;\n    font-weight: 500;\n    padding: 12px 35px;\n    color: white;\n    border-radius: 40px;\n    display: inline-block;\n    border: none;\n    white-space: nowrap;\n    background: var(--skin-color);\n    transition: all .3s ease;\n&#125;\n    .btn:hover &#123; transform: scale(1.05); &#125;\n\n\n    \n.get-start-btn &#123;\n    font-family: &quot;ZCOOL KuaiLe&quot;, serif;\n    position: absolute;\n    top: 65%;\n    left: 45%;\n    font-size: 16px;\n    font-weight: 500;\n    padding: 12px 35px;\n    color: white;\n    border-radius: 40px;\n    display: inline-block;\n    border: none;\n    white-space: nowrap;\n    background: var(--skin-color);\n    transition: all .3s ease;\n&#125;\n    .get-start-btn:hover &#123; transform: scale(1.05); &#125;\n.shadow-dark &#123; box-shadow: 0 0 20px rgba(48, 46, 77, .15); &#125;\n\n/* ASIDE */\n\n.aside &#123; \n    width: 270px; \n    background: var(--bg-black100); \n    position: fixed; \n    left: 0; \n    top: 0; \n    z-index: 10; \n    height: 100%; \n    padding: 30px;\n    border-right: 1px solid var(--bg-black50);\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    transition: all .3s ease;\n&#125;\n    .aside .logo &#123; position: absolute; top: 50px; font-size: 30px; text-transform: capitalize; &#125;\n        .aside .logo a &#123; color: var(--text-black900); font-weight: 700; padding: 15px 20px; font-size: 30px; letter-spacing: 5px; position: relative; &#125;\n            .aside .logo a::before &#123; \n                content: &#39;&#39;; \n                position: absolute; \n                width: 20px; \n                height: 20px; \n                border-bottom: 5px solid var(--skin-color);\n                border-left: 5px solid var(--skin-color);\n                bottom: 0;\n                left: 0; \n            &#125;\n            .aside .logo a::after &#123; \n                content: &#39;&#39;; \n                position: absolute; \n                width: 20px; \n                height: 20px; \n                border-top: 5px solid var(--skin-color);\n                border-right: 5px solid var(--skin-color);\n                top: 0;\n                right: 0; \n            &#125;\n            .aside .logo a span &#123; font-family: &#39;Clicker Script&#39;, cursive; font-size: 40px; &#125;\n    .aside .nav &#123; margin-top: 50px; &#125;\n        .aside .nav li &#123; margin-bottom: 20px; display: block; &#125;\n            .aside .nav li a &#123; font-size: 16px; font-weight: 600; display: block; color: var(--text-black900); padding: 5px 15px; border-bottom: 1px solid var(--bg-black50); &#125;\n                .aside .nav li a.active &#123; color: var(--skin-color); &#125;\n                .aside .nav li a i &#123; margin-right: 15px; &#125;\n    .aside .nav-toggler &#123;\n        height: 40px;\n        width: 45px;\n        border: 1px solid var(--bg-black50);\n        cursor: pointer;\n        position: fixed;\n        left: 300px;\n        top: 20px;\n        border-radius: 5px;\n        background: var(--bg-black100);\n        display: none;\n        align-items: center;\n        justify-content: center;\n        transition: all .3s ease;\n    &#125;\n        .aside .nav-toggler.open span &#123; background-color: transparent; &#125;\n        .aside .nav-toggler span &#123; height: 2px; width: 18px; background: var(--skin-color); display: inline-block; position: relative; &#125;\n            .aside .nav-toggler span::before &#123;\n                content: &#39;&#39;;\n                height: 2px;\n                width: 18px;\n                background: var(--skin-color);\n                position: absolute;\n                top: -6px;\n                left: 0;\n            &#125;\n                .aside .nav-toggler.open span::before &#123; transform: rotate(45deg); top: 0; &#125;\n            .aside .nav-toggler span::after &#123;\n                content: &#39;&#39;;\n                height: 2px;\n                width: 18px;\n                background: var(--skin-color);\n                position: absolute;\n                top: 6px;\n                left: 0;\n            &#125;\n                .aside .nav-toggler.open span::after &#123; transform: rotate(-45deg); top: 0; &#125;\n\n/* HOME */\n\n.home &#123; min-height: 100vh; display: flex; color: var(--text-black900); &#125;\n    .home .home-info &#123; flex: 0 0 60%; max-width: 60%; &#125;\n        h3.hello &#123; font-size: 28px; margin: 15px 0; &#125;\n            h3.hello span &#123; font-family: &#39;Liu Jian Mao Cao&#39;, cursive; font-size: 30px; font-weight: 700; color: var(--skin-color); &#125;\n        h3.profession &#123; font-size: 30px; margin: 15px 0; &#125;\n            .typing &#123; color: var(--skin-color); &#125;\n        .home-info p &#123; margin-bottom: 70px; font-size: 20px; color: var(--text-black700); &#125;\n    .home .home-img &#123; flex: 0 0 40%; max-width: 40%; text-align: center; position: relative; &#125;\n        .home-img::after &#123;\n            content: &#39;&#39;;\n            position: absolute;\n            height: 80px;\n            width: 80px;\n            border-bottom: 10px solid var(--skin-color);\n            border-right: 10px solid var(--skin-color);\n            right: 20px;\n            bottom: -40px;\n        &#125;\n        .home-img::before &#123;\n            content: &#39;&#39;;\n            position: absolute;\n            height: 80px;\n            width: 80px;\n            border-top: 10px solid var(--skin-color);\n            border-left: 10px solid var(--skin-color);\n            left: -20px;\n            top: -40px;\n        &#125;\n        .home .home-img img &#123; height: 400px; margin: auto; border-radius: 4px; &#125;\n\n/* ABOUT */\n\n.about .about-content &#123; flex: 0 0 100%; max-width: 100%;  &#125;\n    .about .about-content .about-text &#123; flex: 0 0 100%; max-width: 100%; &#125;\n        .about .about-content .about-text h3 &#123; font-size: 24px; margin-bottom: 15px; font-weight: 700; color: var(--text-black900); &#125;\n            .about .about-content .about-text h3 span &#123; color: var(--skin-color); &#125;\n        .about .about-content .about-text p &#123; font-size: 16px; line-height: 25px; color: var(--text-black700); text-align: justify; &#125;\n    .about .about-content .personal-info &#123; flex: 0 0 60%; max-width: 60%; margin-top: 40px; &#125;\n        .about .about-content .personal-info .info-item &#123; flex: 0 0 50%; max-width: 50%; &#125;\n            .about .about-content .personal-info .info-item p &#123; font-weight: 600; padding: 10px 0; font-size: 16px; color: var(--text-black900); border-bottom: 1px solid var(--bg-black50); &#125;\n                .about .about-content .personal-info .info-item p span &#123; font-weight: 400; color: var(--text-black700); margin-left: 4px; display: inline-block; &#125;\n    .about .about-content .personal-info .buttons &#123; margin-top: 30px; &#125;\n        .about .about-content .personal-info .buttons .btn &#123; margin-top: 10px; &#125;\n    .about .about-content .skills &#123; flex: 0 0 40%; max-width: 40%; margin-top: 40px; &#125;\n        .about .about-content .skills .skill-item &#123; flex: 0 0 100%; max-width: 100%; margin-bottom: 25px; &#125;\n            .about .about-content .skills .skill-item h5 &#123; line-height: 40px; font-weight: 600; font-size: 16px; color: var(--text-black900); text-transform: capitalize; &#125;\n            .about .about-content .skills .skill-item .skill-percent &#123; position: absolute; right: 0; color: var(--text-black900); top: -40px; font-weight: 400; line-height: 40px; &#125;\n            .about .about-content .skills .skill-item .progress &#123; background-color: var(--bg-black50); height: 7px; border-radius: 4px; width: 100%; position: relative; &#125;\n                .about .about-content .skills .skill-item .progress .progress-in &#123; position: absolute; left: 0; top: 0; height: 100%; border-radius: 4px; background-color: var(--skin-color); &#125;\n    .about .about-content .education, .about .about-content .experience &#123; flex: 0 0 50%; max-width: 50%; margin-top: 30px; &#125;\n        .about .about-content h3.title &#123; font-size: 24px; margin-bottom: 30px; font-weight: 700; color: var(--text-black900); &#125;\n        .about .about-content .timeline-box &#123; flex: 0 0 100%; max-width: 100%; &#125;\n            .about .about-content .timeline &#123; background-color: var(--bg-black100); padding: 30px 15px; border: 1px solid var(--bg-black50); border-radius: 10px; width: 100%; position: relative; &#125;\n                .about .about-content .timeline .timeline-item &#123; position: relative; padding-left: 37px; padding-bottom: 50px; &#125;\n                    .about .about-content .timeline .timeline-item:last-child &#123; padding-bottom: 0; &#125;\n                    .about .about-content .timeline .timeline-item::before &#123; content: &#39;&#39;; width: 1px; position: absolute; height: 100%; left: 7px; top: 0; background-color: var(--skin-color); &#125;\n                .about .about-content .timeline .circle-dot &#123; position: absolute; left: 0; top: 0; height: 15px; width: 15px; border-radius: 50%; background-color: var(--skin-color); &#125;\n                .about .about-content .timeline .timeline-date &#123; font-weight: 400; font-size: 14px; margin-bottom: 12px; color: var(--text-black700); &#125;\n                    .about .about-content .timeline .timeline-date .fa &#123; margin-right: 5px; &#125;\n                .about .about-content .timeline .timeline-title &#123; font-weight: 700; font-size: 18px; margin-bottom: 15px; text-transform: capitalize; color: var(--text-black900); &#125;\n                .about .about-content .timeline .timeline-text &#123; line-height: 25px; font-size: 16px; text-align: justify; color: var(--text-black700); &#125;\n\n/* SERVICE */\n\n.service .container &#123; padding-bottom: 40px; &#125;\n.service .service-item &#123; margin-bottom: 30px; flex: 0 0 33.33%; max-width: 33.33%; &#125;\n    .service .service-item .service-item-inner &#123; background-color: var(--bg-black100); border: var(--bg-black50); border-radius: 10px; padding: 30px 15px; text-align: center; transition: all .3s ease; &#125;\n        .service .service-item .service-item-inner:hover &#123; box-shadow: 0 0 20px rgba(48, 46, 77, .15); &#125;\n        .service .service-item .service-item-inner .icon &#123; height: 60px; width: 60px; border-radius: 50%; display: block; margin: 0 auto 30px; text-align: center; transition: all .3s ease; &#125;\n            .service .service-item .service-item-inner .icon .fa &#123; font-size: 40px; line-height: 60px; color: var(--skin-color); transition: all .3s ease; &#125;\n            .service .service-item .service-item-inner:hover .icon &#123; background: var(--skin-color); &#125;\n                .service .service-item .service-item-inner:hover .icon .fa &#123; font-size: 25px; color: #FFF; &#125;\n        .service .service-item .service-item-inner h4 &#123; font-size: 18px; margin-bottom: 15px; color: var(--text-black900); font-weight: 700; text-transform: capitalize; &#125;\n        .service .service-item .service-item-inner p &#123; font-size: 16px; color: var(--text-black700); line-height: 25px; &#125;\n\n/* PORTFOLIO */\n\n.portfolio .container &#123; padding-bottom: 40px; &#125;\n.portfolio .portfolio-heading &#123; flex: 0 0 100%; max-width: 100%; margin-bottom: 40px; &#125;\n    .portfolio .portfolio-heading h2 &#123; color: var(--text-black900); font-weight: 500; &#125;\n.portfolio .portfolio-item &#123; flex: 0 0 33.33%; max-width: 33.33%; margin-bottom: 30px; &#125;\n.portfolio .portfolio-item-inner &#123; border: 6px solid var(--bg-black100); border-radius: 10px; overflow: hidden; cursor: pointer; &#125;\n    .portfolio .portfolio-item-inner .portfolio-img img &#123; width: 100%; display: block; &#125;\n\n/* CONTACT */\n\n.contact-title &#123; color: var(--skin-color); text-align: center; font-size: 25px; margin-bottom: 20px; &#125;\n.contact-subtitle &#123; color: var(--text-black900); text-align: center; font-size: 15px; margin-bottom: 60px; &#125;\n.contact .contact-info-item &#123; flex: 0 0 25%; max-width: 25%; text-align: center; margin-bottom: 60px; &#125;\n    .contact .contact-info-item .icon &#123; display: inline-block; &#125;\n        .contact .contact-info-item .icon .fa &#123; font-size: 25px; color: var(--skin-color); &#125;\n    .contact .contact-info-item h4 &#123; font-size: 18px; font-weight: 700; color: var(--text-black900); text-transform: capitalize; margin: 15px 0 5px; &#125;\n    .contact .contact-info-item p &#123; font-size: 16px; line-height: 25px; color: var(--text-black700); font-weight: 400; &#125;\n.contact .contact-form &#123; flex: 0 0 100%; max-width: 100%; &#125;\n    .contact .contact-form .col-6 &#123; flex: 0 0 50%; max-width: 50%; &#125;\n    .contact .contact-form .col-12 &#123; flex: 0 0 100%; max-width: 100%; &#125;\n    .contact .contact-form .form-item &#123; margin-bottom: 30px; &#125;\n        .contact .contact-form .form-item .form-control &#123; \n            width: 100%;\n            height: 50px;\n            border-radius: 25px;\n            background: var(--bg-black100);\n            border: 1px solid var(--bg-black50);\n            padding: 10px 25px;\n            font-size: 16px;\n            color: var(--text-black700);\n            transition: all .3s ease;\n            font-family: &#39;Poppins&#39;, sans-serif;\n        &#125;\n            .contact .contact-form .form-item .form-control:focus &#123; box-shadow: 0 0 20px rgba(48, 46, 77, .15); &#125;\n            .contact .contact-form .form-item textarea.form-control &#123; height: 140px; font-family: &#39;Poppins&#39;, sans-serif; &#125;\n    .contact .contact-form .btn &#123; height: 50px; padding: 0 50px; cursor: pointer; font-family: &#39;ZCOOL KuaiLe&#39;, sans-serif; &#125;\n\n@media (max-width: 1199px) &#123;\n\n    .aside &#123; left: -270px; &#125;\n        .aside.open &#123; left: 0; &#125;\n        .aside .nav-toggler &#123; display: flex; left: 30px; &#125;\n            .aside .nav-toggler.open &#123; left: 300px; &#125;\n    .section &#123; left: 0; &#125;\n        .section.open &#123; left: 270px; &#125;\n        .section .container &#123; padding-top: 70px; &#125;\n\n    .home-img::before, .home-img::after &#123; display: none; &#125;\n\n    .about .about-content .personal-info .info-item p span &#123; display: block; margin-left: 0; &#125;\n&#125;\n\n@media (max-width: 991px) &#123;\n\n    .home .home-info &#123; flex: 0 0 100%; max-width: 100%; &#125;\n    .home .home-img &#123; display: none; &#125;\n\n    .about .about-content .skills, .about .about-content .personal-info, .about .about-content .experience, .about .about-content .education &#123; flex: 0 0 100%; max-width: 100%; &#125;\n\n    .contact .contact-info-item, .portfolio .portfolio-item, .service .service-item &#123; flex: 0 0 50%; max-width: 50%; &#125;\n&#125;\n\n@media (max-width: 767px) &#123;\n\n    .portfolio .portfolio-item, .service .service-item, .contact .contact-form .col-6 &#123; flex: 0 0 100%; max-width: 100%; &#125;\n&#125;\n\n@media (max-width: 460px) &#123;\n\n    .contact .contact-info-item &#123; flex: 0 0 100%; max-width: 100%; &#125;\n&#125;\n\n\n/* Style Switcher */\n\n.style-switcher &#123;\n    position: fixed;\n    right: 0;\n    top: 60px;\n    padding: 15px;\n    width: 200px;\n    border: 1px solid var(--bg-black50);\n    background: var(--bg-black100);\n    z-index: 101;\n    border-radius: 5px;\n    transition: all .3s ease;\n    transform: translateX(100%);\n&#125;\n    .style-switcher.open &#123; transform: translateX(-25px); &#125;\n    .style-switcher .s-icon &#123;\n        position: absolute;\n        height: 40px;\n        width: 40px;\n        text-align: center;\n        font-size: 20px;\n        background: var(--bg-black100);\n        color: var(--text-black900);\n        right: 100%;\n        border: 1px solid var(--bg-black50);\n        margin-right: 25px;\n        cursor: pointer;\n        transition: all .3s ease;\n        border-radius: 50%;\n    &#125;\n        .style-switcher .s-icon i &#123; line-height: 40px; &#125;\n    .style-switcher .style-switcher-toggler &#123; top: 0; &#125;\n    .style-switcher .day-night &#123; top: 55px; &#125;\n    .style-switcher .jump &#123; top: 110px; &#125;\n    .style-switcher .language &#123; top: 165px; &#125;\n    .style-switcher h4 &#123; margin: 0 0 10px; color: var(--text-black700); font-size: 16px; font-weight: 600; text-transform: capitalize; &#125;\n    .style-switcher .colors &#123; display: flex; flex-wrap: wrap; justify-content: space-between; &#125;\n        .style-switcher .colors span &#123; display: inline-block; height: 30px; width: 30px; border-radius: 50%; &#125;\n    .style-switcher .color-1 &#123; background: #EC1839; &#125;\n    .style-switcher .color-2 &#123; background: #FA5B0F; &#125;\n    .style-switcher .color-3 &#123; background: #37B182; &#125;\n    .style-switcher .color-4 &#123; background: #1854B4; &#125;\n    .style-switcher .color-5 &#123; background: #F021B2; &#125;\n\n/* Create more 5 css files to enable Toggle style Switcher */\n\n/*\n\nFile Name\ncolor-1.css \n:root &#123;\n    --skin-color: #EC1839;\n&#125;\n\nFile Name\ncolor-2.css\n:root &#123;\n    --skin-color: #F45B0F;\n&#125;\n\nFile Name\ncolor-3.css\n:root &#123;\n    --skin-color: #37B182;\n&#125;\n\nFile Name \ncolor-4.css\n:root &#123;\n    --skin-color: #1854B4;\n&#125;\n\nFile Name\ncolor-5.css\n:root &#123;\n    --skin-color: #F021B2;\n&#125;\n\n*/\n\ntxt4. 创建[web]/static/js/app\n将以下内容输入 app.js\njs/* Toggle Style Switcher */\n\nconst styleSwitcherToggle = document.querySelector(&#39;.style-switcher-toggler&#39;);\nstyleSwitcherToggle.addEventListener(&#39;click&#39;, () =&gt; &#123; document.querySelector(&#39;.style-switcher&#39;).classList.toggle(&#39;open&#39;); &#125;)\n\nwindow.addEventListener(&#39;scroll&#39;, () =&gt; &#123; if(document.querySelector(&#39;.style-switcher&#39;).classList.contains(&#39;open&#39;)) &#123; document.querySelector(&#39;.style-switcher&#39;).classList.remove(&#39;open&#39;); &#125; &#125;)\n\nconst alternateStyles = document.querySelectorAll(&#39;.alternate-style&#39;);\nfunction setActiveStyle(color) &#123;\n    alternateStyles.forEach((style) =&gt; &#123;\n        if(color === style.getAttribute(&#39;title&#39;)) &#123; style.removeAttribute(&#39;disabled&#39;); &#125; else &#123; style.setAttribute(&#39;disabled&#39;, &#39;true&#39;); &#125;\n    &#125;)\n&#125;\n\n/* Dark/Light Mode */\n\nconst dayNight = document.querySelector(&#39;.day-night&#39;);\ndayNight.addEventListener(&#39;click&#39;, () =&gt; &#123;\n    dayNight.querySelector(&#39;i&#39;).classList.toggle(&#39;fa-sun&#39;);\n    dayNight.querySelector(&#39;i&#39;).classList.toggle(&#39;fa-moon&#39;);\n    document.body.classList.toggle(&#39;dark&#39;);\n&#125;)\n\nwindow.addEventListener(&#39;load&#39;, () =&gt; &#123;\n    if(document.body.classList.contains(&#39;dark&#39;)) &#123; dayNight.querySelector(&#39;i&#39;).classList.add(&#39;fa-sun&#39;); &#125; else &#123; dayNight.querySelector(&#39;i&#39;).classList.add(&#39;fa-moon&#39;); &#125;\n&#125;)\n\n/* Typing Animation */\n// 如果需要改动Home的打字特效显示的字要在这里改\nvar typed = new Typed(&#39;.typing&#39;, &#123; strings: [&quot;&quot;,&quot;大学生&quot;, &quot;电子发烧友&quot;, &quot;网页设计师&quot;, &quot;计算机视觉学者&quot;, &quot;博主&quot;], typeSpeed: 100, Backspeed: 60, loop: true &#125;)\n\n/* Changing Aside Active Link */\n\nconst nav = document.querySelector(&#39;.nav&#39;);\nconst navList = nav.querySelectorAll(&#39;li&#39;);\nconst totalNavList = navList.length;\nconst allSection = document.querySelectorAll(&#39;.section&#39;);\nconst totalSection = allSection.length;\n\nfor(let i = 0; i &lt; totalNavList; i++) &#123;\n    const a = navList[i].querySelector(&#39;a&#39;);\n    a.addEventListener(&#39;click&#39;, function()&#123;\n            removeBackSection();\n            for(let j = 0; j &lt; totalNavList; j++) &#123; \n                if(navList[j].querySelector(&#39;a&#39;).classList.contains(&#39;active&#39;)) &#123; addBackSection(j);/*allSection[j].classList.add(&#39;back-section&#39;);*/ &#125;\n                navList[j].querySelector(&#39;a&#39;).classList.remove(&#39;active&#39;); &#125;\n        this.classList.add(&#39;active&#39;);\n        showSection(this);\n\n        if(window.innerWidth &lt; 1200) &#123; asideSectionTogglerBtn(); &#125;\n    &#125;)\n&#125;\n\nfunction addBackSection(num) &#123; allSection[num].classList.add(&#39;back-section&#39;); &#125;\n\nfunction removeBackSection()&#123;\n    for( let i = 0; i &lt; totalSection; i++)&#123; allSection[i].classList.remove(&#39;back-section&#39;); &#125;\n&#125;\n\nfunction showSection(element)&#123;\n    for( let i = 0; i &lt; totalSection; i++)&#123; allSection[i].classList.remove(&#39;active&#39;); &#125;\n\n    const target = element.getAttribute(&quot;href&quot;).split(&quot;#&quot;)[1];\n    document.querySelector(&#39;#&#39; + target).classList.add(&#39;active&#39;);\n&#125;\n\nfunction updateNav(element)&#123;\n    for(let i = 0; i &lt; totalNavList; i++)&#123;\n        navList[i].querySelector(&#39;a&#39;).classList.remove(&#39;active&#39;);\n        const target = element.getAttribute(&#39;href&#39;).split(&#39;#&#39;)[1];\n        if(target === navList[i].querySelector(&#39;a&#39;).getAttribute(&#39;href&#39;).split(&#39;#&#39;)[1]) &#123; navList[i].querySelector(&#39;a&#39;).classList.add(&#39;active&#39;); &#125;\n        \n    &#125;\n&#125;\n\ndocument.querySelector(&#39;.hire-me&#39;).addEventListener(&#39;click&#39;, function()&#123;\n    const sectionIndex = this.getAttribute(&#39;data-section-index&#39;);\n    /*console.log(sectionIndex);*/\n    showSection(this);\n    updateNav(this);\n    removeBackSection();\n    addBackSection(sectionIndex);\n&#125;)\n\n/* Activating Mobile Menu */\n\nconst navTogglerBtn = document.querySelector(&#39;.nav-toggler&#39;);\nconst aside = document.querySelector(&#39;.aside&#39;);\n\nnavTogglerBtn.addEventListener(&#39;click&#39;, () =&gt; &#123; asideSectionTogglerBtn(); &#125;)\n\nfunction asideSectionTogglerBtn()&#123;\n    aside.classList.toggle(&#39;open&#39;);\n    navTogglerBtn.classList.toggle(&#39;open&#39;);\n    for(let i = 0; i &lt; totalSection; i++) &#123; allSection[i].classList.toggle(&#39;open&#39;); &#125;\n&#125;\n\n\ntxt5. 通过 VS Code 右键 index.html 可以在浏览器预览结果，确认显示结果无误后可以进入下一步骤。\n6. 将[web]文件夹上传到一个新建的 Github 仓库[web-repo]\n    \n\n\n\n\n\n\n\n\nTIP\n接下来我们要通过Netlify进行网站部署\n\ntxt7. 登陆 Netlify 网站，进入 Dashboard，新建网站，选择 import an existing project\n    \n\n\ntxt8. 选择 Github， 通过认证之后选择刚刚创建的[web-repo]仓库\n    \n\n\ntxt9. 填写网站名称 Site name，其他的不用管\n    \n\n\ntxt10. 进行网站部署，部署成功后 netlify 会给网站分配域名，后缀是.netlify.app，后期SSL证书和自定义域名需要自己在上\n    \n\n\ntxt11. 通过浏览器输入对应域名检查部署是否成功，之后只需要将文件上传[web-repo]仓库，网站便会从 Github 自动化部署。\n\n\n\n\n\nTIP\n至此网站搭建完毕\n\n新花样1.将你的展示图片变成卡片展示实现下图的样式\n\n    \n\n\n改造步骤txt1. 在 css 文件夹中加入 card.css 文件\n将以下内容输入 card.css\ncss@import url(&quot;https://fonts.googleapis.com/css2?family=Comfortaa:wght@300;400;500;600;700&amp;family=Poppins:wght@300;400;500;600;700;800&amp;display=swap&quot;);\n\n.card-container &#123;\n  *,\n  *::before,\n  *::after &#123;\n    margin: 0;\n    left: 30px;\n    padding: 0;\n    box-sizing: border-box;\n    font-family: &quot;Comfortaa&quot;, cursive;\n  &#125;\n\n  section &#123;\n    position: relative;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    background: #262626;\n    min-height: 100vh;\n    overflow: hidden;\n  &#125;\n\n  .content &#123;\n    display: flex;\n    flex-direction: row;\n    justify-content: center;\n    align-items: center;\n    gap: 30px;\n    background: linear-gradient(\n      180deg,\n      rgba(255, 255, 255, 0.28) 0%,\n      rgba(255, 255, 255, 0) 100%\n    );\n    backdrop-filter: blur(30px);\n    border-radius: 20px;\n    width: min(900px, 100%);\n    box-shadow: 0 0.5px 0 1px rgba(255, 255, 255, 0.23) inset,\n      0 1px 0 0 rgba(255, 255, 255, 0.66) inset, 0 4px 16px rgba(0, 0, 0, 0.12);\n    z-index: 10;\n  &#125;\n\n  .info &#123;\n    display: flex;\n    flex-direction: column;\n    justify-content: center;\n    align-items: center;\n    max-width: 450px;\n    padding: 0 35px;\n    text-align: justify;\n  &#125;\n\n  .info p &#123;\n    color: #fff;\n    font-weight: 500;\n    font-size: 1rem;\n    margin-bottom: 20px;\n    line-height: 1.5;\n  &#125;\n\n  .movie-night &#123;\n    background: linear-gradient(225deg, #ff3cac 0%, #784ba0 50%, #2b86c5 100%);\n  &#125;\n\n  .btn &#123;\n    display: block;\n    padding: 10px 40px;\n    margin: 10px auto;\n    font-size: 1.1rem;\n    font-weight: 700;\n    border-radius: 4px;\n    outline: none;\n    text-decoration: none;\n    color: #784ba0;\n    background: rgba(255, 255, 255, 0.9);\n    box-shadow: 0 6px 30px rgba(0, 0, 0, 0.1);\n    border: 1px solid rgba(255, 255, 255, 0.3);\n    cursor: pointer;\n  &#125;\n\n  .btn:hover &#123;\n    animation: gelatine 0.5s 1;\n  &#125;\n\n  @keyframes gelatine &#123;\n    0%,\n    100% &#123;\n      transform: scale(1, 1);\n    &#125;\n    25% &#123;\n      transform: scale(0.9, 1.1);\n    &#125;\n    50% &#123;\n      transform: scale(1.1, 0.9);\n    &#125;\n    75% &#123;\n      transform: scale(0.95, 1.05);\n    &#125;\n  &#125;\n\n  /* SWIPER */\n\n  .swiper &#123;\n    width: 250px;\n    height: 450px;\n    padding: 50px 0;\n  &#125;\n\n  .swiper-slide &#123;\n    position: relative;\n    box-shadow: 0 15px 50px rgba(0, 0, 0, 0.2);\n    border-radius: 10px;\n    user-select: none;\n  &#125;\n\n  .swiper-slide img &#123;\n    position: absolute;\n    inset: 0;\n    width: 100%;\n    height: 100%;\n    object-fit: cover;\n  &#125;\n\n  .img-position &#123;\n    object-position: 50% 0%;\n  &#125;\n\n  .overlay &#123;\n    position: absolute;\n    inset: 0;\n    width: 100%;\n    height: 100%;\n    background: linear-gradient(to top, #0f2027, transparent, transparent);\n    background-repeat: no-repeat;\n    background-size: cover;\n  &#125;\n\n  .overlay span &#123;\n    position: absolute;\n    top: 0;\n    right: 0;\n    color: #fff;\n    padding: 7px 18px;\n    margin: 10px;\n    border-radius: 20px;\n    letter-spacing: 2px;\n    font-size: 0.8rem;\n    font-weight: 700;\n    font-family: inherit;\n    background: rgba(255, 255, 255, 0.095);\n    box-shadow: inset 2px -2px 20px rgba(214, 214, 214, 0.2),\n      inset -3px 3px 3px rgba(255, 255, 255, 0.4);\n    backdrop-filter: blur(74px);\n  &#125;\n\n  .overlay h2 &#123;\n    position: absolute;\n    bottom: 0;\n    left: 0;\n    color: #fff;\n    font-weight: 400;\n    font-size: 1.1rem;\n    line-height: 1.4;\n    margin: 0 0 20px 20px;\n  &#125;\n\n  /* ANIMATED BACKGROUND */\n\n  .circles &#123;\n    position: absolute;\n    top: 0;\n    left: 0;\n    width: 100%;\n    height: 100%;\n    overflow: hidden;\n  &#125;\n\n  .circles li &#123;\n    position: absolute;\n    display: block;\n    list-style: none;\n    width: 20px;\n    height: 20px;\n    background-color: #ff3cac;\n    background-image: linear-gradient(\n      225deg,\n      #ff3cac 0%,\n      #784ba0 50%,\n      #2b86c5 100%\n    );\n    animation: animate 25s linear infinite;\n    bottom: -150px;\n  &#125;\n\n  .circles li:nth-child(1) &#123;\n    left: 25%;\n    width: 80px;\n    height: 80px;\n    animation-delay: 0s;\n  &#125;\n\n  .circles li:nth-child(2) &#123;\n    left: 10%;\n    width: 20px;\n    height: 20px;\n    animation-delay: 2s;\n    animation-duration: 12s;\n  &#125;\n\n  .circles li:nth-child(3) &#123;\n    left: 70%;\n    width: 20px;\n    height: 20px;\n    animation-delay: 4s;\n  &#125;\n\n  .circles li:nth-child(4) &#123;\n    left: 40%;\n    width: 60px;\n    height: 60px;\n    animation-delay: 0s;\n    animation-duration: 18s;\n  &#125;\n\n  .circles li:nth-child(5) &#123;\n    left: 65%;\n    width: 20px;\n    height: 20px;\n    animation-delay: 0s;\n  &#125;\n\n  .circles li:nth-child(6) &#123;\n    left: 75%;\n    width: 110px;\n    height: 110px;\n    animation-delay: 3s;\n  &#125;\n\n  .circles li:nth-child(7) &#123;\n    left: 35%;\n    width: 150px;\n    height: 150px;\n    animation-delay: 7s;\n  &#125;\n\n  .circles li:nth-child(8) &#123;\n    left: 50%;\n    width: 25px;\n    height: 25px;\n    animation-delay: 15s;\n    animation-duration: 45s;\n  &#125;\n\n  .circles li:nth-child(9) &#123;\n    left: 20%;\n    width: 15px;\n    height: 15px;\n    animation-delay: 2s;\n    animation-duration: 35s;\n  &#125;\n\n  .circles li:nth-child(10) &#123;\n    left: 85%;\n    width: 150px;\n    height: 150px;\n    animation-delay: 0s;\n    animation-duration: 11s;\n  &#125;\n\n  @keyframes animate &#123;\n    0% &#123;\n      transform: translateY(0) rotate(0deg);\n      opacity: 1;\n      border-radius: 0;\n    &#125;\n\n    100% &#123;\n      transform: translateY(-1000px) rotate(720deg);\n      opacity: 0;\n      border-radius: 50%;\n    &#125;\n  &#125;\n\n  /*\n  @media (max-width: 750px) &#123;\n    .content &#123;\n       flex-direction: column-reverse;\n    &#125;\n    \n    .btn &#123;\n      margin: 10px auto 40px;\n    &#125;\n  &#125;\n  */\n&#125;\n\n\n\ntxt2. 在 js 文件夹中加入 card.js 文件\n将以下内容输入 card.js\njsvar swiper = new Swiper(&quot;.swiper&quot;, &#123;\n    effect: &quot;cards&quot;,\n    grabCursor: true,\n    initialSlide: 2,\n    speed: 500,\n    loop: true,\n    rotate: true,\n    mousewheel: &#123;\n    invert: false,\n  &#125;,\n&#125;);\n\n\ntxt3. 在 index.html 中进行更改\n在index.html中进行如下更改\ntxt找到如下代码段html&lt;div class=&quot;home-img padd-15&quot;&gt;\n    &lt;img src=&quot;../static/img/me.png&quot; alt=&quot;hero image&quot;&gt;\n&lt;/div&gt;\ntxt将其更改为如下代码段html&lt;div class=&quot;card-container&quot;&gt;\n    &lt;div class=&quot;swiper&quot;&gt;\n        &lt;div class=&quot;swiper-wrapper&quot;&gt;\n            &lt;div class=&quot;swiper-slide&quot;&gt;\n                &lt;img\n                src=&quot;../static/img/doti.jpg&quot;\n                alt=&quot;&quot; /&gt;\n                &lt;!-- 更改为你自己的照片 --&gt;\n                &lt;div class=&quot;overlay&quot;&gt;\n                    &lt;!-- &lt;span&gt;8.5&lt;/span&gt; --&gt;\n                     &lt;!-- 评分 --&gt;\n                    &lt;h2&gt;The High Priestess&lt;/h2&gt;\n                    &lt;!-- 介绍文字 --&gt;\n                    &lt;/div&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n\n","slug":"从零开始搭建-HTML-静态个人展示主页","date":"2025-03-09T15:43:14.000Z","categories_index":"Web Design","tags_index":"HTML","author_index":"Yesord"},{"id":"6a624db9cd9d3612f9788dcb3f188088","title":"事件流数据提取直线","content":"前言最近在跟导师做科研项目需要测量高频振动正好用到事件相机，需要提取事件流数据中的直线特征，在这里记录一下实验数据采集与处理的过程。\n相机配置事件相机：我们使用的是 inivation 的 DAVIS346 ，是一款346x240 分辨率的 DAVIS 相机。镜头：由于事件相机的分辨率受限，尽量使用长焦镜头可以得到更多的微振动细节信息，我们使用50cm焦段的变焦镜头。\n系统搭建我们使用直尺和砝码（重物）在椅子上构造了一个悬臂梁系统。使用事件相机的目的是测量尺子的振动频率与幅度，因此我们通过简单的架设相机和适当的补光，将事件相机对准悬臂梁系统。\n软件采集使用 inivation 公司提供的DV软件进行事件相机的数据采集， DV 的功能已经相对完善，但是由于一些软件 bug ，切换不同的可视化页面容易闪退，因此受不了的也可以自己DIY写脚本进行采集。采集的数据为 .aedat4 文件，这是 inivation 自己的一种文件格式（详情可以参考 doc ）\n事件数据由于事件相机的事件流数据的特殊之处，我们采集得到的 aedat4 文件里面存储的数据不能像常规处理 RGB 相机那样直接对图像进行处理，aedat4 里的一个事件读取出来的结果是 [x坐标，y坐标，t时间戳，p极性] ，其所代表的含义就是在 t 时间点时，事件相机的 CMOS 坐标 (x,y) 光强变化 p 是 + or -。\n因此我想到了有两种方法来处理事件数据：\n\n最直接的处理就是将 t 作为第三个坐标轴，极性不参与考虑，将事件数据转变成 3D 点云数据，按照点云的处理方法来处理事件数据。\n我们也可以将事件数据进行时间上的切片与积累，例如每次处理 10ms（100Hz）的数据，直接得到一个事件流的积累图像，即一个二维的图像，很多方法就可以沿用以前处理RGB图像的方法了。\n\n数据预处理我采用的是上述的方法2进行事件数据的初步预处理，可以得到类似下图的效果，红色为负极性事件，绿色为正极性事件\n\n\n然后进行事件数据的降噪，排除掉周围噪点，得到下图效果。\n\n\n\n直线提取直线的提取我试过的方法共有两种，最简单直接的就是直接用 LSD 进行直线的提取，这个通过 openCV 直接调库就能简单实现，LSD 也有很多参数可以进行调整，调一调的效果也许会比较好，在下面我贴出我使用 LSD 跑出来的结果，只能说差强人意，太吃数据了，跑出来的结果与其说是提取直线，我倒是觉得像是提取边缘。我调研到的用 LSD 提线的效果比较好的场景是 2D 激光雷达，对于事件相机这种相对弥散的点数据来说 LSD 真不太行。\n\n\n\n\n第二种我采用的方案是 DBSCAN 聚类 + RANSAC 拟合直线的方法来进行直线的提取，这种方法对于我们这种希望在事件数据弥散点云中提取出直线信息的目的更为契合。DBSCAN 聚类能有效的将不同的事件分类成簇，再使用 RANSAC 能排除杂散点有效的拟合最符合的直线作为我们认定的直线特征。结果如下视频所示。\n\n  \n\n\n后记本次实验所采用的方法还是以往的机器学习方法，在现在深度学习发展很火热的情况下，只要不是追求实时的提线，做检测而不是监测，那么深度学习的方法是可以去钻研钻研的。\n","slug":"事件流数据提取直线","date":"2025-02-17T11:41:20.000Z","categories_index":"ComputerVision","tags_index":"event-base","author_index":"Yesord"},{"id":"3ff61eba6150d3a0aff8b5852c3e4e2e","title":"安知鱼博客主题魔改记录","content":"前言安知鱼主题是一个基于Butterfly主题二次创作的很美观便捷的博客主题。我也搭建了一个基于安知鱼主题的网站，本文旨在记录本人使用安知鱼主题进行的博客魔改记录。只要我有进行魔改便会记录在这篇文章中。\n魔改记录本人不是计算机专业，没有系统性学过前端，魔改代码可能比较业余，如有问题，希望各位大佬指正。\n\n\n\n\n\n\n\n⚠️⚠️⚠️\n魔改有风险，注意先备份！！！\n\n\n关于页面的简单卡片扩展本身安知鱼主题的 about 卡片是不能够直接在配置文件里新加的，本人感觉展示页的内容不够多，希望再加上几张卡片。\n\n  \n\n\n效果展示\n魔改过程进入网页开发者模式（Windows 用 F12），可以很容易查到安知鱼主题的 about 卡片都是属于一个叫author-content-item 类的\n\n\n又通过查看安知鱼主题下的 about.pug 模版文件，搜索 author-content-item ，找到了相应的位置，大概在 100 多行。如下所示，对应的是关注偏好和音乐偏好的两张卡片。about.pug 模版文件在博客根目录的 &#x2F;theme&#x2F;anzhiyu&#x2F;layout&#x2F;includes&#x2F;page 里，pug 文件是指导生成最后的 HTML 文件的，我们更改它是可以最终更改最终页面的显示的。\n\n关注偏好和音乐偏好卡片源码\npug      .author-content\n        - let &#123;music_tips, music_title, music_link, music_bg&#125; = item.music\n        - let &#123;like_tips, like_title, like_bottom, like_bg&#125; = item.like\n        .author-content-item.like-technology(style=`background: url($&#123;like_bg&#125;) top / cover no-repeat;`)\n          .card-content\n            .author-content-item-tips=like_tips\n            span.author-content-item-title=like_title\n            .content-bottom\n              .tips=like_bottom\n        .author-content-item.like-music(style=`background: url($&#123;music_bg&#125;) top / cover no-repeat;`)\n          .card-content\n            .author-content-item-tips=music_tips\n            span.author-content-item-title=music_title\n            .content-bottom\n              .tips=`跟 $&#123;aboutName&#125; 一起欣赏更多音乐`\n            .banner-button-group\n              a.banner-button(onclick=`pjax.loadUrl(&quot;$&#123;music_link&#125;&quot;)`)\n                i.anzhiyufont.anzhiyu-icon-arrow-circle-right\n                |  \n                span.banner-button-text 更多推荐\n\n\n因此我们只要想办法在 pug 上改这个就可以实现加减卡片了。介于本人的技术力不够高，只能在 pug 模版文件上改改，再新写一个 js 文件辅助。\ntxt1. 更改 pug 文件，加入新增的两张卡片的代码，具体代码可以与下文比较确定在 pug 文件中的位置再添加。\nabout.pug 部分魔改源码\npug      \n      .author-content\n        .author-content-item.maxim\n          - let &#123;maxim_tips, maxim_top, maxim_bottom&#125; = item.maxim\n          .author-content-item-tips=maxim_tips\n          span.maxim-title\n            span(style=&#39;opacity:.6;margin-bottom:8px&#39;)=maxim_top\n            |  \n            span=maxim_bottom\n        .author-content-item.buff\n          .card-content\n          - let &#123;buff_tips, buff_top, buff_bottom&#125; = item.buff\n            .author-content-item-tips=buff_tips\n            span.buff-title\n              span(style=&#39;opacity:.6;margin-bottom:8px&#39;)=buff_top\n              |  \n              span=buff_bottom\n          .card-background-icon\n            i.anzhiyufont.anzhiyu-icon-dice-d20\n\n      //- 新加的，在about页多一栏展示，但是得纯手调\n      .author-content\n        .author-content-item.like-technology#rotating-content(style=`background: url(/img/about_img/titanfall2.jpg) top / cover no-repeat;`)\n          .card-content\n            .author-content-item-tips=&quot;第九艺术&quot;\n            span.author-content-item-title=&quot;协议三：保护铁驭&quot;\n            .content-bottom\n              .tips=&quot;TitanFall2&quot;\n        .author-content-item.like-music(style=`background: url(&quot;/img/about_img/sea.jpg&quot;) top / cover no-repeat;`)\n          .card-content\n            .author-content-item-tips=&quot;我的图片集&quot;\n            span.author-content-item-title=&quot;见证生活&quot;\n            .content-bottom\n              .tips=`如生命般的谜团`\n            .banner-button-group\n              a.banner-button(onclick=`pjax.loadUrl(&quot;/album&quot;)`)\n                i.anzhiyufont.anzhiyu-icon-arrow-circle-right\n                |  \n                span.banner-button-text 瞅一眼\n        //\n\n      .author-content\n        - let &#123;game_tips, game_title, game_uid, game_bg&#125; = item.game\n      \n        .author-content-item.game-yuanshen(style=`background: url($&#123;game_bg&#125;) top / cover no-repeat;`)\n          .card-content\n            .author-content-item-tips=game_tips\n            span.author-content-item-title=game_title\n            .content-bottom\n              .icon-group\n                .loading-bar(role=&#39;presentation&#39;, aria-hidden=&#39;true&#39; style=`$&#123;game_title != &quot;原神&quot; ? &quot;display: none&quot;: &quot;&quot;&#125;`)\n              .tips.game-yuanshen-uid=game_uid\n\n        .author-content-item.comic-content\n          .card-content\n            - let &#123;comic_tips, comic_title, comic_list&#125; = item.comic\n            .author-content-item-tips=comic_tips\n            .author-content-item-title=comic_title\n            .comic-box\n              if comic_list\n                each i in comic_list\n                  a.comic-item(href=i.href, target=&quot;_blank&quot;, title=i.name)\n                    .comic-item-cover\n                      img(src=i.cover, alt=i.name)\n      \n      .author-content\n        - let &#123;music_tips, music_title, music_link, music_bg&#125; = item.music\n        - let &#123;like_tips, like_title, like_bottom, like_bg&#125; = item.like\n        .author-content-item.like-technology(style=`background: url($&#123;like_bg&#125;) top / cover no-repeat;`)\n          .card-content\n            .author-content-item-tips=like_tips\n            span.author-content-item-title=like_title\n            .content-bottom\n              .tips=like_bottom\n        .author-content-item.like-music(style=`background: url($&#123;music_bg&#125;) top / cover no-repeat;`)\n          .card-content\n            .author-content-item-tips=music_tips\n            span.author-content-item-title=music_title\n            .content-bottom\n              .tips=`跟 $&#123;aboutName&#125; 一起欣赏更多音乐`\n            .banner-button-group\n              a.banner-button(onclick=`pjax.loadUrl(&quot;$&#123;music_link&#125;&quot;)`)\n                i.anzhiyufont.anzhiyu-icon-arrow-circle-right\n                |  \n                span.banner-button-text 更多推荐\n                \n\ntxt2. 添加循环图片的 js 代码， 新建rotating-content.js 文件在 /public/js/anzhiyu 下， 添加如下内容\nrotating-content.js\njs\ndocument.addEventListener(&#39;DOMContentLoaded&#39;, function() &#123;\n    const content = document.getElementById(&#39;rotating-content&#39;);\n    const images = [ //这里的url可以填入图链也可以是相对/public的文件路径\n      &#123; url: &#39;/img/about_img/titanfall2.jpg&#39;, tips: &#39;第九艺术&#39;, title: &#39;协议三：保护铁驭&#39;, bottom: &#39;TitanFall 2&#39; &#125;,\n      &#123; url: &#39;/img/about_img/cyberpunk2077.jpg&#39;, tips: &#39;第九艺术&#39;, title: &#39;以我残躯化烈火&#39;, bottom: &#39;CyberPunk 2077&#39; &#125;,\n      &#123; url: &#39;/img/about_img/assassins_creed.jpg&#39;, tips: &#39;第九艺术&#39;, title: &#39;万物皆虚，万事皆允&#39;, bottom: &#39;Assassin\\&#39;s Creed&#39; &#125;,\n      &#123; url: &#39;/img/about_img/riden.jpg&#39;, tips: &#39;第九艺术&#39;, title: &#39;我要你知道，开膛手杰克回来了！&#39;, bottom: &#39;Metal Gear Rising: Revengeance&#39; &#125;,\n      &#123; url: &#39;/img/about_img/detroit_become_human.jpg&#39;, tips: &#39;第九艺术&#39;, title: &#39;RA9&#39;, bottom: &#39;Detroit: Become Human&#39; &#125;,\n      &#123; url: &#39;/img/about_img/seker0.jpg&#39;, tips: &#39;第九艺术&#39;, title: &#39;犹豫就会败北&#39;, bottom: &#39;Sekiro: Shadows Die Twice&#39; &#125;,\n      &#123; url: &#39;/img/about_img/death_stranding.jpg&#39;, tips: &#39;第九艺术&#39;, title: &#39;Don\\&#39;t be so serious&#39;, bottom: &#39;Death Stranding&#39; &#125;,\n    \n      \n      // 添加更多图片和文字\n    ];\n    let currentIndex = 0;\n  \n    function updateContent() &#123;\n      const &#123; url, tips, title, bottom &#125; = images[currentIndex];\n      const nextIndex = (currentIndex + 1) % images.length;\n      const nextImage = images[nextIndex];\n  \n      // 设置当前背景图层\n      content.style.backgroundImage = `url($&#123;url&#125;), url($&#123;nextImage.url&#125;)`;\n      content.style.transition = &#39;background-image 0.5s ease-in-out&#39;;\n      content.querySelector(&#39;.author-content-item-tips&#39;).textContent = tips;\n      content.querySelector(&#39;.author-content-item-title&#39;).textContent = title;\n      content.querySelector(&#39;.content-bottom .tips&#39;).textContent = bottom;\n  \n      setTimeout(() =&gt; &#123;\n        content.style.backgroundImage = `url($&#123;nextImage.url&#125;)`;\n        content.querySelector(&#39;.author-content-item-tips&#39;).textContent = nextImage.tips;\n        content.querySelector(&#39;.author-content-item-title&#39;).textContent = nextImage.title;\n        content.querySelector(&#39;.content-bottom .tips&#39;).textContent = nextImage.bottom;\n        currentIndex = nextIndex;\n      &#125;, 500); // 500ms 与 CSS 过渡时间匹配\n    &#125;\n  \n    setInterval(updateContent, 5000); // 每5秒轮换一次\n  &#125;);\n\n\ntxt3. 在 about.pug 中要引用这个 js 文件，即在 pug 文件的末尾加上如下代码\n添加引用\npug    script(src=url_for(&#39;/js/anzhiyu/rotating-content.js&#39;))\n\n魔改完成！！！\n","slug":"安知鱼博客主题魔改记录","date":"2025-01-29T09:38:30.000Z","categories_index":"hexo","tags_index":"blog","author_index":"Yesord"},{"id":"cae7ea9bc7a219316383e5056946cd4d","title":"相机标定总结","content":"前言想做相机的定位和测量，这是必不可少的一步就是相机标定。相机标定的目的是确定相机的内参和外参，以便将图像坐标转换为实际的三维坐标，亦可以将实际的三维坐标转换为图像坐标。本文就从简单的单目相机出发，主要从代码上对相机标定进行解析，相关的理论可以去参考其他博客。\n单目相机标定实践目的txt使用张正友标定法对手机主摄相机进行标定。实践步骤\n\n\n\n\n\nTIP\ntxta. 准备工作：选择一台手机，写明手机型号。\n\nb. 实践准备：下载并安装OpenCV库，确保可以在编程环境中调用相关函数。\n\nc. 实践实施：\n拍摄标定棋盘格照片：在不同的角度、距离和姿态下，使用手机主摄相机拍摄至少10张标定棋盘格照片。\n图像处理：使用OpenCV库中的函数对照片进行图像处理，提取出标定棋盘格的角点坐标。\n标定相机：利用提取得到的角点坐标，使用张正友标定法计算相机的内参和畸变参数。\n\nd. 实践结果分析：根据标定结果，分析相机的内参和畸变参数的含义和作用。\n\n实现过程手机选型txt使用Apple iPhone Xs max\n\n\n库环境配置txt实践代码主要采用OpenCV库进行编写，需要下载OpenCV-python库，本次实践是在anaconda和vscode编辑器的环境下进行的，\n下载安装OpenCV-python库可以就通过vscode的内置终端完成。只需要在终端输入如下命令：bashpip install opencv-python\n采集标定板图像在线棋盘格生成\ntxt在本次实践中，我们使用的是棋盘格标定板\n8*11 格宽20mm的棋盘格标定板\n\ntxt固定好手机，手持该标定板，站在手机的视场范围内，上下左右平移和旋转移动标定板位置，进行图像采集。\n标定图像采集结果\n\nOpenCV图像处理角点计算txt将采集到的图像输入OpenCV脚本进行亚像素级别的角点检测提取角点位置并建立对应的三维空间坐标\n角点位置可视化\n\n\n三维空间坐标系可视化\n\n\ntxt将三维坐标和图像中的角点坐标带入OpenCV函数可以得到五个参数python&quot;&quot;&quot;\n求解参数\n输入：世界坐标系里的位置；像素坐标；图像的像素尺寸大小；\n输出：\nret: 重投影误差；\nmtx：内参矩阵；\ndist：畸变系数；\nrvecs：旋转向量 （外参数）；\ntvecs：平移向量 （外参数）；\n&quot;&quot;&quot;\nret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n优化内参数和畸变系数txt使用相机内参mtx和畸变系数dist，并使用cv2.getOptimalNewCameraMatrix()python# 优化内参数和畸变系数\n# 使用相机内参mtx和畸变系数dist，并使用cv.getOptimalNewCameraMatrix()\n# 通过设定自由自由比例因子alpha。\n# 当alpha设为0的时候，将会返回一个剪裁过的将去畸变后不想要的像素去掉的内参数和畸变系数；\n# 当alpha设为1的时候，将会返回一系个包含额外黑色像素点的内参数和畸变数，并返回一个ROI用于将其剪裁掉。\nnewcameramtx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (img_w, img_h), 0, (img_w, img_h))\n矫正畸变txt使用cv2.undistort()函数pythondst = cv2.undistort(img2, mtx, dist, None, newcameramtx)\n矫正畸变之后的图像\n\ntxt可以观察到其实畸变矫正前后的图像变化并不大，可知手机镜头的畸变并不大。计算反投影误差与外参矩阵txt根据得到的相机的外参数矩阵和内参数矩阵，将三维世界坐标系映射到二维图像坐标系中，\n计算三维世界坐标系和二维图像坐标系之间的 L2 范数，然后除以点的数量以得到平均误差。python# 反投影误差total_error,越接近0，说明结果越理想。\ntotal_error = 0\nfor i in range(len(objpoints)):\n    imgpoints2, _ = cv2.projectPoints(objpoints[i], rvecs[i], tvecs[i], mtx, dist)   # 计算三维点到二维图像的投影\n    error = cv2.norm(imgpoints[i], imgpoints2, cv2.NORM_L2) / len(imgpoints2)   # 反投影得到的点与图像上检测到的点的误差\n    total_error += error\nprint((&quot;total error: &quot;), total_error / len(objpoints))   # 记平均\nprint((&quot;actual_error: &quot;), total_error * square_size / len(objpoints)) # 实际尺度上的误差单位（mm）txt通过罗德里格斯变换将旋转向量转换成旋转矩阵，将旋转矩阵和平移向量进行拼接形成外参矩阵python# 将旋转向量转换为旋转矩阵\nR, _ = cv2.Rodrigues(rvecs[0])\n# 构建外参矩阵\nextrinsic_matrix = np.hstack((R, tvecs[0]))标定结果bash\nret（重投影误差）: 0.1413772517683631\nmtx（内参矩阵）:\n [[1.31882019e+03 0.00000000e+00 8.57509174e+02]\n [0.00000000e+00 1.31860717e+03 6.33641314e+02]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]]\ndist（畸变参数）:\n [[ 2.20700651e-01 -9.82057636e-01  1.24391971e-03  5.90204064e-04\n   1.05299236e+00]]\nrvecs（旋转向量）:\n (array([[-0.07040504],\n       [ 0.02458443],\n       [ 0.23714922]]), array([[-0.07666068],\n       [ 0.21833802],\n       [ 0.0387807 ]]), array([[-0.22630376],\n       [-0.16093939],\n       [ 0.04463943]]), array([[-0.21575282],\n       [ 0.169161  ],\n       [ 0.01774507]]), array([[-0.04331008],\n       [ 0.26869301],\n       [-0.04642156]]), array([[ 0.01362953],\n       [ 0.11523612],\n       [-0.27805932]]), array([[-0.07350822],\n       [-0.11668527],\n       [ 0.37951501]]), array([[-0.01487231],\n       [-0.16699536],\n       [ 0.13681347]]), array([[ 0.00788721],\n       [-0.15841166],\n       [-0.25513863]]), array([[-0.10608478],\n       [ 0.19683212],\n       [ 0.4318067 ]]), array([[-0.13768347],\n       [-0.0315557 ],\n       [ 0.03572001]]), array([[-0.05685385],\n       [ 0.07553545],\n       [-0.16208654]]), array([[-0.06280539],\n       [ 0.13300909],\n       [ 0.04773781]]), array([[-0.01146554],\n       [ 0.00381729],\n       [ 0.03160458]]), array([[0.07776906],\n       [0.18055448],\n       [0.04001535]]), array([[-0.27461065],\n       [ 0.19452584],\n       [ 0.07211498]]), array([[-0.27116111],\n       [-0.00959587],\n       [ 0.05686497]]), array([[ 0.04804264],\n       [-0.06006959],\n       [ 0.05891735]]), array([[0.20324042],\n       [0.16290313],\n       [0.00683915]]), array([[-0.17702452],\n       [ 0.02876084],\n       [ 0.02663478]]))\ntvecs（平移向量）:\n (array([[  5.89107127],\n       [-90.20451726],\n       [740.72705078]]), array([[ 71.10716462],\n       [-71.0584125 ],\n       [712.66197123]]), array([[-169.97106025],\n       [ -58.59923787],\n       [ 794.78611693]]), array([[ 48.43607734],\n       [-74.90782221],\n       [811.81301621]]), array([[ 103.69168092],\n       [-156.16442287],\n       [ 810.12092573]]), array([[  21.16412172],\n       [-126.01889817],\n       [ 696.46118615]]), array([[-165.6582237 ],\n       [-201.04863421],\n       [ 672.13293143]]), array([[-155.67441912],\n       [-172.14787045],\n       [ 680.15419596]]), array([[-215.57912659],\n       [-147.6266857 ],\n       [ 609.9832129 ]]), array([[ 103.63010108],\n       [-209.23021318],\n       [ 774.8869188 ]]), array([[ -31.98368466],\n       [-138.22382981],\n       [ 763.95694362]]), array([[-90.84268402],\n       [-95.07723685],\n       [700.66792599]]), array([[ 109.57667227],\n       [-125.08005104],\n       [ 748.29416674]]), array([[-92.05567469],\n       [-93.55191194],\n       [684.95003553]]), array([[ 37.54766021],\n       [-80.46249655],\n       [700.43156552]]), array([[ 46.82545373],\n       [-79.72855492],\n       [788.33084715]]), array([[-165.36023846],\n       [ -88.2298791 ],\n       [ 784.91078991]]), array([[-159.86010857],\n       [ -95.03131382],\n       [ 716.96008586]]), array([[ 41.42167738],\n       [-70.53566402],\n       [691.08121097]]), array([[-24.66936381],\n       [-91.20291511],\n       [744.95852765]]))\nnewcameramtx（优化后相机内参）:\n [[1.33806621e+03 0.00000000e+00 8.57359155e+02]\n [0.00000000e+00 1.33715907e+03 6.35504339e+02]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]]\ntotal error:  0.014879780065884956\nactual_error:  0.29759560131769913\n外参矩阵：\n[[ 9.71724002e-01 -2.35575063e-01  1.60266417e-02  5.89107127e+00]\n [ 2.33853091e-01  9.69558949e-01  7.25821976e-02 -9.02045173e+01]\n [-3.26373296e-02 -6.67819839e-02  9.97233659e-01  7.40727051e+02]]\ntxt可以看到误差很小，基本可以认为标定结果在1个毫米内是准确的。MATLAB快速标定txt使用MATLAB可以低代码的实现快速单目标定。\n使用MATLAB的APP里的Camera Calibrator\n\n\ntxt向Camera Calibrator中通过FILE选项卡下的Add Images选项加入图片，\n在使用CALIBRATE选项卡下的calibrate选项进行标定\n\n\ntxt通过EXPORT选项卡下的Export Camera Parameters向外导出相机参数。\n之后可以在MATLAB的变量空间中查看到cameraParams\nshow cameraParams\ntxt内参矩阵\n\ntxt外参矩阵\n\ntxt旋转向量\n\ntxt平移向量\n\n\n\n\n双目相机标定txt未完待续","slug":"相机标定总结","date":"2024-12-11T14:59:03.000Z","categories_index":"ComputerVision","tags_index":"calibration","author_index":"Yesord"},{"id":"3a5fab53c5d0fadb2462ad0b4deca422","title":"光学设计实操（二）—— 地面望远镜设计","content":"前言三大经典光学系统设计之望远镜\n\n设计要求\n\n\n\n\n\n\nREQUIREMENT\n设计一个用于看演出看比赛的望远镜。其物镜孔径42mm，放大倍数为12倍，物方最大视场角为3度（全视场6度）。（如需使用薄透镜，系统最后一面应距薄透镜10mm，薄透镜焦距25mm）\n硬性要求：不能使用非球面。\n\n\n\n\n设计分析采用伽利略式望远镜\n\n\n\n\n\n\n\n截止频率\n\n计算衍射极限分辨率\n\n\n\n计算截止频率\n\n\n\n\n\n\n\n\n\n\n\n目镜设计\n最后的望远镜结构是根据目镜的选用而决定的。本次设计没有对目镜有所限制因此根据网上的一般设计，我们认为目镜需要至少20度的半视场所以，我们决定使用相对比较复杂的目镜设计——埃尔弗型目镜。出曈5mm，采用35mm的目镜焦距。\n\n\n\n\n\n\n\n\n\n物镜设计\n物镜的孔径已知为42mm直径，根据目镜焦距可以推算出需要35x12&#x3D;420mm的焦距。考虑需要消像差，因此我们选择使用双胶合镜头为初始结构进行优化。\n\n\n设计过程\n\n\n\n\n\n\nEXPERIENCE\n物目镜分别设计，目镜很多现成的可以直接套，再EFLY优化焦距，物镜就用双胶合就行，用EFLY优化焦距，一定要把物镜设计的比较完美不然上目镜就是一坨，如果设计物镜的时候光路长度过大可以考虑之后加转向棱镜，光路越长转折越小像质越好\n\n\n目镜采用埃尔弗型目镜\n\n目镜初始结构\n\n\n设置EFLY操作数然后设置中间的对称镜厚度进行优化\n\n\n\n再进行反转镜头，得到的最终目镜镜头\n\n\n\n\n目镜系统数据\n\n\n物镜没有选取初始结构只是简单写出了一个双胶合消色差透镜，通过直接优化得到的\n\n\n\n\n\n\n\n优化过程\nSTEP1: 使用默认函数进行优化加上EFLY控制焦距，如果像质不理想可以考虑STEP2STEP2: 将双胶合镜片改装成具有空气间隔的双分离镜。重新设置对双分离镜进行优化STEP3: 将全部曲率和焦点设置成变量进行优化，直到现有的像差小于0.25个波长认为可以了\n\n\n得到的物镜光路长度有500mm总体是偏长的，如果不加入转向系统的话对于用户来说几乎是过于笨重，使用起来极度不舒适。因此在系统中加入平板模拟棱镜，最终得到参数如下：\n\n\n\n\n目镜系统数据\n\n\n镜组拼接将设计好的物镜和目镜进行拼接，调整目镜与物镜之间的距离，找到合适的位置（找共焦就行了），设置出曈距10mm处的25mm焦距薄透镜模拟人眼，设置 Marginal Ray Normal 最终可以得到效果如下\n\n\n\n\n望远系统数据\n\n\n\n\n最终设计完成\n\n\n后记有了上次设计猫眼的经历积累了使用zemax的经验，这次的优化就比较得心应手了。主要的设计参数可以通过几何光学算出个大概，只能说，要想学好光学设计，几何光学必须得滚瓜烂熟\n","slug":"光学设计实操（二）——-地上望远镜设计","date":"2024-12-07T06:16:36.000Z","categories_index":"Optic Design","tags_index":"Zemax","author_index":"Yesord"},{"id":"b951929798617cb5b56075fc7436b29b","title":"光学设计实操（一）——PeepHole门镜设计","content":"前言两周时间从零开始蹿出来的猫眼设计\n设计要求（1）设计防盗门上的猫眼（透镜直径为1cm），给出猫眼透镜组整体结构参数，并给出门内看门外和门外看门内两种工作模式下成像效果。（2）有不法分子携带“反猫眼”，用于门外投过猫眼看清室内状况，请尝试分析该反猫眼原理，并设计之。评分标准：性价比好（性能优异，价格有优势）；结构紧凑，功能齐全，符合用户需求。不能使用非球面。\n设计分析基础知识：（1）明视距离（25cm），近点距离（8.3cm）；（2）人眼明视距离看物体分辨率0.1-0.5mm\n\n\n\n\n\n\nTIP\n光路长度没有统一标准，但应该和常见防盗门匹配（厚度5cm、7cm、9cm、11cm…..）视场角假设猫眼距离地面1.5米，敲门人身高1.2-1.8米，则根据敲门人距离门距离不同，视场角需求变化例如：站门前30cm，则45度                站门前50cm，则30度                站门前60cm，则25度\n假设直接肉眼看人分辨率0.5 mm不影响结果，那么猫眼呈缩小像，不同猫眼放大率不同。放大率等于像距除以物距，因此如果只是看敲门的人，站在门前0.5 m，成像在明视距离0.25 m处，放大率为0.5。但考虑猫眼除了看敲门人以外，也有其他功能，例如听到门外有声响，透过看看门外发生了什么，假设看看两三米外的情况，则物体缩小近十倍（即放大率为0.1），看小的像对弥散控制要求更高，假设现在弥散0.05mm可以识别出人。那么传递函数需要参考空间频率20以内的情况\n\n\n截止频率：1&#x2F;(0.5*放大率）。总体来说对像差要求并不高。更直观的可以看“几何像分析”直接衡量成像效果\n功能正确正看：成像明视距离处，25度视场成像清晰反看：成像近点距离以内，且越近越好。\n猫眼设计评价顺序\n\n优先满足功能，即功能上是猫眼（只能单向用）\n其次满足尺寸，即能用，装的上\n再次保证性能：正看：给定视场角成像较清晰；反看：越近越好\n\n能用：（1）功能上是猫眼（单向工作）；（2）光路长度&gt;50 mm；（3）镜头参数符合标准；（4）镜头孔径1cm以内，特别是目镜；（5）视场角不能太低。好用：（1）考虑缩小率后确定截止频率内，像差矫正良好，传递函数曲线视场均一（或不同视场几何像清晰可见）。（2）视场应达到25度。（3）门外看门内像点越近越好，如靠近近点出现成像错误不能清晰获得像也可。\n对该应用场曲畸变不是很重要，不用看波像差曲线：最大波像差正负5个波长偏大点列图：色差大，且随视场角增大RMS值快速增大传递函数曲线：截止频率只有8左右，离预期20很远，且随视场增大曲线快速下降，同一视场子午弧矢曲线分离严重轴上（看1.5m身高的人）成像还行，但偏离则成像很差\n设计流程系统建模\n\n参考专利 CN 112859302 A\n\n\n\n\n\n\n\n此结构的合理性\n第一个镜片为弯月形的镜头，对于门内看门外可以有效得接收到大视场的光线，而对于门外看门内可以有效使光线发散，实际成像面非常靠近镜头，使门外无法看清门内中间两个镜片为对称式结构，对于门内看门外可以有效消除像差最后一个镜片为成像目镜，对于门内看门外使像最终成在明视距离处\n\n\n\n详细操作\n\n\n设定镜头初始参数\n\n\n\n2D Lay\n\n\n\n光圈随光阑尺寸浮动\n\n\n\n开启光线追踪\n\n\n\n100度大视场\n\n\n\n设置RGB可见光波长\n\n\n\n设置pick up保证对称式结构\n\n\n\n成像面(虚像)应在-250mm明视距离\n\n\n\n控制口半径5mm以下\n\n\n\n系统优化\n\n\n\n\n\n\n\n配置默认评价函数\n设置Spot Radius 可以有效优化像差，优先优化点列图和MTF,如果之后MTF基本优化成一束之后遇到瓶颈，可以重新改回wavefront设置玻璃的厚度限制 保证在优化过程中玻璃厚度不会优化成很抽象的东西\n\n\n\n\n赛德尔系数\n\n\n\n添加操作数\n\n\n\n\n\n\n\n\n优化流程\n首先默认函数直接优化，收敛到镜头参数相对合理的情况，再一点一点优化厚度，建议0.1mm慢慢调，一次动一个厚度通过观察赛德尔系数确定每个面对像差的贡献从而在操作数里面特别优化\n\n\n\n一些经验\n(叠个甲，这不一定在你的应用场景中是有效的)\n\n先保证视场光波长孔径这些参数正确\n先调两个镜片，调好后再加镜片，通常厚度距离都设个10，半径直接无穷\n后直接四个半径一起优化，优化出来如果慧差大，那就加个COMA的操作数（优化慧差）；后像散大，就加个ASTI操作数优化像散，但通常优化像散的同时，球差会增大；后对于MTF曲线，一般可能轴外视场的MTF曲线不好，可以用MTFT慢慢拉高（后面做烦了，就没怎么整这个）\n插入操作数后可以直接将全部曲面半径一起优化，这样变化稍微大点，对于thickness则初始值正常都是10来一个一个优化，（不要一起优化，不然必爆），初始值不能太离谱，优化完曲率半径之后，初始值基本就那边了\n优化完两个透镜之后，保存为一个文件，后可在两个透镜中加个双胶合透镜，但玻璃材料须为一个BK7,一个F2（一定不能都是BK7），接着可以选择（1）不动前面优化好的透镜，只优化双胶合，以及透镜间的距离（2）将全部曲率半径一起优化\n先优化双胶合的曲率，再优化双胶合的thickness\n优化像差先优化曲率半径，就是看点列图，优先把球差搞小，其实球差中间用对称式然后微调距离都可以变小，用spha操作数，搞小之后看慧差，用coma操作数优化，像散可以稍稍优化，用asti操作数，优化目标都是0，权重设成1就行\n优化距离就用totr操作数控制整体的距离，然后开锤空气，镜头厚度就手动慢慢调整，一次只调一个，调一次镜头厚度就锤目镜，mtf也可以通过mtfa操作数一点一点拉高\n\n\n\n优化结果门内看门外\n点列图、波像差曲线、畸变场曲曲线、调制传递函数曲线\n\n\n\n\n\n\n\n\n\n\n\n\n\n像分析\n\n\n可以看出成像的畸变比较大，且大市场的成像质量比较差，考虑到孔径大小的约束，这个结果也已经可以接受\n\n\n\n系统参数\n\n\n后焦距在-234mm即明视距离左右，说明成像位置还是优化的比较正确\n\n\n门外看门内\n\n\n\n5度视场无法通过，3度勉强通过\n\n\n点列图、调制传递函数曲线、像分析\n\n\n\n\n\n\n\n\n\n系统参数\n\n\n后焦距在-4mm说明门内的情况对门外的人来说是几乎没有可能看清的\n\n\n反猫眼反猫眼理应设置成变焦系统以供不同款式的猫眼使用，但是由于本次设计的工期较短，且本人才刚开始学习Zemax，所以能力有限，只设计出匹配我自己设计的猫眼的反猫眼镜头，没能在ddl前设计出变焦的反猫眼镜组，这里提供一个思路，去用三双胶合镜头为初始结构，使用Zemax的多重结构设计来控制焦距优化出镜头。\n\n\n\n\n\n\n\nWARNING\n在课堂上老师提出了从用户角度思考问题，对于使用反猫眼的人来说大多数都是小偷，对于他们来说使用一个变焦系统来看还是太麻烦了，他们大多的心态还是广撒网，对于一家来说只会凑上去看那么一下下，如果需要调焦可能也过于麻烦了。因此应该设计成成像大景深的镜组，通过多重结构去优化不同物距下的成像效果，最终实现在一定的物距范围内都能透过猫眼看清门内的情况。\n\n\n\n\n\n\n\n\n\n点列图、调制传递函数曲线\n\n\n\n\n\n\n\n\n像分析\n\n\n\n\n\n系统参数\n\n\n后焦距在大致明视距离处，说明反猫眼有效\n\n\n后记\n\n我这两周都不会忘记这张笑脸的>","slug":"光学设计实操-1-——PeepHole门镜设计","date":"2024-11-18T15:25:51.000Z","categories_index":"Optic Design","tags_index":"Zemax","author_index":"Yesord"},{"id":"d05ea03e60dbacb8a8e193106dfdc2a8","title":"光学设计资料整合","content":"前言本文主要记载我在入门光学设计所看到的有帮助的帖子\n帖子\n\n\n\n\n\n\n\n\n\nZEMAX光学设计—单透镜（a singlet）\nZEMAX光学设计实例（2）—双透镜（a doublet）\nZEMAX详解（5）—评价函数与操作数详解\nZemax光学设计实例（37）—变焦镜头\nZEMAX优化要点\nZEMAX像质评价及六种像差汇总\nZEMAX操作25–像差理论和修正（球差、慧差、像散）\nZEMAX主要优化函数表\n光学镜头参数详解（EFL、TTL、BFL、FFL、FBL&#x2F;FFL、FOV、F&#x2F;NO、RI、MTF、TV-Line、Flare&#x2F;Ghost）\nZemax光学设计实例（19）—凯涅尔目镜的设计\n目镜的光学和相差特性\n\n","slug":"光学设计资料整合","date":"2024-11-18T14:46:59.000Z","categories_index":"Optic Design","tags_index":"Zemax","author_index":"Yesord"},{"id":"2f2ae9cabd3d239359b1e3a9dc1dff4f","title":"图像拼接编程实践","content":"前言本项目是关于的图像拼接练习，我使用的编程语言是python，是笔者在校系统性学习机器学习的课程小项目，也不是啥新鲜玩意了，GitHub上很多复现的，感兴趣可以去GitHub上多找找。\n项目前置python版本要求\n\n\n\n\n\nTIP\npython&#x3D;3.x\n\n\n库依赖要求\n\n\n\n\n\nTIP\nopencv-pythonstitchingnumpy &lt;&#x3D; 1.26.4\n\n\n要求\n\n\n\n\n\n\n\n\n\n图像采集：使用相机或手机拍摄至少三幅图像。确保这些图像具有重叠部分（重叠区域建议为30%-50%）。建议选择一个具有丰富细节的场景，比如风景、建筑物或室内环境。\n图像预处理：使用图像处理软件（如OpenCV）读取和预处理这些图像；对图像进行畸变校正（如果必要），确保图像质量。\n特征检测与匹配：使用SIFT、ORB等特征检测算法提取图像中的关键点，应用匹配算法找到不同图像之间的特征匹配。\n拼接与合成：将图像进行变换和拼接，创建一幅全景图像。\n\n实现过程通过网上查找资料总共有三套多图像拼接的解决方案\n使用OpenCV stitcher类实现思路\n\nOpencv的Stitcher类的内部逻辑\n\n\n\n\n\n\n\n\n这张图展示了 OpenCV Stitcher 类中的图像拼接流程。\n首先，它从一组输入图像开始，将图像调整到中等分辨率，然后在图像中提取特征点并匹配这些特征点，以找到相邻图像之间的重叠区域。接着，程序对相机参数进行初步估计，并通过全局优化来确保图像的精确对齐。同时，还会进行波形校正和全景图的尺度估计，使图像在同一尺度上对齐。在合成阶段，图像经过透视变换后能够在同一平面上对齐，然后系统会自动补偿不同图像之间的曝光差异，计算并修正曝光误差。随后，程序会找到图像的接缝区域，并调整掩码的分辨率以确保无缝融合。最后，对齐后的图像进行融合处理，消除接缝区域的边界痕迹，最终输出一张完整的全景图。整个流程实现了从多张图像生成无缝全景图的功能直接使用stitcher类来生成拼接图像就很简单，速度也很快\n\n\n代码实现\n图像加载\npython\ndef load_images(image_path):\n    &quot;&quot;&quot;\n    加载指定路径下的所有图像\n    :param image_path: 图像文件夹路径\n    :return: 加载的图像列表\n    &quot;&quot;&quot;\n    logging.info(&quot;正在加载图像...&quot;)\n    # 获取所有以 .jpg 结尾的图像文件路径\n    image_files = [osp.join(image_path, file) for file in os.listdir(image_path) if file.endswith(&#39;.jpg&#39;)]\n    # 读取图像文件\n    images = [cv2.imread(file) for file in image_files]\n    # 检查是否有图像加载失败\n    if any(img is None for img in images):\n        raise ValueError(&quot;加载图像时出错，请检查图像路径和文件格式。&quot;)\n    return images\n\n\n\n拼接图像\npython\ndef stitch_images(images):\n    &quot;&quot;&quot;\n    拼接图像\n    :param images: 图像列表\n    :return: 拼接后的图像\n    &quot;&quot;&quot;\n    logging.info(&quot;正在拼接图像...&quot;)\n    # 创建拼接器对象\n    stitcher = cv2.createStitcher() if imutils.is_cv3() else cv2.Stitcher_create()\n    # 拼接图像\n    status, stitched = stitcher.stitch(images)\n    # 检查拼接状态\n    if status != 0:\n        raise RuntimeError(f&quot;图像拼接失败，错误代码：&#123;status&#125;&quot;)\n    return stitched\n\n\n\n裁切图片\npython\ndef crop_stitched_image(stitched):\n    &quot;&quot;&quot;\n    裁剪拼接后的图像，去除黑边\n    :param stitched: 拼接后的图像\n    :return: 裁剪后的图像\n    &quot;&quot;&quot;\n    logging.info(&quot;正在裁剪...&quot;)\n    # 在图像周围添加边框\n    stitched = cv2.copyMakeBorder(stitched, 2, 2, 2, 2, cv2.BORDER_CONSTANT, (0, 0, 0))\n    # 转换为灰度图像\n    gray = cv2.cvtColor(stitched, cv2.COLOR_BGR2GRAY)\n    # 二值化处理\n    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)[1]\n\n    # 查找轮廓\n    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    cnts = imutils.grab_contours(cnts)\n    # 获取最大轮廓\n    c = max(cnts, key=cv2.contourArea)\n\n    # 创建掩码\n    mask = np.zeros(thresh.shape, dtype=&quot;uint8&quot;)\n    x, y, w, h = cv2.boundingRect(c)\n    cv2.rectangle(mask, (x, y), (x + w, y + h), 255, -1)\n\n    minRect = mask.copy()\n    sub = mask.copy()\n\n    # 逐步腐蚀掩码，去除黑边\n    while cv2.countNonZero(sub) &gt; 0:\n        minRect = cv2.erode(minRect, None)\n        sub = cv2.subtract(minRect, thresh)\n\n    # 查找腐蚀后的轮廓\n    cnts = cv2.findContours(minRect.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    cnts = imutils.grab_contours(cnts)\n    c = max(cnts, key=cv2.contourArea)\n    x, y, w, h = cv2.boundingRect(c)\n\n    # 裁剪图像\n    stitched = stitched[y:y + h, x:x + w]\n    return stitched\n\n\n\nmain\npython\ndef main(args):\n    &quot;&quot;&quot;\n    主函数，执行图像拼接和保存\n    :param args: 命令行参数\n    &quot;&quot;&quot;\n    print(output_path)\n    try:\n        # 获取当前时间戳\n        timestamp = datetime.datetime.now().strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;)\n        # 加载图像\n        images = load_images(args.images)\n        # 拼接图像\n        stitched = stitch_images(images)\n        # 如果需要裁剪拼接图像\n        if args.crop:\n            stitched = crop_stitched_image(stitched)\n            # 确保输出文件路径包含有效的扩展名\n            valid_extensions = [&#39;.png&#39;, &#39;.jpg&#39;, &#39;.jpeg&#39;]\n            if not any(args.output.lower().endswith(ext) for ext in valid_extensions):\n                args.output = os.path.join(args.output, f&#39;stitched_&#123;timestamp&#125;.png&#39;)\n            print(args.output)\n        # 保存拼接后的图像\n        cv2.imwrite(args.output, stitched)\n        # 显示拼接后的图像\n        cv2.imshow(&quot;Stitched&quot;, stitched)\n        cv2.waitKey(0)\n    except Exception as e:\n        logging.error(e)\n\n\n使用stitching库实现思路这个库是我在github上闲逛找到的一个python图像拼接库stitching\n\n\n这也是受 OpenCV stitcher类启发并基于此写出的python包，原理上和openCV差不多。可以通过在conda环境中输入\nbashpip install stitching从PyPi上安装该库，通过研读stitching库的API，可以通过简单几行代码实现多图像的拼接\n代码实现\nonly a simple sample\npython\nfrom stitching import Stitcher\nimport os.path as osp\nimport os\nimport cv2\nimport datetime\nscript_path = osp.dirname(osp.abspath(__file__))\nproject_path = osp.dirname(script_path)\nimage_path = osp.join(project_path, &#39;images&#39;)\noutput_path = osp.join(project_path, &#39;output&#39;)\nimages = [cv2.imread(osp.join(image_path, file)) for file in os.listdir(image_path) if file.endswith(&#39;.jpg&#39;)]\n\nstitcher = Stitcher()\n\nstitcher = Stitcher(detector=&quot;sift&quot;, confidence_threshold=0.3)\ntimestamp = datetime.datetime.now().strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;)\n        \npanorama = stitcher.stitch(images)\ncv2.imwrite(output_path + f&#39;/stitched_&#123;timestamp&#125;.png&#39;, panorama)\nprint(&quot;Panorama save&quot;)\ncv2.imshow(&quot;Panorama&quot;, panorama)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n\n\n半手搓复现stitch调OpenCV的一些检测与计算算法\n实现思路\n\n复现的stitch类流程图\n\n\n\n\n\n\n\n\n我的复现流程分为3个步骤\n\n图像预处理：首先加载需要拼接的图像，通过 SIFT提取每张图像的特征点。使用FLANN进行特征点的匹配，找到相邻图像之间的匹配点。\n图像变换计算与拼接准备：利用匹配点计算单应性矩阵 H，应用 RANSAC 算法来筛选出最佳变换模型，将变换后的图像通过多张图像的融合拼接成一个整体，对拼接图像进行边缘处理，去除边界的黑边，使得拼接区域更平滑，调整图像的曝光以保持整体亮度的一致性\n图像融合：使用多频段融合消除图像交界处的拼接痕迹\n\n\n\n代码实现\n复现的关键点检测\npython\ndef detect_and_compute(self, img):\n    # 使用SIFT检测关键点并计算描述符\n    sift = cv2.SIFT_create(nfeatures=0, nOctaveLayers=3, contrastThreshold=0.04, edgeThreshold=10, sigma=1.6)\n    kp, des = sift.detectAndCompute(img, None)\n    return kp, des\n\n\n\n复现的关键点匹配\npython\ndef match_features(self, des1, des2):\n    # 使用FLANN匹配器匹配特征点描述符\n    FLANN_INDEX_KDTREE = 0\n    indexParams = dict(algorithm=FLANN_INDEX_KDTREE, trees=4)\n    searchParams = dict(checks=32)\n    flann = cv2.FlannBasedMatcher(indexParams, searchParams)\n    matches = flann.knnMatch(des1, des2, k=2) # k=2表示返回两个最近邻\n    # 应用Lowe&#39;s ratio test，筛选出好的匹配\n    good_matches = [m for m, n in matches if m.distance &lt; 0.6 * n.distance]\n    return good_matches\n\n\n\n复现的计算单应性矩阵与透视变换\npython\ndef find_homography(self, kp1, kp2, good_matches):\n    # 计算单应矩阵\n    src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n    M, mask = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 5.0)\n    return M\n\ndef warp_image(self, img, M, dsize):\n    # 应用透视变换，得到变换后的图像\n    return cv2.warpPerspective(img, M, dsize)\n\n\n\n复现的去除图像边缘黑边\npython\ndef remove_black_borders(self, img):\n    # 去除图像中的黑边\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    _, thresh = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    x, y, w, h = cv2.boundingRect(contours[0])\n    # 裁剪图像，去除黑边\n    return img[y:y+h, x:x+w]\n\n\n\n复现的图像重叠区域优化\npython# 采用重叠区域加权平均的算法，会有缺点\n# 可以加入曝光补偿来优化拼接处的色彩不连续\ndef optimize_stitching(self, warpImg, img1):\n    # 优化拼接，处理重叠区域\n    rows, cols = img1.shape[:2]\n    left = 0\n    right = cols\n\n    # 找到重叠区域的左边界\n    for col in range(0, cols):\n        if img1[:, col].any() and warpImg[:, col].any():\n            left = col\n            break\n\n    res = np.zeros([rows, cols, 3], np.uint8)\n\n    for row in range(0, rows):\n        for col in range(0, right):\n            if not img1[row, col].any():\n                # 如果img1在该像素为黑色，使用warpImg的像素\n                res[row, col] = warpImg[row, col]\n            elif not warpImg[row, col].any():\n                # 如果warpImg在该像素为黑色，使用img1的像素\n                res[row, col] = img1[row, col]\n            else:\n                # 对重叠区域进行加权平均\n                srcImgLen = float(abs(col - left))\n                testImgLen = float(abs(col - right))\n                alpha = srcImgLen / (srcImgLen + testImgLen)\n                res[row, col] = np.clip(img1[row, col] * (1 - alpha) + warpImg[row, col] * alpha, 0, 255)\n\n    warpImg[0:img1.shape[0], 0:img1.shape[1]] = res\n    return warpImg\n\n\n\n复现的多图像拼接\n\n\n\n\n\n\n\n\n\n我复现的多图像拼接的本质还是做双图像的拼接，好处是处理起来简单，坏处是遇到很多的图像的时候拼接效果会畸变会很严重导致效果很差，这其实也是0几年算法的缺陷，仅仅对一张图片做单应性矩阵的运算，然后只进行一次的透射变换，视差一大必然导致畸变放大，单次的透射变换复杂度不够描述一些关键点对应关系比较复杂的图像拼接，而且这种定下来一个主图其他图仿射变换到其上的拼接方式会导致很严重的误差累积，在一张一张分别处理拼接的时候。\npython\ndef stitch_images(self, img1_path, img2_path):\n    # 拼接两张图像\n    img1 = self.read_image(img1_path)\n    img2 = self.read_image(img2_path)\n\n    kp1, des1 = self.detect_and_compute(img1)\n    kp2, des2 = self.detect_and_compute(img2)\n\n    good_matches = self.match_features(des1, des2)\n\n    if len(good_matches) &gt; self.min_matches:\n        # 计算单应矩阵\n        M = self.find_homography(kp1, kp2, good_matches)\n        # 应用透视变换\n        warpImg = self.warp_image(img2, M, (img1.shape[1] + img2.shape[1],\n                                                max(img2.shape[0], img1.shape[0])))\n\n        # 优化拼接\n        optimize = self.optimize_stitching(warpImg, img1)\n        # 去除黑边\n        optimize = self.remove_black_borders(optimize)\n\n        # 保存并显示结果\n        cv2.imwrite(img1_path.split(&quot;.jpg&quot;)[0] + &quot;optimize.png&quot;, optimize)\n        print(&quot;result saved!&quot;)\n        #cv2.imshow(&quot;result&quot;, optimize)\n        #cv2.waitKey(0)\n        #cv2.destroyAllWindows()\n    else:\n        print(&quot;not enough matches!&quot;)\n\ndef stitch_multiple_images(self, image_paths):\n    # 拼接多张图像\n    if len(image_paths) &lt; 2:\n        print(&quot;需要至少两张图像进行拼接&quot;)\n        return\n\n    base_image_path = image_paths[0]\n    for next_image_path in image_paths[1:]:\n        # 逐步拼接图像\n        self.stitch_images(base_image_path, next_image_path)\n        # 更新基准图像为上一次的拼接结果\n        base_image_path = base_image_path.split(&quot;.jpg&quot;)[0] + &quot;optimize.png&quot;\n\n\n\n测试结果使用images_a数据集7张图片小视差数据集，每张图片之间的重叠区域在70%~80%左右，\nOpenCV stitcher实现\n\nOpenCV stitcher类底层是由C++实现的在加上算法优化可以保证5秒钟之内融合出图，速度特别快，但是由于其会使边缘变形较为严重，截成矩形图像后保存的拼接图看起来会比较短。\nstitching库实现\n\n生成速度也比较快，估计在7秒左右，图片的调教做的不错，边缘处理变形不大，可以保留大部分边缘信息完好，但是在多张图像重叠区域还是会出现一点模糊的情况。\n复现stitch\n\n自己写的拼接生成拼接图片的速度比较慢，在2022年的拯救者r9000p笔记本上2分钟左右才能跑出来，而且会产生鬼影，重叠区域处理采用最简单的加权处理就会产生这种问题。\n使用images_b数据集7张图片大视差数据集，每张图片之间的重叠区域在40%~50%左右由于视差比较大，三种方法出来的图畸变都很严重\nOpenCV stitcher实现\n\n由于严重的变形，前面的部分都被截掉了。\nstitching库实现\n\n整体最像一张全景图，但是中间的连接处不够平滑，有突变。\n复现stitch\n\n视差太大，我自己写的算法仅仅只是对一整个图像做单次的单应性矩阵的透视变换，仅仅做了线性变换每次都矫正的不够彻底，每次做变换都会导致误差累积，最后只融合了4张图就不够关键点匹配了。边缘的鬼影问题还是照旧。\n目前想到的优化方案\n\n\n\n\n\nTIP\n(1) 采用2013年CVPR的APAP算法的思想，采用网格分割，每个网格分别应用局部单应性矩阵，得到曲面的拼接图，但是这需要的算力会变大，运算时间会上升。\n\n\nAs-Projective-As-Possible Image Stitching with Moving DLT\n\n(2) 减少使用for循环多使用numpy，增加并行运算提高运算效率。\n\n\n心得与体会\n\n\n\n\n\n\n总结 \n通过这次图像拼接的课程作业，我深入学习并实践了图像拼接的核心流程，包括图像特征提取、匹配、变换计算以及图像融合等关键技术。在实现过程中，我使用了 OpenCV 的 Stitcher 类和 stitching 库，并尝试复现拼接算法，从中获得了许多收获。首先，通过使用 OpenCV 的 Stitcher 类，我体会到基于已有库的实现优势。这种方法速度快、效果较好，尤其是在小视差的场景下，可以高效地生成质量不错的全景图。Stitcher 类内部的优化算法让拼接过程变得简单，同时避免了许多细节上的困难。不过，OpenCV 拼接的边缘变形问题在一些场景下仍然较明显，需要进一步裁剪和调整。在尝试 stitching 库的过程中，我发现这类开源库通过 Python API 进行了更高层次的封装，使得代码实现更加便捷。尽管效果与 OpenCV 类似，但在多图像拼接时，该库在保留边缘细节方面表现稍好，尤其是对中等视差场景的拼接有一定优势。不过，拼接处可能会出现模糊的问题。最后，通过复现 Stitcher 类，我在算法的实现细节上加深了理解。通过手动编写特征提取、匹配、单应性计算和图像融合的过程，我掌握了拼接算法的关键步骤。然而，自实现算法的效率较低，尤其在大视差情况下，由于误差累积，拼接效果不够理想，甚至出现了鬼影和边缘重叠的现象。这一过程让我意识到精确控制参数、使用更高效的算法（如 APAP 局部单应性算法）和提升计算效率的重要性。\n\n\n\n\n\n\n\n\n\n\n未来展望\n由于iphone的相机软件也有产生全景图的功能，我希望之后能通过优化算法实现实时的图像拼接，然后再使用相机（或手机）实时采图，也做出这种效果。\n\n\n\n\n","slug":"图像拼接编程实践","date":"2024-11-13T14:12:35.000Z","categories_index":"MashineLearning","tags_index":"python","author_index":"Yesord"},{"id":"4bc11b96bc151d97b948e3471f93512a","title":"光学设计入门（一）——牛顿反射式望远镜设计","content":"前言我们使用的光学设计软件是Zemax，这是光学设计的入门软件，比较简单，优化参数相对CodeV没那么多。\n\n\n这是Zemax2005版\n\n现在的Zemax已经被Ansys收购了。\n\n\n\n设计流程设计一个1000 mm焦距 F&#x2F;5的牛顿望远镜物镜\n1000&#x2F;D&#x3D;5，所以物镜孔径D&#x3D;200mm。球面反射镜焦距近似是半径的一半，所以设计球面物镜的半径应该是2000mm\n1. 使用Lens data editor定义球面物镜。注意球面物镜的半径和厚度的符号。对凹面物镜，圆心在镜面左边，所以Radius是-2000，由于像面在物镜左边，所以thickness是-1000。详细表格如下图：\n\n\n2. 定义孔径D：选择zemax主菜单-&gt;系统-&gt;通用配置-&gt;aperture，在入瞳直径栏键入200，如下图：\n\n\n3. 查看光路图：从zemax主界面，分析-&gt;草图-&gt;D草图可以看到下图的光路图：\n\n\n4. 如果想更加形象化的看光路，也可从主菜单选择：分析-&gt;草图-&gt;渲染模型。可以使用键盘上下左右键来调整三维合适的观测角度，如下图：\n\n\n我们来看性能分析曲线：\n5. 先看点列图。从主菜单选择分析-&gt;点列图-&gt;标准选项，可以看到下图：\n\n\n从图中我们可以看到像差均方根（RMS）值为77.604。弥散斑直径约400微米。球差较大。这里我们只观测了550nm的波长。我们选择多波长，看是否有色差。\n6. 选择zemax主界面，系统-&gt;光波长菜单，可以看到下图：\n\n\n\n7. 如果我们要选择可见光的RGB三原色波长，我们可以按上图红圈所示的选择键，出现下图：\n\n\n这样我们观测波长就选择三个。正好是三原色波长。再点击确定，如上图圈所示键。\n8. 重复步骤5，可以看到下图：\n\n\n和步骤5图相比较，可以发现数值一样。证明反射镜没有引入色差。\n9. 看MTF曲线。在主界面里选择分析调制传递函数（MTF）快速傅里叶变换(FFT MTF)。可以看到下图，\n\n\n\n10. 如果我们想和没有像差，只存在衍射时的MTF曲线做比较，可点击设置按键，设置，如下图\n\n\n\n在上图所示选项打钩。再点下面的确定，就会看到在9步骤MTF曲线里出现一条黑线，如下图所示：\n\n\n黑色曲线就是没有像差影响，只存在凹面镜孔径引起的衍射作用时的MTF曲线。\n11. 注意，现在的结果是没有经过优化的，即使用几何光学中的公式R&#x3D;2f来定义凹面镜曲率半径和焦距的关系。下面我们将半径改设为变量，使用优化功能做的最佳的表面曲率半径，并和原结果做对比。回到透镜编辑表里：\n\n\n鼠标左键双击图中-2000部位，跳出如下对话框\n\n\n将图中菜单下拉，将求解类型由默认的“Fixed”改为第二项“variable”，如下图所示，并点击确定。\n回到透镜编辑表里，可以看到在半径后面列中出现一个“V”符号，代表当前已经将半径设为变量。\n12. 系统优化，自动搜寻最佳的变量Radius。过程为，回到主菜单，点击“编辑”菜单下的“优化函数”选项，跳出如下所示评价函数编辑表格：\n\n\n选择“工具”下“默认评价函数”，将跳出如下对话框\n\n\n该对话框用于对默认评价函数，即像差最小的搜索方法做编辑，这里我们先什么都不用改动，直接点击确定。回到评价函数编辑表里，可以看到表格变为下图：\n\n\n这就是系统根据“像差最小”的默认目标需求，自动生成的源代码。我们可以最小化或者直接关闭该表格，并回到主菜单：\n\n\n点击图中“工具”菜单下“优化”下第一个选项“优化”，跳出如下对话框：\n\n\n\n可见半径数值已由原来的“-2000”变为现在的“-2001.250106”。\n13. 下面我们检验优化后性能是否改进，按照步骤5，查看点列图，如下图所示\n\n\n\n\n\n14. 将球面改为抛物面。直接从透镜编辑里，在凹面镜conic的值置为-1。\n\n\n15. 重复步骤5，可以看到如下点列图：\n\n\n可以看到RMS值为0，没有球差、色差。\n16. 重复步骤9和10，可以看到如下MTF曲线，\n\n\n可以看到实际MTF曲线和没有像差只存在衍射时的MTF曲线完全重合。达到最佳值。\n17. 观察轴外成像情况，在主界面选择，系统-&gt;视场，跳出如下图所示对话框，\n\n\n键入0.75和1.5度两个轴外视场。再重复步骤5和9&#x2F;10，可以看到下面的图，\n\n\n\n从点列图中可以看到，虽然抛物线凹面镜没有球差、色差，但是还是有轴外高阶像差的，如彗差、像散等。\n\n\n\n可以看到虽然轴上MTF曲线非常完美，但是轴外信息传递能力快速恶化。\n18. 为了方便观察，必须加转镜，以便人观察时能不把物方光挡住。注意因为进来的光束为200mm宽，因此成像平面至少在离光轴100mm的上方，如此”看”像的时候才不会挡住入射光。我们决定用200mm，而fold mirror离先前的反射镜面为800mm，因为200+800&#x3D;1000等于原先在STO上的thickness，即成像”距离”不变。操作如下，先把STO的thickness改为-800。再在STO面后新插入一个平面，作为我们的反射镜，如下图所示，\n\n\n注意在Zemax里，如果出现多个反射镜时，一般出现奇数个反射面，当前面thichkness值为负，如果出现偶数个反射面时，当前面thickness为正。\n19. 现在我们将编号为“2”的表面设置为让光线转折90度的转镜。回到主界面，选择工具-&gt;折叠反射镜-&gt;添加折叠反射镜选项，弹出下图所示对话框，并编辑数据，\n\n\n\n我们想将“2”号表面定为转镜，所以我们就将上图圈内选项选为“2”即可。点确定。\n20. 看光路图，注意，一旦系统里有了转镜，就无法用2D草图显示。应从zemax主界面，分析-&gt;草图-&gt;3D草图\n\n\n或按照步骤4，看渲染图，如下\n\n\n注意到，镜头数据编辑变成下图，\n\n\n图中红色行只是用来坐标变换，为虚拟平面，不会在光路图里显示出来。\n21. 目前的系统还有一个重要问题，由于转镜与光轴呈45度夹角，因此入射光如果直接照射到转镜背面，可能会与镜筒多次反射干扰最终成像，引起大的照射。既然入射的光会被转镜挡住，无法参与成像。那么不如干脆在望远镜入口处设置障碍物把这部分光挡住。我们在STO面前插入一个平面，该面到凹面镜的距离为900mm。则镜头数据编辑表变为下图：\n\n\n22. 下面我们再学习将该表面设置为障碍物，双击上图红圈内容，在弹出的对话框里点击Aperture选项，如下图所示，\n\n\n\n如上图，将光圈类型下拉菜单选为“圆形挡光”，设置参数时选择Max Radius，即最大挡光半径为“63”。之所以这里值置为63，是因为步骤18中表格，转光镜最大半径为62.71。圆形挡光板应完全覆盖转光镜。最后点击确定，设置完毕。\n23. 现在来看光路渲染效果图，如下图所示\n\n\n\n24. 看现在的点列图和MTF曲线，如下\n\n\n可以看到，轴上无变化，轴外性能略有恶化。这是因为加入的挡光板后抛物线面中间不工作了，因此成像质量进一步恶化。MTF曲线如下，\n\n\n\n如图所示，MTF曲线相比先前也有恶化。主要表现为低频段下降。这是因为挡光板挡住了部分中低频分量的进入。但注意截止频率不变，因为加入反射镜不改变系统焦距，因此系统可以接收的最高空间频率分量未发生变化。\n后记光学设计的可调参数实在太多了，实际设计起来优化很是费劲，最近在做门镜（猫眼）设计，给我调麻了。\n","slug":"光学设计入门（一）——牛顿反射式望远镜设计","date":"2024-11-12T15:22:20.000Z","categories_index":"Optic Design","tags_index":"Zemax","author_index":"Yesord"},{"id":"3d0614aff1cb1047dc60965f7de9aa6f","title":"ComfyUI初体验--个性化二维码生成","content":"前言我在2022年就有在关注ai绘画领域，当时stable diffusion算刚刚出圈，现在flux都出来了，sd都更到SDXL了。ai绘画基本都使用WebUI作为GUI界面，虽然说WebUI也不是不好用，但就是比较传统。随着WebUI停止维护，许多的ai绘画工作者或爱好者都转到了ComfyUI的阵地，正好最近ComfyUI也更新了2.0版本的前端GUI，flux也说很牛，我上半年也搞了张4070super，种种机缘巧合之下，我又拾起了以前落下的AI绘画 （炼丹）开始玩。\n\nSD1代的上古遗骸–图生图，校徽拟人\n\n\n\n\n\n\n\nComfyUI\n\n\n\n\n\n\nComfyUI官网\nComfyUI官网\n\n\nComfyUI的下载安装网上教程业已详尽，我不过多赘述。\n\n\n\n\n\n\nTIP\n\n这里推荐一个入门视频\n不想动脑可以用秋叶大佬秋葉aaaki的ai绘画整合包\n\n\n\n我是自个上 ComfyUI GitHub库拉的release\n把release下载下来之后解压就可以看到\nwindows用户就可以直接运行如下的.bat批处理文件开启ComfyUI了\n\n如果电脑没Nvidia显卡就用第一个cpu版的，如果有就可以用第二个gpu版的。\n\n如果想通过命令行启动就要进到ComfyUI目录下找到如下的main.py执行（需要自己配环境，建议使用conda）\n\n\nbashpip install -r requirements.txt\npython main.py\n\n\n\n\n\n\nComfyUI使用体验\nComfyUI使用起来就像是画一个流程图，搞清楚你想要的生成图形的流程，用上ComfyUI的节点可以轻松完成很多的图形处理和生成工作。\n\n\n二维码生成思路参考的这篇文章Stable Diffusion QR Code 101\n总体流程参考二维码图像进行控制生成（图生图），使用prompt去控制生成的对象。\n二维码制作使用的是Anthony Fu大佬的QRCode制作器，这个二维码制作器可以很容易生成美观的二维码基模\n模型选取我这里只使用了controlnet和checkpoint这两个模型下载链接\ncheckpoint模型Checkpoint模型是真正的Stable Diffusion模型，包含了生成图像所需的所有内容，不需要任何额外的文件。我使用的是MeinaMix v10是基于Stable diffusion 1.5 训练出来的二次元风格特化的模型\n\n\n\n\n\n\n\n注意\n下载了checkpoint模型文件之后要把它放到ComfyUI的models&#x2F;checkpoint文件夹下\n\ncontrolnet模型ControlNet是一个控制预训练图像扩散模型（例如 Stable Diffusion）的神经网络。它允许输入调节图像，然后使用该调节图像来操控图像生成。能干的事情很多，想详细了解可以看这篇blog这两个网络是SD1.5的控制生成二维码的控制模型\n\n\n\n\n\n\n\n注意\n下载了controlnet模型文件之后要把它放到ComfyUI的models&#x2F;controlnet文件夹下\n\n搭建工作流参考工作流\n\n\n\n\n\n\n大概流程就是\n\n读取二维码图像，写prompt\n送入controlnet控制生成\n送入checkpoint第一次生成512x512的粗图\n读取人脸做精修\n生成图像超分放大\n\n\n\n\n生图结果展示\n\n\nshow more\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING\n刚把json文件导入ComfyUI可能会看到很多的节点缺失，ComfyUI是提供了一键安装的选项，只要直接安装等待安装结束重启ComfyUI即可使用。\n\n\n后记ComfyUI还挺好玩的吧\n\n嗨害嗨\n其实这篇blog的封面就是MeinaMix生成的哦\n\n\n","slug":"ComfyUI初体验-个性化二维码生成","date":"2024-11-05T14:59:50.000Z","categories_index":"AI art","tags_index":"Meinamix,ComfyUI","author_index":"Yesord"},{"id":"e7674ca755d44195295fe130df18e24b","title":"朴素贝叶斯-邮件分类编程实践","content":"前言本项目是基于朴素贝叶斯算法的侮辱性和非侮辱性邮件分类，使用的编程语言是python，是笔者在校系统性学习机器学习的课程小项目。\n项目前置python 版本要求\n\n\n\n\n\nTIP\npython&gt;&#x3D;3.10\n\n\n库依赖要求\n\n\n\n\n\n数据处理\nnumpy &lt;&#x3D; 1.26.4pandas\n\n\n\n\n\n\n\n\nGoogle API\ngoogle-auth-oauthlibgoogle-auth-httplib2google-api-python-client\n\n\n实现过程工程框架\n\n\n\n\n\n\n\nWARNING\nApp:存放用户代码，包括训练脚本train和测试脚本testdata:存放数据文件，包括数据集、OAuth登录令牌utils:存放工具集，包括gmail工具、数据准备工具、朴素贝叶斯类model:存放模型参数文件，贝叶斯模型参数\n\n本项目的工程文件已上传GitHub\n朴素贝叶斯类实现朴素贝叶斯算法基于贝叶斯定理，通过计算先验概率和后验概率来进行分类。在处理文本数据时，将邮件内容中的每个词视为特征，利用频率来估计条件概率。这种方法简单但有效，尤其适合文本分类任务。\n\n详细公式\n计算侮辱邮件的先验概率\n计算侮辱邮件条件下每个词的后验概率\n计算得分\n计算准确率\n\n训练训练步骤\n\n\n\n\n\nTIP\nSTEP1: 对分类器进行初始化，将每个类别的先验概率、存储每个类别下每个词的条件概率、词汇表、侮辱非侮辱类别标签进行初始化。STEP2: 遍历所有的邮件，将侮辱邮件的先验概率计算出来。STEP3: 根据邮件的词语使用，将词语记录到词库之中，并对每个词出现的频率进行统计得到训练集侮辱或非侮辱邮件条件下每个词出现的数量。STEP4: 得到每个词在侮辱或非侮辱邮件条件下的后验概率。STEP5: 所有邮件遍历完成之后保存模型参数。\n\n\n代码实现python\ndef train(self, training_set):\n        # 训练分类器\n        class_counts = &#123;&#125;  # 存储每个类别的文档数量\n        for doc, label in training_set:\n            if label not in class_counts:\n                class_counts[label] = 0\n            class_counts[label] += 1\n            for word in doc:\n                if word not in self.vocabulary:\n                    self.vocabulary[word] = &#123;&#39;spam&#39;: 0, &#39;ham&#39;: 0&#125;\n                if label == &#39;0&#39;:  # 假设 &#39;0&#39; 表示垃圾邮件\n                    self.vocabulary[word][&#39;spam&#39;] += 1\n                else:  # 假设 &#39;1&#39; 表示非垃圾邮件\n                    self.vocabulary[word][&#39;ham&#39;] += 1\n        self.class_labels = list(class_counts.keys())  # 获取所有类别标签\n        num_docs = len(training_set)  # 总文档数量\n        for label, count in class_counts.items():\n            self.class_priors[label] = count / num_docs  # 计算先验概率\n        # 计算后验概率\n        word_counts = &#123;label: &#123;&#125; for label in self.class_labels&#125;  # 初始化词频统计\n        for word, counts in self.vocabulary.items():\n            word_counts[&#39;0&#39;][word] = counts[&#39;spam&#39;]\n            word_counts[&#39;1&#39;][word] = counts[&#39;ham&#39;]\n        for label in self.class_labels:\n            total_count = sum(word_counts[label].values())  # 计算总词数\n            self.conditional_prob[label] = &#123;&#125;\n            for word in self.vocabulary:\n                count = word_counts[label].get(word, 0)\n                # 使用拉普拉斯平滑\n                self.conditional_prob[label][word] = (count + self.alpha) / (total_count + self.alpha * len(self.vocabulary))\n        self.save_model(model_path)\n        print(&quot;训练完成.&quot;)预测预测步骤\n\n\n\n\n\nTIP\nSTEP1: 获取训练得到的先验概率。STEP2: 遍历侮辱与非侮辱条件下的每个词，获得其后验概率，并与先验结合得到得分。STEP3: 根据得分结果，对邮件是否为侮辱邮件进行分类。\n\n代码实现pythondef predict(self, docs):\n        # 预测文档的类别\n        predictions = []\n        for doc in docs:     \n            scores = &#123;label: np.log(self.class_priors[label]) for label in self.class_labels&#125;\n            for label in self.class_labels:\n                for word in doc:\n                    if word in self.vocabulary:\n                        scores[label] += np.log(self.conditional_prob[label][word])\n                    else:\n                        # 对于没见过的词，使用拉普拉斯平滑\n                        scores[label] += np.log(self.alpha / (self.alpha * len(self.vocabulary)))\n            # 将对数概率转换为实际概率\n            max_log_score = max(scores.values())\n            exp_scores = &#123;label: np.exp(score - max_log_score) for label, score in scores.items()&#125;\n            total_exp_scores = sum(exp_scores.values())\n            probabilities = &#123;label: exp_score / total_exp_scores for label, exp_score in exp_scores.items()&#125;\n            prediction = max(probabilities, key=scores.get)  # 选择概率最大的类别作为预测结果\n            predictions.append(prediction)\n        return predictions评估评估步骤\n\n\n\n\n\nTIP\nSTEP1: 对测试集进行预测。STEP2: 计算准确率。\n\n代码实现pythondef evaluate(self, test_set):\n        # 评估分类器的性能\n        predictions = self.predict([doc for doc, label in test_set])  # 对测试集进行预测\n        correct = np.sum([label == pred for (doc, label), pred in zip(test_set, predictions)])  # 计算正确预测的数量\n        return correct / len(test_set)  # 返回准确率数据处理工具实现在处理邮件数据时，数据的清洗和预处理至关重要。通过将邮件内容拆分为标签和文本，并进一步拆分为词，能够有效地构建训练集和测试集，为后续的模型训练提供基础\n\n\n\n\n\n\n\nWARNING\n我们使用的数据集：训练集为spam_train.txt，包含5000封邮件（txt 5000行）；测试集为spam_test.txt（txt 1000行），包含1000封邮件，其中侮辱类邮件行首用0标识，非侮辱类邮件行首用1标识。\n\n\n首先，根据提供的数据集格式为.txt，同时可以得到数据集内部数据的格式为label为0表示侮辱邮件，label为1表示非侮辱邮件，Text每个词之间以空格隔开。\n处理步骤因此处理步骤可分为以下4步\n\n\n\n\n\n\nTIP\nSTEP1: 读取数据集的.txt的每一行STEP2: 检索第一个空格，将label和Text拆分，label存入标签列表里。STEP3: 遍历剩余的每个空格，将Text里的词进行拆分，将词存入数据列表里。STEP4: 遍历完每一行之后输出标签列表和数据列表。\n\n注：标签列表存的是数据集每封邮件是否是侮辱邮件的标签，数据列表存的是数据集每封邮件的内容，是以每邮件每词进行拆分的二维列表。\n代码实现python\ndef load_data():\n    try:\n        train_labels, train_data = _split_data_form_txt(train_data_path)\n        test_labels, test_data = _split_data_form_txt(test_data_path)\n    except FileNotFoundError:\n        print(&quot;The data files are not found. Please check the data directory.&quot;)\n        return None, None, None, None\n    return train_labels, train_data, test_labels, test_data\n\n\ndef _split_data_form_txt(data_path):\n    labels = []\n    data = []\n    with open(data_path, &#39;r&#39;, encoding=&#39;utf-8&#39;) as file: # 读取数据\n        lines = file.readlines()\n    for line in lines:\n        split_line = line.strip().split(&quot; &quot;, 1)\n        labels.append(split_line[0])\n        data.append(split_line[1].split())\n    return labels, dataGmail工具实现在访问 Gmail API 时，由于需要设置 OAuth2.0 认证以及代理的使用，我遇到了一些配置上的困难。通过查阅文档和进行调试，最终成功实现了邮件的读取和分类\n\n实现思路首先在Google Cloud上创建项目\n\nDetails\n\n\n并开启Gmail API的调用权限与服务\n\nDetails\n\n\n\n将OAuth2.0客户端ID密钥等详细信息保存到本地pickle文件，并通过这些信息连接到Google Cloud上的项目，开始调用Gmail API，对Gmail邮件进行读取等操作。\n\n客户端打开如图\n\n\n\n部分代码pythonimport pickle\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom google.auth.transport.requests import Request\nfrom email.mime.text import MIMEText\n #如果修改了这些作用域，删除token.pickle文件。\nSCOPES = [&#39;https://www.googleapis.com/auth/gmail.modify&#39;]\n\n# 设置代理 需要设置不然中国大陆无法访问google邮箱\nos.environ[&#39;HTTP_PROXY&#39;] = &#39;http://127.0.0.1:7890&#39;\nos.environ[&#39;HTTPS_PROXY&#39;] = &#39;http://127.0.0.1:7890&#39;\n\ndef get_gmail_service():\n    creds = None\n    # token.pickle文件存储了用户的访问和刷新令牌\n    if os.path.exists(token_path):\n        with open(token_path, &#39;rb&#39;) as token:\n            creds = pickle.load(token)\n            print(&#39;Loaded token&#39;)\n    # 如果没有有效的凭证，让用户登录\n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            creds.refresh(Request())\n            print(&#39;Refreshed token&#39;)\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file(\n                json_path, SCOPES)\n            creds = flow.run_local_server(port=12345)\n            print(&#39;New token&#39;)\n        # 保存凭证以供下次运行使用\n        with open(token_path, &#39;wb&#39;) as token:\n            pickle.dump(creds, token)\n\ndef read_messages(service, user_id=&#39;me&#39;, label_ids=[&#39;INBOX&#39;], max_results=10, is_print=False):\n    try:\n        msg_list = []\n        messages = service.users().messages().list(userId=user_id, \n                                                   labelIds=label_ids, \n                                                   maxResults=max_results).execute()\n        \n        for message in messages[&#39;messages&#39;]:\n            msg = service.users().messages().get(userId=user_id, id=message[&#39;id&#39;]).execute()\n            if is_print:\n                print(f&quot;Subject: &#123;_get_subject(msg)&#125;&quot;)\n                print(f&quot;From: &#123;_get_sender(msg)&#125;&quot;)\n                print(f&quot;Snippet: &#123;msg[&#39;snippet&#39;]&#125;\\n&quot;)\n            msg_list.append(msg)\n        return msg_list\n    except Exception as error:\n        print(f&#39;An error occurred: &#123;error&#125;&#39;)\n\ndef send_message(service, user_id, subject, body, to):\n    message = MIMEText(body)\n    message[&#39;to&#39;] = to\n    message[&#39;subject&#39;] = subject\n    raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode(&#39;utf-8&#39;)\n    \n    try:\n        message = service.users().messages().send(userId=user_id, body=&#123;&#39;raw&#39;: raw_message&#125;).execute()\n        print(f&quot;Message Id: &#123;message[&#39;id&#39;]&#125;&quot;)\n        return message\n    except Exception as error:\n        print(f&#39;An error occurred: &#123;error&#125;&#39;)\n        return None测试结果测试集准确率运行train.py脚本可以得到测试集的准确率为 97.4% 。说明该朴素贝叶斯分类器在测试集上的分类表现良好，能够正确分类大多数测试样本。\n实际邮件的预测效果手动输入文本\n\nDetails\n\n\n\n先验概率是从模型计算分数并进行归一化得到的。该朴素贝叶斯分类器能简单的根据邮件的内容（例如“love”“stupid”之类的词）进行侮辱邮件的分类。简单邮件的分类表现不错。\n从Gmail获取最近的一封邮件\nDetails\n\n\n运行test.py脚本能有效地读出测试gmail邮箱中邮件的信息并进行是否为侮辱邮件的简单判断。但是实际效果还是不太好，可能是由于数据集数据量或数据质量受限。\n心得与体会总结\n\n\n\n\n\n\nWARNING\n在本次项目中，我深入学习和应用了朴素贝叶斯算法进行邮件分类，主要针对侮辱性和非侮辱性邮件的识别。这一过程让我对贝叶斯的基本原理有了更深刻的理解，尤其是在文本分类方面的应用。在未来的学习中，我希望能继续深化对机器学习算法的理解，并尝试更复杂的模型和数据集。\n\n\n改进方向\n\n\n\n\n\n\nWARNING\n在实际应用中，数据集的质量直接影响到模型的效果。虽然使用了5000封邮件进行训练，但数据的多样性和标签的准确性仍需要关注。在特征提取方面，可以使用一些技巧去提取关键词，而不是将所有词都作为词库，这样在关键词上的处理可以提高模型的表现。在之后的学习可以学到更多的机器学习算法，可以尝试使用它们来对比朴素贝叶斯算法的效果，看看是否能进一步提高分类准确率。\n\n","slug":"朴素贝叶斯-邮件分类编程实践","date":"2024-10-06T14:34:15.000Z","categories_index":"MashineLearning","tags_index":"python","author_index":"Yesord"},{"id":"5ba4847e568bd703f1becbdbb108530a","title":"python环境搭建","content":"前言作为工科生，环境搭建是必不可少的一种能力，学会环境搭建是小白步向菜鸟的第一步。python是一门相对较新的语言，我记得在我读中学的时候这个时候python还没出3，当时懵懵懂懂的，听网上的言论，感觉python是一个和basic、scratch一样的新手入门级编程语言，但实际上python是一个很好用的编程语言，突出一个生态多库多能实现的功能很多涉及的领域也很多，像是网络爬虫、神经网络、软件自动化甚至是硬件操控都可以，感兴趣的话可以去多多了解一下，话不多说，我们准备开始了。\n\n方案\n\n\n\n\n\n\n方案一：Anaconda + Visual Studio Code\nAnaconda是一个python多功能工具箱\n就像手机的应用商店，Anaconda提供了一个集成的环境，可以轻松安装、更新和管理各种Python库和工具。你只需通过简单的命令，就能获取你需要的应用（库），让开发和分析变得简单。\nVisual Studio Code是一个代码编辑器（之后简称Vscode）\nVSCode就像一把瑞士军刀，集成了多种工具和功能。无论你需要剪刀、螺丝刀还是开瓶器，VSCode都能满足你的需求。它支持多种编程语言、调试工具和插件，让你在同一个环境中完成各种任务。\n\n\n方案二pycharm之后有机会再做吧\nAnaconda配置windows端Anaconda安装\n\n\n\n\n\nAnaconda官网\nAnaconda官网\n\n\nSTEP1 点击这个 skip registration 跳过注册\n\nSTEP2 进入下载界面点击Downlaod\n\n\n\n\n\n\n\n\n注意\n\nAnaconda占用的存储空间比较大，尽量不要下到C盘，如果分了很多个盘的话。不然你的C盘很快就要红温了。\n需要记住Anaconda的安装路径，之后步骤需要使用\n\n\n\nSTEP3 正常下载完成会让你打开Anaconda Navigator，暂时不用管它，右上角关闭，我们这个教程不会使用到，感兴趣我之后可以再做。\n\n至此Anaconda下载完毕！\n环境变量配置安装完毕后我们要开始配置环境变量了\n\n\n\n\n\n\n环境变量是个啥\n当你在厨房做饭时，你需要各种调料（盐、糖、酱油等）。这些调料放在一个固定的地方（调料架）上，方便你随时取用。环境变量就像是这个调料架，存放着各种系统和应用程序需要的配置信息，方便程序随时读取。\n\n\nSTEP1 找到你windows的开始键，按下进入搜索，搜索环境变量，点击“编辑系统环境变量”\n\nSTEP2 先按下“path”，再点击“编辑”，进入配置环境变量路径的列表\n\n\n\n\n\n\n\nTIP\n配置环境变量路径的列表如下图所示\n\n双击已有的就可以修改已有的环境变量，双击空白列表处就可以添加新的环境变量\n\n\nSTEP3 需要去Anaconda的安装路径下找到图中的三个文件夹，并将它们加入环境变量路径列表中\n\n至此环境变量配置完毕！\n创建环境STEP1 打开Anaconda Prompt\n\n\n\n\n\n\n\n打开的命令行界面如图所示\n\n(base) 代表的是你当前所处的虚拟环境，base是基础虚拟环境，是下载anaconda后的默认的虚拟环境。\n\n\nSTEP2 创建环境\n在Prompt命令行界面输入如下命令\nbashconda create -n Env_Name python=3.9 \n\n\n\n\n\n\nconda create 参数解析\n-n 代表取名 Env_Name 可以换成任意你喜欢的名字，但是得是英文哦python&#x3D;3.9 表示该环境预装python3.9\n\n\n之后命令行打印这个界面按下y就会继续安装\n\n命令行打印这些信息就是安装成功啦\n\nSTEP3 启动环境\n在Prompt命令行界面输入如下命令\nbashconda activate Env_Name #注意替换Env_Name忘了自己刚刚取的环境名字可以输入以下命令查看\nbashconda env list #查看Anaconda的所有环境当前路径前面的(base)变成了(Env_Name)就是成功启动环境了\n\n\n\n\n\n\n\nconda activate &amp; conda deactivate\nconda activate 表示激活环境 后面接你的环境名conda deactivate 表示退出环境 后面接你的环境名\n\n\n至此创建环境完毕！\nMac端Anaconda安装\n\n\n\n\n\nAnaconda官网\nAnaconda官网\n\n\nSTEP1 去Anaconda官网下载注意你的Mac是M系列芯片还是Intel芯片\n\nSTEP2 “Download”里打开安装包\n\nSTEP3 一路点击继续，等待安装结束\n\n\nClick to see more\n整个安装流程是往你的用户路径下的.zshrc和.bash_profile文件里写conda的环境变量，可以通过以下命令去终端查看究竟往这两个用户配置文件里面写了什么。\nbashnano ~/.zshrc # 查看.zshrc的内容\nnano ~/.bash_profile # 查看.bash_profile的内容环境变量大概如图所示\n\n所以使用安装包安装anaconda不需要自己配置anaconda的环境变量，但如果以后要移动anaconda文件夹的位置就需要去重写环境变量\n\n\nSTEP3 打开你的终端，如果工作目录前面出现(base)即是安装成功\n\n至此Anaconda安装完毕！\n创建环境操作与windows端一致，在终端进行即可。\nLinux端使用 Linux 操作系统一般要熟悉命令行操作，如果你用的是有 desktop 的 Linux 请按下你键盘上的  Ctrl + Alt + T 打开终端，咱们不要当用可视化界面的懦夫，要成为用命令行的强者，在不懂计算机的同学面前秀上一把。\n下载 miniconda一般对于我们不搞服务器的、不做大数据计算的, 用 Linux 大多是为了 嵌入式Linux，这边就建议下载一个轻量化的 Anaconda —— miniconda ，不然你那小小的 32G 身板可顶不住 anaconda 的存储空间冲击。\n在命令行中输入下面的命令，下载 miniconda 安装的 shell 脚本\nbashwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n阅读并接受许可协议，选择安装位置，设置环境变量等等…自个用不用管这些。默认情况的话：阅读协议，按空格跳到最下面，输入yes；\nbashDo you accept the license terms? [yes|no]\n\nyes\n默认安装位置，一直按回车；\nbash installation finished. Do you wish the installer to initialize Miniconda3 by running conda init?\n [yes|no] [no] &gt;&gt;&gt; yes接着要 source 一下 .bashrc 应用环境变量激活 conda，重启也行（如果你愿意的话）\nbashsource ~/.bashrc激活后可以看到启动了 base 环境，此时conda 命令也可以用了。\n查看 conda 版本，确认是装上了哈\nbashconda --version至此Anaconda安装完毕！\nVscode配置\nClick to see more\nVscode在Mac、Windows、Linux下都差不多我就不分着写了。\n\nVscode下载\n\n\n\n\n\nVscode官网\nVscode官网\n\n\n扩展推荐\nClick to see more\n\npython相关扩展没了你别想用Vscode开发python程序\nC&#x2F;C++相关扩展没了你别想用Vscode开发C&#x2F;C++程序\nChinese好好想想为什么你需要这个插件\n改变图标真看不惯vscode的文件图标显示吧\n代码补全代码补完计划\n书签给你的bug留个存档点\n\n\n\n尝试HelloworldSTEP1 创建Helloworld.py文件，并在工作区打开\n\nSTEP2 Vscode右下角点击“选择python解释器”\n\nSTEP3 选择你在Anaconda中创建的虚拟环境\n\nSTEP4 Hello,World!在Helloworld.py文件中添加如下代码\npythonprint(&quot;Hello, World!&quot;) # 你好，世界！至此Vscode配置完毕！\n之后这个工作区的python开发就是使用的Anaconda里你创建的虚拟环境安装库什么的就可以在Vscode的终端里进行安装了\n\n\n\n\n\n\n\n提一嘴\npython库安装\n使用pip工具在终端进行库安装（以安装numpy为例）\nbashpip install numpy千万要注意你是在哪个环境进行安装的，注意前头的 ( )\n\n\n后记恭喜你，已经成功从环境搭建小白进化成菜鸟了！\n\n","slug":"python环境搭建","date":"2024-09-09T14:55:02.000Z","categories_index":"","tags_index":"python","author_index":"Yesord"},{"id":"46d63a6191ce7c7d772a0f14338f0d34","title":"三子棋对弈装置--视觉部分","content":"前言本篇blog是本人在打2024电赛E题学习到的相关知识的整理，由于我们选择的方案是深度数据和RGB数据融合的方法来判断棋子的位置。不同于大多数人选择的openmv作为视觉传感，所以我觉得在实现思路上会相对不那么随大流一点，当然我也会提供我自己个人的传统RGB 2DV的思路。在此，我仅会提供部分代码以供参考。\n\n自我吐槽\n由于当时全心全意在攒这个代码，没去做照片的记录，所以以下很多的代码没有效果图的作证会有点难受，现在真的有点想穿越到那个时候给自己来一巴掌\n\n\n硬件组成\n\n\n\n\n\n \n\nprocessor: jetson nano 4G\ni-TOF camera: orbbec Femto Bolt\n\n\n\n我们使用 jetson nano 作为处理器，读取深度相机 Femto Bolt 的视频流并作相应的图像处理，最终将识别出的棋子和棋盘坐标提取出来，通过串口传到下位机MCU进行控制。\n\n\n软件部分在 jetson nano 上运行的程序\n数据流获取应用奥比中光的orbbecSDK去采深度相机的数据。orbbecSDK的环境配置不过多赘述，不是本篇的重点，如果读者感兴趣，我未来或许会再做。\n不同厂商的相机SDK肯定是不一样的，这里仅作奥比中光相机通过orbbecSDK的数据流获取思路和代码实现(奥比中光相机的SDK和微软realsense相机的SDK是比较相像的应该可以有所借鉴)\n获取思路\n\n\n\n\n\n\nWARNING\n获取视频流管道类Pipeline和视频流配置类Config。获取对RGB和Depth流的配置并使能，主要可以调帧率和分辨率，如果有需要做RGBD对齐这里也可以做。将管道的数据输出成Frameset类的帧格式，Frameset类有很多帧相关的方法。将每一帧Frameset帧数据给转换成numpy数组便可以通过opencv再做处理。\n\n代码实现python\nfrom pyorbbecsdk import Config \nfrom pyorbbecsdk import OBError\nfrom pyorbbecsdk import OBSensorType, OBFormat\nfrom pyorbbecsdk import Pipeline, FrameSet\nfrom pyorbbecsdk import VideoStreamProfile\nfrom utils import frame_to_bgr_image\nfrom pyorbbecsdk import *\n\n#相机初始化\ndef camera_init():\n    pipeline = Pipeline()\n    config = Config()\n    return pipeline, config\n\n# 配置RGB流\ndef config_color_stream(pipeline,config):\n    \n    try:\n        profile_list = pipeline.get_stream_profile_list(OBSensorType.COLOR_SENSOR)\n        try:\n            color_profile: VideoStreamProfile = profile_list.get_stream_profile_by_index(1)\n        except OBError as e:\n            print(e)\n            color_profile = profile_list.get_default_video_stream_profile()\n            print(&quot;color profile: &quot;, color_profile)\n        config.enable_stream(color_profile)\n    except Exception as e:\n        print(e)\n        return config\n\n# 配置深度流\ndef config_depth_stream(pipeline,config):\n    try:\n        profile_list = pipeline.get_stream_profile_list(OBSensorType.DEPTH_SENSOR)\n        try:\n            depth_profile = profile_list.get_stream_profile_by_index(3)\n        except OBError as e:\n            print(e)\n            depth_profile = profile_list.get_default_video_stream_profile()\n            print(&quot;depth profile: &quot;, depth_profile)\n        config.enable_stream(depth_profile)\n    except Exception as e:\n        print(e)\n        return config\n\n# 配置RGBD对齐\ndef config_color_depth_align(pipeline,config):\n    pipeline.enable_frame_sync()\n    config.set_align_mode(OBAlignMode.SW_MODE)\n\n# 读颜色深度帧\ndef read_color_depth_frame(pipeline):\n    frame = pipeline.wait_for_frames(100)\n    if frame is None:\n        return None, None\n    color_frame = frame.get_color_frame()\n    if color_frame is None:\n        return None, None\n    depth_frame = frame.get_depth_frame()\n    if depth_frame is None:\n        return None, None\n    \n    width = depth_frame.get_width()\n    height = depth_frame.get_height()\n    scale = depth_frame.get_depth_scale()\n\n    depth_data = np.frombuffer(depth_frame.get_data(), dtype=np.uint16)\n    depth_data = depth_data.reshape((height, width))\n\n    depth_data = depth_data.astype(np.float32) * scale\n    depth_data = np.where((depth_data &gt; MIN_DEPTH) &amp; (depth_data &lt; MAX_DEPTH), depth_data, 0)\n    depth_data = depth_data.astype(np.uint16)\n\n    depth_image = cv2.normalize(depth_data, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n    depth_image = cv2.applyColorMap(depth_image, cv2.COLORMAP_JET)\n    color_image = frame_to_bgr_image(color_frame)\n    print(depth_image.shape,color_image.shape)\n    return color_image, depth_image\n\n# 读颜色帧\ndef read_color_frame(pipeline):\n    frame = pipeline.wait_for_frames(100)\n    if frame is None:\n        return None\n    color_frame = frame.get_color_frame()\n    if color_frame is None:\n        return None\n    color_image = frame_to_bgr_image(color_frame)\n    return color_image\n\n# 读深度帧\ndef read_depth_frame(pipeline):\n    frame = pipeline.wait_for_frames(100)\n    if frame is None:\n        return None\n    depth_frame = frame.get_depth_frame()\n    if depth_frame is None:\n        return None\n    \n    width = depth_frame.get_width()\n    height = depth_frame.get_height()\n    scale = depth_frame.get_depth_scale()\n\n    depth_data = np.frombuffer(depth_frame.get_data(), dtype=np.uint16)\n    depth_data = depth_data.reshape((height, width))\n\n    depth_data = depth_data.astype(np.float32) * scale\n    depth_data = np.where((depth_data &gt; MIN_DEPTH) &amp; (depth_data &lt; MAX_DEPTH), depth_data, 0)\n    depth_data = depth_data.astype(np.uint16)\n\n# 读深度颜色帧\ndef read_color_depth_frame(pipeline):\n    frame = pipeline.wait_for_frames(100)\n    if frame is None:\n        return None, None\n    color_frame = frame.get_color_frame()\n    if color_frame is None:\n        return None, None\n    depth_frame = frame.get_depth_frame()\n    if depth_frame is None:\n        return None, None\n    \n    width = depth_frame.get_width()\n    height = depth_frame.get_height()\n    scale = depth_frame.get_depth_scale()\n\n    depth_data = np.frombuffer(depth_frame.get_data(), dtype=np.uint16)\n    depth_data = depth_data.reshape((height, width))\n\n    depth_data = depth_data.astype(np.float32) * scale\n    depth_data = np.where((depth_data &gt; MIN_DEPTH) &amp; (depth_data &lt; MAX_DEPTH), depth_data, 0)\n    depth_data = depth_data.astype(np.uint16)\n    color_image = frame_to_bgr_image(color_frame)\n    return color_image,depth_data相机标定\nWhat is the camera celibration ?\n相机标定（Camera Calibration）是计算机视觉中的一个重要步骤，旨在确定相机的内参和外参。内参包括焦距、光学中心和畸变系数，外参包括相机在世界坐标系中的位置和方向。通过相机标定，可以将二维图像中的点映射到三维空间中的点，从而实现精确的测量和重建。\n相机标定的主要步骤包括：\n\n拍摄标定图像：通常使用棋盘格图案，拍摄多张不同角度的图像。\n检测角点：在每张图像中检测棋盘格的角点。\n计算内参和外参：使用检测到的角点和已知的棋盘格尺寸，通过算法（如张正友标定法）计算相机的内参和外参。\n校正图像：使用计算出的参数校正图像中的畸变。:::\n\n标定思路:::warning我们采用的是固定相机机位的方法，如果想要将我们的运动机构和相机传感做联系是要做手眼标定的。因为我们使用的运动机构是龙门架，由于龙门架自身的特性，我们只需要将相机标定到我们的棋盘平面即可得到比较精准的结果，龙门架的初始点只需要做简单的平移就可以做到与相机标定平面的对应。我们使用张正友标定法将相机标定到棋盘平面之上，使用 11x8 grid size 20mm的棋盘格标定板进行标定。\n对张正友标定法感兴趣的读者可以去看这篇论文《A Flexible New Technique for Camera Calibration》\n\n这里提供一个脚本做相机标定\n\nA script for camera celibration.\npython\nimport cv2\nimport numpy as np\nimport os\n\nscript_dir = os.path.dirname(os.path.abspath(__file__)) #&lt;-- absolute dir the script is in\n# 图片在当前文件夹的位置\nfile_in = os.path.join(script_dir,&#39;p1&#39;)   # 原始图片存放位置\nfile_out = os.path.join(script_dir,&#39;p3&#39;)   # 最后图片的保存位置\n\n# 棋盘格模板规格，只算内角点个数，不算最外面的一圈点\nw = 11\nh = 8\n\n# 没考虑棋盘格的大小，只考虑角点的数量，因此计算出来的平移量要乘上棋盘格的大小\n\n# 找棋盘格角点\n# 世界坐标系中的棋盘格点，在张正友标定法中认为Z = 0\n# mgrid创建了大小为8×5×2的三维矩阵，在reshape成二维以后赋给objp，objp最后为(0,0,0), (1,0,0), (2,0,0) ....,(8,5,0)\nobjp = np.zeros((w * h, 3), np.float32)   # 大小为wh×3的0矩阵\nobjp[:, :2] = np.mgrid[0:w, 0:h].T.reshape(-1, 2)   # :2是因为认为Z=0\nobjpoints = []  # 储存在世界坐标系中的三维点\nimgpoints = []  # 储存在图像平面的二维点\n\nimages = os.listdir(file_in)   # 读入图像序列\ni = 0\nimg_h = 0\nimg_w = 0\n\n# 算法迭代的终止条件，第一项表示迭代次数达到最大次数时停止迭代，第二项表示角点位置变化的最小值已经达到最小时停止迭代\ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\nfor fname in images:\n    img = cv2.imread(file_in + &#39;/&#39; + fname)\n    img_h = np.size(img, 0)\n    img_w = np.size(img, 1)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   # RGB转灰度\n    # 找到棋盘格角点，存放角点于corners，如果找到足够点对，将其存储起来，ret为非零值\n    ret, corners = cv2.findChessboardCorners(gray, (w, h), None)\n    # 检测到角点后，进行亚像素级别角点检测，更新角点\n    if ret == True:\n        i += 1\n        # 输入图像gray；角点初始坐标corners；搜索窗口为2*winsize+1；表示窗口的最小（-1.-1）表示忽略；求角点的迭代终止条件\n        cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)\n        objpoints.append(objp)   # 空间坐标\n        imgpoints.append(corners)  # 角点坐标即图像坐标\n        # 角点显示\n        cv2.drawChessboardCorners(img, (w, h), corners, ret)\n        cv2.imshow(&#39;findCorners&#39;, img)\n        cv2.imwrite(file_out + &#39;/print_corners&#39; + str(i) + &#39;.jpg&#39;, img)\n        cv2.waitKey(0)\ncv2.destroyAllWindows()\n\n&quot;&quot;&quot;\n求解参数\n输入：世界坐标系里的位置；像素坐标；图像的像素尺寸大小；\n输出：\nret: 重投影误差；\nmtx：内参矩阵；\ndist：畸变系数；\nrvecs：旋转向量 （外参数）；\ntvecs：平移向量 （外参数）；\n&quot;&quot;&quot;\nret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n\nprint((&quot;ret（重投影误差）:&quot;), ret)\nprint((&quot;mtx（内参矩阵）:\\n&quot;), mtx)\nprint((&quot;dist（畸变参数）:\\n&quot;), dist)  # 5个畸变参数，(k_1,k_2,p_1,p_2,k_3)\nprint((&quot;rvecs（旋转向量）:\\n&quot;), rvecs)\nprint((&quot;tvecs（平移向量）:\\n&quot;), tvecs)\n\n\n# 优化内参数和畸变系数\n# 使用相机内参mtx和畸变系数dist，并使用cv.getOptimalNewCameraMatrix()\n# 通过设定自由自由比例因子alpha。\n# 当alpha设为0的时候，将会返回一个剪裁过的将去畸变后不想要的像素去掉的内参数和畸变系数；\n# 当alpha设为1的时候，将会返回一系个包含额外黑色像素点的内参数和畸变数，并返回一个ROI用于将其剪裁掉。\nnewcameramtx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (img_w, img_h), 0, (img_w, img_h))\n\n\n# 矫正畸变\nimg2 = cv2.imread(file_in + &#39;\\\\img_0.jpg&#39;) # 第几张图片\ndst = cv2.undistort(img2, mtx, dist, None, newcameramtx)\ncv2.imwrite(file_out + &#39;/calibresult.jpg&#39;, dst)\nprint(&quot;newcameramtx（优化后相机内参）:\\n&quot;, newcameramtx)\n\n# 反投影误差total_error,越接近0，说明结果越理想。\ntotal_error = 0\nfor i in range(len(objpoints)):\n    imgpoints2, _ = cv2.projectPoints(objpoints[i], rvecs[i], tvecs[i], mtx, dist)   # 计算三维点到二维图像的投影\n    error = cv2.norm(imgpoints[i], imgpoints2, cv2.NORM_L2) / len(imgpoints2)   # 反投影得到的点与图像上检测到的点的误差\n    total_error += error\nprint((&quot;total error: &quot;), total_error / len(objpoints))   # 记平均\n\n# 将旋转向量转换为旋转矩阵\nR, _ = cv2.Rodrigues(rvecs[0])\n# 构建外参矩阵\nextrinsic_matrix = np.hstack((R, tvecs[0]))\nprint(&quot;外参矩阵：&quot;)\nprint(extrinsic_matrix)\n\n坐标系映射做完相机的内外参标定之后就可以通过深度数据、相机内参K和相机外参E去建立标定的世界坐标系和我们相机的像素坐标系之间的映射关系。\n原理公式\n\n\n\n\n\n\n!!!\n捏嘿^o^待补充~~~~\n\n\n代码实现python\n# 世界坐标系到像素坐标系的投影\ndef project_world_to_pixel(point_3d):\n    # 将3D点转换为齐次坐标\n    point_world = np.vstack((point_3d.reshape(3, 1), 1))\n    # 将3D点从世界坐标系转换到相机坐标系\n    point_camera = Rt @ point_world\n    # 求出齐次坐标下的像素坐标\n    point_pixel_homogeneous = K @ point_camera[:3]\n    # 归一化像素坐标\n    point_pixel = point_pixel_homogeneous / point_pixel_homogeneous[2]\n    return point_pixel[:2]\n\n# 像素坐标系到世界坐标系的投影\ndef project_pixel_to_world(point_pixel, depth=t[2]):\n    Rt_extended = np.vstack((Rt, [0, 0, 0, 1]))\n    point_pixel_homogeneous = np.vstack((point_pixel, 1))\n    point_camera = np.linalg.inv(K) @ point_pixel_homogeneous\n    point_camera = point_camera * depth\n    point_camera_homogeneous = np.vstack((point_camera, 1))\n    point_world_homogeneous = np.linalg.inv(Rt_extended) @ point_camera_homogeneous\n    point_world = point_world_homogeneous[:3] / point_world_homogeneous[3]\n    return point_world棋盘检测由于比赛的时间关系，我采用的是传统2D视觉图像处理的方法来检测棋盘。\n检测思路\n\n\n\n\n\n\nWARNING\n采用传统轮廓检测算法，首先进行canny边缘提取，通过设置相应的阈值实现将棋盘的边缘进行提取，再执行膨胀操作，将内外边缘合一，通过设置面积特征、形状特征和边长特征的提取，将我对棋盘的ROI区域画出来，完成对棋盘外轮廓的确定。再通过棋盘三等分的几何特性，我将九个格子的十六个格点分别进行提取，再根据这十六个格点相邻之间的像素坐标关系，计算出各个格子的中心点的像素坐标值，由此完成对棋盘的识别。\n\n\n\n\n\n\n\nMY VIEW\n优点：图像处理算法相对简单运行起来帧率高。缺陷：需要固定相机位，如果相机位置改变则需要改很多参数。\n\n\n代码实现python\n# 找棋盘格\ndef find_chessboard(color_image, depth_image=None):\n    find_center_method = 1\n    pixels_threshold = 80\n    area_threshold = 50\n    appo_contour = None\n    grid_size = None\n    sorted_center_list = [] # 排序后的中心点像素坐标列表\n\n    # 转换为灰度图像\n    gray_image = cv2.cvtColor(color_image, cv2.COLOR_RGB2GRAY)\n    # 高斯模糊\n    # blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n    # 边缘检测\n    edged = cv2.Canny(gray_image, 75, 150)\n    \n    # 膨胀处理\n    kernel = np.ones((5, 5), np.uint8)\n    dilated = cv2.dilate(edged, kernel, iterations=1)\n    # cv2.imshow(&quot;dilated&quot;,dilated)\n    # 选取ROI区域\n    dilated_ROI = dilated[10:700,100:900]\n    # ROI与原图像对齐\n    dilated_ROI = extend_frame_boarden(dilated_ROI)\n    cv2.imshow(&quot;dilated_ROI&quot;,dilated_ROI)\n    # 查找轮廓\n    contours, _ = cv2.findContours(dilated_ROI, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n    if len(contours) &gt; 0:\n        # 筛选出最大的轮廓\n        largest_contour = contours[:2]\n        for contour in largest_contour:\n            area = cv2.contourArea(contour)\n            \n            # 近似多边形\n            epsilon = 0.02 * cv2.arcLength(contour, True)\n            approx = cv2.approxPolyDP(contour, epsilon, True)\n            # print(area,approx)\n            if len(approx) == 4:\n               if area &gt;=30000 and area &lt;= 50000 :\n                appo_contour = contour\n                    # corners = approx.reshape((4, 2))\n                break\n\n        if appo_contour is not None:\n            corners = approx.reshape((4, 2))\n            tl = corners[0]\n            bl = corners[1]\n            br = corners[2]\n            tr = corners[3]\n            # 计算3x3格子的交叉点\n            cross_points = []\n            for i in range(4):\n                for j in range(4):\n                    # 线性插值计算交叉点\n                    cross_x = int((tl[0] * (3 - i) + tr[0] * i) * (3 - j) / 9 +\n                                (bl[0] * (3 - i) + br[0] * i) * j / 9)\n                    cross_y = int((tl[1] * (3 - i) + tr[1] * i) * (3 - j) / 9 +\n                                (bl[1] * (3 - i) + br[1] * i) * j / 9)\n                    cross_points.append((cross_x, cross_y))\n                    cv2.circle(color_image, (cross_x, cross_y), 3, (0, 255, 0), -1)\n\n            distances = []\n            # 计算每行相邻交叉点的距离\n            for i in range(4):\n                for j in range(3):\n                    p1 = cross_points[i * 4 + j]\n                    p2 = cross_points[i * 4 + j + 1]\n                    distance = np.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)\n                    distances.append(distance)\n            # 计算每列相邻交叉点的距离\n            for j in range(4):\n                for i in range(3):\n                    p1 = cross_points[i * 4 + j]\n                    p2 = cross_points[(i + 1) * 4 + j]\n                    distance = np.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)\n                    distances.append(distance)\n            grid_size = np.mean(distances)\n            # print(&quot;grid_size is&quot;, grid_size)\n            centers = []\n            \n            # 找格子中心点方法一：直接根据顶点计算\n            #if find_center_method == 1:\n            for i in range(3):\n                for j in range(3):\n                    center_x = int((cross_points[i * 4 + j][0] + cross_points[i * 4 + j + 1][0] + cross_points[(i + 1) * 4 + j][0] + cross_points[(i + 1) * 4 + j + 1][0]) / 4)\n                    center_y = int((cross_points[i * 4 + j][1] + cross_points[i * 4 + j + 1][1] + cross_points[(i + 1) * 4 + j][1] + cross_points[(i + 1) * 4 + j + 1][1]) / 4)\n                    centers.append((center_x, center_y))\n                    cv2.circle(color_image, (center_x, center_y), 2, (0, 255, 0), -1)\n            # elif find_center_method == 2:\n            # 对找到的中心点进行编号, y + x 最大就是右下角，最小就是左上角， y-x 最大就是左下角，y-x 最小就是右上角，其它几个点根据在旁边两个点中间判断\n            if len(centers) == 9:\n                centers = np.array(centers)\n                rect = np.zeros((9, 2), dtype=&quot;float32&quot;)\n                dist = np.zeros(9, dtype=&quot;float32&quot;)\n                s = centers.sum(axis=1)\n                idx_0 = np.argmin(s)\n                idx_8 = np.argmax(s)\n                diff = np.diff(centers, axis=1)\n                idx_2 = np.argmin(diff)\n                idx_6 = np.argmax(diff)\n                rect[0] = centers[idx_0]\n                rect[2] = centers[idx_2]\n                rect[6] = centers[idx_6]\n                rect[8] = centers[idx_8]\n                #   其它点\n                calc_center = (rect[0] + rect[2] + rect[6] + rect[8]) / 4\n                mask = np.zeros(centers.shape[0], dtype=bool)\n                idxes = [1, 3, 4, 5, 7]\n                mask[idxes] = True    \n                # 筛选出其他中心点\n                others = centers[mask]\n                # 找到最左、最右、最上和最下的点的索引\n                idx_l = others[:,0].argmin()\n                idx_r = others[:,0].argmax()\n                idx_t = others[:,1].argmin()\n                idx_b = others[:,1].argmax()\n                found = np.array([idx_l, idx_r, idx_t, idx_b])\n                mask = np.isin(range(len(others)), found, invert=False)\n                idx_c = np.where(mask == False)[0]\n                if len(idx_c) == 1:\n                    rect[1] = others[idx_t]\n                    rect[3] = others[idx_l]\n                    rect[4] = others[idx_c]\n                    rect[5] = others[idx_r]\n                    rect[7] = others[idx_b]\n                    # 写编号\n                    for i in range(9):\n                                                                        \n                        # print(f&quot;&#123;i&#125;格的中心点&quot;,rect[i][0],rect[i][1])\n                        cv2.putText(color_image, str(i+1), (int(rect[i][0]), int(rect[i][1])), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n                        sorted_center_list.append((int(rect[i][0]), int(rect[i][1])))\n                else:\n                    # 大于 45度的情况\n                    print(&quot;&gt; 45 degree&quot;) \n        else:\n            print(&quot;without appopriate contour&quot;)\n    return sorted_center_list if &#39;sorted_center_list&#39; in locals() else [], grid_size if &#39;grid_size&#39; in locals() else None棋子检测这里提供两种方法\n3DV方法3D视觉方法\n实现思路\n\n\n\n\n\n\nWARNING\n采用二值化+3D点云数据融合分辨黑色棋子和白色棋子。首先，通过对图像滤波再做二值化处理就可以得到黑色棋子的完美轮廓，通过棋子形状和面积等方法去提取出黑色棋子的中心坐标点。再通过对相机做depth和RGB的数据对齐，可以使得到的点云数据和RGB数据有一个对应关系，将点云数据提取出来之后，由于棋盘是一个光滑平面，可以通过ransac拟合平面，再通过点云处理将这个平面给隐藏，再通过 DBSCAN 聚类提取出每个棋子的点云，选取棋子点云中心点作为棋子的相机坐标系坐标，再将每个棋子的相机坐标系坐标映射到像素坐标系上，便可以得到棋子的像素坐标中心点，再将之前二值化得到的黑色棋子的中心坐标点剔除，剩下的就是白色棋子中心点像素坐标。由此，我们成功地完成了对棋子的识别。\n\n\n\n\n\n\n\nMY VIEW\n优点：识别基本不受环境光线的干扰，通过三维的信息来识别棋子，你放张棋子照片上去也不会误识别。缺点：点云数据量大，即使做了降采样和裁切之后对算力的要求仍旧很高，帧率很低，体验3帧电竞，对一些实时性有要求的场景不太适合。\n\n\n代码实现（仅代码块）python\n# 拟合平面ransac \ndef fit_plane(points):\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(points)\n    plane_model, inliers = pcd.segment_plane(distance_threshold=0.27,\n                                             ransac_n=3,\n                                             num_iterations=1000)\n    return plane_model, inliers\n\ndef detect_objects_on_plane(points, distance_threshold=5):\n    # 拟合平面模型，返回平面模型参数和内点索引\n    plane_model, inliers = fit_plane(points)\n    # 解包平面模型参数 a, b, c, d\n    [a, b, c, d] = plane_model\n    # 计算每个点到平面的距离\n    distances = np.abs(a * points[:, 0] + b * points[:, 1] + c * points[:, 2] + d) / np.sqrt(a**2 + b**2 + c**2)\n    # 找到距离大于阈值的点的索引\n    object_indices = np.where(distances &gt; distance_threshold)[0]\n    # 提取这些点作为检测到的物体\n    objects = points[object_indices]\n    objects_pcd = o3d.geometry.PointCloud()\n    objects_pcd.points = o3d.utility.Vector3dVector(objects)\n    cl, ind = objects_pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n    filtered_pcd = objects_pcd.select_by_index(ind)\n    # 返回检测到的物体点云 \n    return objects_pcd\n\n\n# 使用 DBSCAN 聚类算法提取不同物体\ndef extract_objects_from_pcd(objects_pcd, eps=1, min_points=10):\n    # 进行 DBSCAN 聚类\n    labels = np.array(objects_pcd.cluster_dbscan(eps=eps, min_points=min_points, print_progress=True))\n    # 获取聚类数目\n    max_label = labels.max()\n    print(f&quot;point cloud has &#123;max_label + 1&#125; clusters&quot;)\n    # 将不同聚类的点云提取出来\n    clusters = []\n    for i in range(max_label + 1):\n        cluster_indices = np.where(labels == i)[0]\n        cluster_pcd = objects_pcd.select_by_index(cluster_indices)\n        clusters.append(cluster_pcd)\n    return clusters\n\npcd = convert_to_o3d_point_cloud(np.array(points))\n# 降采样\ndownsampled_pcd = pcd.voxel_down_sample(voxel_size=0.01)\n# 裁剪点云\n# 定义包围盒的最小和最大边界点\n# 创建轴对齐包围盒\naabb = pcd.get_axis_aligned_bounding_box()\nmin_bound = aabb.min_bound\nmax_bound = aabb.max_bound\naabb1 = o3d.geometry.AxisAlignedBoundingBox([min_bound[0]+250,min_bound[1]+200,min_bound[2]],\n                                                [max_bound[0]-250,max_bound[1]-200,max_bound[2]])\ncropped_pcd = downsampled_pcd.crop(aabb1)\n# 将 Open3D 点云对象转换为 numpy 数组\npoints_np = np.asarray(cropped_pcd.points)\nobjects_pcd = detect_objects_on_plane(points_np)\n# 提取不同物体的点云\ncluster_pcd_list = extract_objects_from_pcd(objects_pcd)\n# 可视化处理后的点云\n# 创建坐标系对象\ncoordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=100, origin=[0, 0, 0])\no3d.visualization.draw_geometries([coordinate_frame, cluster_pcd_list[0]])2DV方法传统2D视觉方法\n实现思路\n\n\n\n\n\n\nWARNING\n采用背景减除+不同色域提取掩膜将黑色棋子和白色棋子在红色背景下提取出来。首先，采用固定机位的相机因此可以相对准确的标记出大致的ROI区域。根据环境预先获取没有棋子的相机RGB图，在处理时对相机的帧数据上与预先的背景RGB图的每个通道做绝对求差，比较两个图像的像素值，提取出前景对象。这样可以很好的将棋子的轮廓和背景之间进行分离。根据分离出来的颜色的对比程度，可以简易判断做掩膜提取效果较好的空间色域。将分离出来的前景图像在RGB空间中做色域提取，通过设置RGB颜色阈值可以将黑色棋子很容易的提取出来，均值滤波后通过K_Means聚类算法将轮廓聚集，二值化前景得到掩膜图。通过上述步骤，就成功地在掩膜图上提取到了特别完整的棋子轮廓，再通过轮廓检测将棋子的中心像素坐标值进行确定，将这个中心坐标和轮廓匹配对齐到RGB帧图像上，最终完成对棋子的识别。\n\n\n\n\n\n\n\nMY VIEW\n优点：识别速度快，算法简单。缺点：会受到环境光的影响，鲁棒性较差，只能使用在理想测试环境下，实际工况复杂需要更多的其他增强稳定性的算法，可以考虑机器学习那一套搞决策树、随机森林、支持向量机之类的（这个后续我也会跟进去学习的）。\n\n\n代码实现python# 自适应调节rgb阈值 \ndef adjust_rgb_range(image, base_lower, base_upper):\n    # 计算图像的加权平均亮度\n    brightness = np.mean(image[:, :, 0] * 0.299 + image[:, :, 1] * 0.587 + image[:, :, 2] * 0.114)\n\n    # 根据亮度调整色域范围\n    adjustment_factor = brightness / 128.0  # 假设 128 是中等亮度\n    lower_rgb = np.array([base_lower[0] * adjustment_factor, base_lower[1] * adjustment_factor, base_lower[2] * adjustment_factor], dtype=np.uint8)\n    upper_rgb = np.array([base_upper[0] * adjustment_factor, base_upper[1] * adjustment_factor, base_upper[2] * adjustment_factor], dtype=np.uint8)\n\n# 背景减除二值化\ndef background_bin_subtraction(background, current_frame, binary_threshold=100):\n    # 调整二值化阈值 binary_threshold 可以有效地筛选黑白棋\n    # 调整图像尺寸和通道数\n    if background.shape != current_frame.shape:\n        current_frame = cv2.resize(current_frame, (background.shape[1], background.shape[0]))\n        if len(background.shape) == 2 and len(current_frame.shape) == 3:\n            current_frame = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n        elif len(background.shape) == 3 and len(current_frame.shape) == 2:\n            current_frame = cv2.cvtColor(current_frame, cv2.COLOR_GRAY2BGR)\n    \n    foreground = cv2.absdiff(background, current_frame)\n    global foreground_image \n    foreground_image = foreground\n    gray_foreground = cv2.cvtColor(foreground, cv2.COLOR_BGR2GRAY)\n    _, binary_foreground = cv2.threshold(gray_foreground, binary_threshold, 255, cv2.THRESH_BINARY) \n    return binary_foreground\n\n# 背景减除RGB空间\ndef background_rgb_subtraction(background, current_frame):\n    rgb_low_threshold = np.array([0, 0, 160])\n    rgb_high_threshold = np.array([65, 50, 235])\n    # 调整图像尺寸和通道数\n    if background.shape != current_frame.shape:\n        current_frame = cv2.resize(current_frame, (background.shape[1], background.shape[0]))\n        if len(background.shape) == 2 and len(current_frame.shape) == 3:\n            current_frame = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n        elif len(background.shape) == 3 and len(current_frame.shape) == 2:\n            current_frame = cv2.cvtColor(current_frame, cv2.COLOR_GRAY2BGR)\n    \n    # 计算差值\n    foreground = cv2.absdiff(background, current_frame)\n    mask = cv2.inRange(foreground, rgb_low_threshold, rgb_high_threshold)\n    #cv2.imshow(&quot;mask&quot;, mask)\n    return mask\n\n# 找白色棋子\ndef find_white_piece(color_image,area_low_threshold = 1000, area_high_threshold = 10000):\n    # 棋子认定面积阈值\n    # area_low_threshold\n    # area_high_threshold\n    # 背景减除二值化\n    white_piece_center_list = []\n    foreground = background_bin_subtraction(background_image, color_image)\n\n    # 使用形态学操作\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n    # closed = cv2.morphologyEx(foreground, cv2.MORPH_CLOSE, kernel)\n    #腐蚀\n    erode = cv2.erode(foreground, kernel, iterations=5)\n    # 膨胀\n    dilate = cv2.dilate(erode, kernel, iterations=5)\n    # cv2.imshow(&quot;dilate&quot;,dilate)\n    # 查找轮廓\n    contours, _ = cv2.findContours(dilate.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    i = 0\n    for contour in contours:\n        # 计算并绘制外接矩形框\n        x, y, w, h = cv2.boundingRect(contour)\n        cv2.rectangle(foreground, (x, y), (x + w, y + h), (0, 255, 0), 2)\n        area = cv2.contourArea(contour)\n        if area &gt; area_low_threshold and area &lt; area_high_threshold:\n            i += 1\n            # 绘制轮廓\n            cv2.drawContours(foreground, [contour], -1, (255, 255, 0), 2)\n            \n            # 计算轮廓的矩\n            M = cv2.moments(contour)\n            \n            # 计算轮廓的质心\n            if M[&quot;m00&quot;] != 0:\n                cX = int(M[&quot;m10&quot;] / M[&quot;m00&quot;])\n                cY = int(M[&quot;m01&quot;] / M[&quot;m00&quot;])\n            else:\n                cX, cY = 0, 0\n            \n            ## 打印质心坐标\n            #print(f&quot;Contour &#123;i&#125; center: (&#123;cX&#125;, &#123;cY&#125;)&quot;)\n\n            # 添加到白棋中心点列表\n            white_piece_center_list.append((cX, cY))\n            # 在质心处绘制数字\n            cv2.putText(foreground, str(i), (cX , cY), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n    if i == 0:\n        print(&quot;without any chess&quot;)\n    return white_piece_center_list, foreground\n\n# 找黑色棋子\ndef fine_black_piece(color_image,area_low_threshold = 1000, area_high_threshold = 40000):\n    black_piece_center_list = []\n    foreground = background_rgb_subtraction(background_image, color_image)\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n    dilate = cv2.dilate(foreground, (5,5), iterations=2)\n    #腐蚀\n    erode = cv2.erode(dilate, kernel, iterations=3)\n    # 膨胀\n    dilate = cv2.dilate(erode, kernel, iterations=3)\n    # cv2.imshow(&quot;dilate&quot;,dilate)\n    contours, _ = cv2.findContours(dilate.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    i = 0\n    for contour in contours:\n        area = cv2.contourArea(contour)\n        # 计算并绘制外接矩形框\n        x, y, w, h = cv2.boundingRect(contour)\n        # print(w*h)\n        if area &gt; area_low_threshold and area &lt; area_high_threshold:\n            i += 1\n            # 绘制轮廓\n            cv2.drawContours(foreground, [contour], -1, (255, 255, 0), 2)\n            # 计算轮廓的矩\n            M = cv2.moments(contour)\n            # 计算轮廓的质心\n            if M[&quot;m00&quot;] != 0:\n                cX = int(M[&quot;m10&quot;] / M[&quot;m00&quot;])\n                cY = int(M[&quot;m01&quot;] / M[&quot;m00&quot;])\n            else:\n                cX, cY = 0, 0\n            # # 打印质心坐标\n            # print(f&quot;Contour &#123;i&#125; center: (&#123;cX&#125;, &#123;cY&#125;)&quot;)\n            black_piece_center_list.append((cX, cY))\n            # 在质心处绘制数字\n            cv2.putText(foreground, str(i), (cX , cY), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n    if i == 0:\n        print(&quot;without any chess&quot;)\n    return black_piece_center_list, foreground串口通信通过串口通信与下位机通信\n实现思路\n\n\n\n\n\n\nWARNING\n在做这个串口通信的时候，使用的是serial库用python来进行串口通信，这个其实是人家通过C++写的封装成python库来用的相对会有很多的受限。因为jetson nano没有硬件中断，所以只能采用多线程的方式来模拟中断。所以调用threading库去实现对串口数据的监听。\n\n代码实现python\nimport threading\nimport serial \n\n# 串口读取数据线程\nclass SerialCommunication:\n    def __init__(self, port, baudrate):\n        self.serial_port = serial.Serial(\n            port=port,\n            baudrate=baudrate,\n            bytesize=serial.EIGHTBITS,\n            parity=serial.PARITY_NONE,\n            stopbits=serial.STOPBITS_ONE,\n            timeout=1\n        )\n    \n    def read_from_port(self): # 监听串口数据\n        while True:\n            try:\n                if self.serial_port.in_waiting &gt; 0:\n                    data = self.serial_port.readline().decode(&#39;utf-8&#39;).rstrip()\n                    print(f&quot;Receive:&#123;data&#125;&quot;)\n                    # 读到的数据会存入data，可以自己再写类的接口把这个data读出来\n\n            except Exception as e:\n                print(f&quot;An error occurred: &#123;e&#125;&quot;)其他\n\n\n\n\n\n\nWARNING\n解决了最大头的图像处理部分之后剩下的其实就是一些简单的逻辑判断了，基本就是一些if-else，我感觉不太重要就不太想讲了。就是作三子棋对弈算法会麻烦一点不过也没什么的我就也放在这里了。\n\n这里提供一点我当时写的函数\n\n对接对弈算法的\npython\n# 检查棋子是否在格子内，只是简单判断，通过比较点的坐标和格子的边界来判断点是否在格子内，复杂的需要考虑坐标系映射\ndef is_point_in_grid(point, grid_center, grid_size, piece_radius):\n    half_size = grid_size / 2\n    return (grid_center[0] - half_size &lt;= point[0] &lt;= grid_center[0] + half_size and\n            grid_center[1] - half_size &lt;= point[1] &lt;= grid_center[1] + half_size)\n\n# 检查棋子在哪个格子内\ndef check_pieces_in_grids(grid_centers, black_piece_centers, white_piece_centers, grid_size, black_piece_radius=2, white_piece_radius=2):\n    grid_status = []\n    black_pieces_outside = []\n    white_pieces_outside = []\n\n    for grid_center in grid_centers:\n        status = &quot;empty&quot;\n        for black_center in black_piece_centers:\n            if is_point_in_grid(black_center, grid_center, grid_size, black_piece_radius):\n                status = &quot;black&quot;\n                break\n        for white_center in white_piece_centers:\n            if is_point_in_grid(white_center, grid_center, grid_size, white_piece_radius):\n                status = &quot;white&quot;\n                break\n        grid_status.append(status)\n\n    # 检查不在任何格子内的黑棋\n    for black_center in black_piece_centers:\n        in_any_grid = any(is_point_in_grid(black_center, grid_center, grid_size, black_piece_radius) for grid_center in grid_centers)\n        if not in_any_grid:\n            black_pieces_outside.append(black_center)\n\n    # 检查不在任何格子内的白棋\n    for white_center in white_piece_centers:\n        in_any_grid = any(is_point_in_grid(white_center, grid_center, grid_size, white_piece_radius) for grid_center in grid_centers)\n        if not in_any_grid:\n            white_pieces_outside.append(white_center)\n\n    return grid_status, black_pieces_outside, white_pieces_outside\n\n\n对弈算法\n使用的是在棋类对弈中广泛使用的Minimax算法。又名极小化极大算法，是一种找出失败的最大可能性中的最小值的算法。Minimax算法常用于棋类等由两方较量的游戏和程序。该算法是一个零总和算法，即一方要在可选的选项中选择将其优势最大化的选择，另一方则选择令对手优势最小化的方法。而系统中函数的具体实现是minmax函数，而best_move函数则是通过调用minmax函数来找到机器的最佳移动位置，其返回值为机器的最佳移动位置。具体来说，Minimax算法模拟了所有可能的棋盘状态，本设计中的minmax函数返回值为得分，用于表示当前棋盘对玩家的有利程度，其主要包含以下几个步骤：\n\n检查当前棋盘状态是否有胜利者。如果有胜利者，返回相应的分数。\n检查棋盘是否已满。如果已满，返回 0 表示平局。\n如果轮到机器移动，则尝试所有可能的移动，返回得分最高的移动。\n如果轮到人类移动，则尝试所有可能的移动，返回得分最低的移动。minmax函数的具体实现见附录1。\n\npython\n#################################################################################################\n# 定义常量\nPLAYER_X = 1    #机器棋\nPLAYER_O = 2    #玩家棋\nEMPTY = 0       #无\n\ndef check_winner(board):\n    # 检查行、列和对角线\n    lines = [\n        [board[0][0], board[0][1], board[0][2]],\n        [board[1][0], board[1][1], board[1][2]],\n        [board[2][0], board[2][1], board[2][2]],\n        [board[0][0], board[1][0], board[2][0]],\n        [board[0][1], board[1][1], board[2][1]],\n        [board[0][2], board[1][2], board[2][2]],\n        [board[0][0], board[1][1], board[2][2]],\n        [board[2][0], board[1][1], board[0][2]]\n    ]\n    for line in lines:\n        if line == [PLAYER_X, PLAYER_X, PLAYER_X]:\n            return PLAYER_X\n        if line == [PLAYER_O, PLAYER_O, PLAYER_O]:\n            return PLAYER_O\n    return None\n\ndef is_board_full(board):\n    return all(cell != EMPTY for row in board for cell in row)\n\ndef minimax(board, is_maximizing):\n    winner = check_winner(board)\n    if winner == PLAYER_X:\n        return 1\n    if winner == PLAYER_O:\n        return -1\n    if is_board_full(board):\n        return 0\n\n    if is_maximizing:\n        best_score = -float(&#39;inf&#39;)\n        for row in range(3):\n            for col in range(3):\n                if board[row][col] == EMPTY:\n                    board[row][col] = PLAYER_X\n                    score = minimax(board, False)\n                    board[row][col] = EMPTY\n                    best_score = max(score, best_score)\n        return best_score\n    else:\n        best_score = float(&#39;inf&#39;)\n        for row in range(3):\n            for col in range(3):\n                if board[row][col] == EMPTY:\n                    board[row][col] = PLAYER_O\n                    score = minimax(board, True)\n                    board[row][col] = EMPTY\n                    best_score = min(score, best_score)\n        return best_score\n\ndef best_move(board):\n    best_score = -float(&#39;inf&#39;)\n    move = (-1, -1)\n    for row in range(3):\n        for col in range(3):\n            if board[row][col] == EMPTY:\n                board[row][col] = PLAYER_X\n                score = minimax(board, False)\n                board[row][col] = EMPTY\n                if score &gt; best_score:\n                    best_score = score\n                    move = (row, col)\n\ndef get_board(chessboard_block_list,black_piece_list,white_piece_list,grid_size,state):\n    # 检查棋子在哪个格子内\n    grid_status, black_piece_outside_list, white_piece_outside_list = check_pieces_in_grids(\n                            chessboard_block_list, \n                            black_piece_list, \n                            white_piece_list, \n                            grid_size)\n    # 将棋盘状态映射为棋盘\n    if state == 0:\n        def map_board_state(board):\n            state_mapping = &#123;\n                &quot;empty&quot;: 0,\n                &quot;black&quot;: 1,\n                &quot;white&quot;: 2\n            &#125;\n            return [[state_mapping[cell] for cell in row] for row in board]\n        mapped_board = map_board_state(grid_status)\n        return mapped_board\n    else :\n        def map_board_state(board):\n            state_mapping = &#123;\n                &quot;empty&quot;: 0,\n                &quot;black&quot;: 2,\n                &quot;white&quot;: 1\n            &#125;\n            return [[state_mapping[cell] for cell in row] for row in board]\n        mapped_board = map_board_state(grid_status)\n        return mapped_board\n\ndef play_chess(now_board,chessboard_grid_world_list=None,black_piece_world_list=None,white_piece_world_list=None, serial_comm=None,k_flag=1):\n    global last_board\n    row,col = 0,0\n    for i in range(3):      ##第六问判断棋子是否变化\n                for j in range(3):\n                    if now_board[i][j]-last_board[i][j]&gt;0:\n                        row,col = i,j\n                    elif now_board[i][j]-last_board[i][j]&lt;-1:\n                        print(&quot;请不要拿走&quot;+str(i)+&quot;,&quot;+str(j)+&quot;黑棋子,请放回去&quot;)\n                        k_flag = 0\n                        code = i*3+j+1\n                        \n                        \n                        \n                    elif now_board[i][j]-last_board[i][j]&lt;0:\n                        k_flag = 0\n                        print(&quot;请不要拿走&quot;+str(i)+&quot;,&quot;+str(j)+&quot;白棋子,请放回去&quot;)\n                        code = i*3+j+1\n    if k_flag :     \n        last_board[row][col] = PLAYER_O     ##棋子被放置完之后记录上一步是玩家下的\n        print(&quot;对手完成：&quot;+str(row)+&quot;,&quot;+str(col))\n        \n        if check_winner(last_board):\n            print(last_board)\n            print(&quot;玩家 O 胜利!&quot;)\n            k_flag = 0\n        if is_board_full(last_board):\n            print(last_board)\n            print(&quot;平局!&quot;)\n            k_flag = 0\n\n    if k_flag:  ##如果标志位为1则继续此回合\n            # 机器回合\n            row, col = best_move(last_board)    ##判断最佳决策\n            print(&quot;机器的回合:&quot;+str(row)+&quot;,&quot;+str(col))\n            if serial_comm.turn4_flag == 1:\n                black_piece_world_interger_x, black_piece_world_decimal_x = split_float_number_to_str(black_piece_world_list[0][0]/10.0)\n                black_piece_world_interger_y, black_piece_world_decimal_y = split_float_number_to_str(black_piece_world_list[0][1]/10.0)\n                message_piece = &quot;(*&quot; + black_piece_world_interger_y + black_piece_world_decimal_y + black_piece_world_interger_x + black_piece_world_decimal_x + &quot;)&quot;\n                serial_comm.serial_port.write(message_piece.encode(&quot;utf-8&quot;))       ##等待机器将棋子放置完毕\n            elif serial_comm.turn5_flag == 1:\n                white_piece_world_interger_x, white_piece_world_decimal_x = split_float_number_to_str(white_piece_world_list[0][0]/10.0)\n                white_piece_world_interger_y, white_piece_world_decimal_y = split_float_number_to_str(white_piece_world_list[0][1]/10.0)\n                message_piece = &quot;(*&quot; + white_piece_world_interger_y + white_piece_world_decimal_y + white_piece_world_interger_x + white_piece_world_decimal_x + &quot;)&quot;\n                serial_comm.serial_port.write(message_piece.encode(&quot;utf-8&quot;))       ##等待机器将棋子放置完毕\n            chessboard_grid_world_interger_x, chessboard_grid_world_decimal_x = split_float_number_to_str(chessboard_grid_world_list[row*3+col][0]/10.0)\n            chessboard_grid_world_interger_y, chessboard_grid_world_decimal_y = split_float_number_to_str(chessboard_grid_world_list[row*3+col][1]/10.0)\n            message = &quot;(*&quot; + chessboard_grid_world_interger_y + chessboard_grid_world_decimal_y + chessboard_grid_world_interger_x + chessboard_grid_world_decimal_x + &quot;)&quot;\n            serial_comm.serial_port.write(message.encode(&quot;utf-8&quot;))       ##等待机器将棋子放置完毕\n            \n            print(&quot;机器完成:&quot;)\n            ##记录上一步机器下的位置\n            last_board[row][col] = PLAYER_X\n\n            ##判断游戏是否结束\n            if check_winner(last_board):\n                print(last_board)\n                print(&quot;机器 胜利!&quot;)\n                \n            if is_board_full(last_board):\n                print(last_board)\n                print(&quot;平局!&quot;)\n            \n#################################################################################################\n\n\n后记\n\n\n\n\n\nTIP\n四天三夜的电赛，人要颠了，好悬没把我攒昏过去。总算搞完电赛的总结了~~\n\n","slug":"三子棋对弈装置-视觉部分","date":"2024-08-12T14:55:02.000Z","categories_index":"Electronic","tags_index":"ComputerVision","author_index":"Yesord"},{"id":"71bced55db8d61b5404a5480e19d78af","title":"树莓派数字识别——openCV方法","content":"前言本项目是树莓派识别数字的小例程电赛备赛时写的，实现对1-8的数字识别并与下位机通信控制小车进行运动\n\n\n\n项目前置python 版本要求\n\n\n\n\n\nTIP\npython&gt;&#x3D;3.6\n\n库依赖要求做图像处理\n\n\n\n\n\n\nTIP\nnumpyopencv-python\n\n做引脚操控\n\n\n\n\n\n\nTIP\nRPi.GPIOpyserialrpi-lgpi\n\n项目主体主要实现思路传统图像处理，使用模板匹配\n主要步骤\n\n\n\n\n\n\nWARNING\n\n对模板图像进行预处理，得到1-8总共8个模板\n对识别图像进行预处理，将图像二值化转换取出矩形区域组成ROI区域\nROI和模板做模板匹配\n\n\n\n详细过程仅展示部分源码，详细源码可以去 GitHub库\n模板图像预处理\n\n\n模板预览图本身是含有八个数字带外接矩形黑框的一张图，我们对它作二值化处理，在对二值化模板图像边缘提取，通过轮廓检测将模板的轮廓提取，在将其外接矩形坐标画出来，在对不同数字1-8进行排序，最后将整个图像按比例内截，在重排成（100，150）的固定的模板存入字典。\n\npython\n# 定义需识别数字\nto_detect_num = None\n\n# 硬件初始化\nser = hardware_init()\n\n    \ntemplate = cv2.imread(osp.join(template_dir, &#39;num_template.jpg&#39;))\ngray_template = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\nprint(&#39;二值化模板图像已读取&#39;)\n\n# 边缘检测\nedged_template = cv2.Canny(template, 80, 200) # 边缘低阈值75和高阈值20\n\n# 轮廓检测\ntemplate_contours, hierarchy = cv2.findContours(edged_template, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n# 读取模板轮廓的矩形坐标\nx, y, w, h = cv2.boundingRect(template_contours[1])\n\nprint(f&#39;检测到&#123;len(template_contours)&#125;个轮廓&#39;)\n\n# 模板数字排序 1-8 (太菜了写不出自动排序的脚本 &gt;_&lt; )\nsorted_template_contours = [template_contours[5], template_contours[4], template_contours[1],template_contours[0], template_contours[7], template_contours[6], template_contours[3], template_contours[2]]\n\n# 构造模板字典\ndigit_dict = &#123;&#125;\n\n# 分割模板图像\nfor (i, c) in enumerate(sorted_template_contours):\n    (x, y, w, h) = cv2.boundingRect(c)\n    roi = gray_template[y+10:y+h-10, x+10:x+w-10]\n    \n    roi = cv2.resize(roi, (100, 150))\n    digit_dict[i] = roi识别图像预处理从摄像头获取视频流，将其缩放固定比例减少后续的运算量，转换为灰度图并应用高斯模糊将锐度过高的区域排除边缘和消除噪点，再使用Canny边缘检测，提取边缘后进行膨胀，得到识别数字完整的矩形边缘轮廓。通过轮廓检测再将轮廓面积最大的前5个轮廓进行提取并排序，计算轮廓周长，进行近似轮廓，看是否能构成四边形，将能构成四边形的轮廓且面积符合一定要求的轮廓进行筛选，并通过两个矩形左x坐标的大小来判断数字的左右并重排。本身通过相机拍到的矩形数字框就不可能是正好的矩形，需要我们进行图像变换，因此进行透射变换，将提取到的四边形变换成矩形然后二值化。\n\n\n\n\n\npython\n# 初始化视频捕获对象\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    # 逐帧读取视频\n    start_time = time.time()\n    ret, frame = cap.read()\n    if not ret:\n        break\n    # 图像resize\n    ratio = frame.shape[0] / 500.0\n    orig_frame = frame.copy()\n    # 转换为灰度图像\n    gray_img = cv2.cvtColor(orig_frame, cv2.COLOR_BGR2GRAY)\n    # 应用高斯模糊\n    blur_img = cv2.GaussianBlur(gray_img, (5, 5), 0)\n    \n    # Canny 边缘检测\n    edged_img = cv2.Canny(blur_img, low_threshold, high_threshold) # 边缘低阈值75和高阈值200\n    cv2.imshow(&quot;edged_img&quot;,edged_img)\n    # 定义膨胀操作的核\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n    # 对边缘检测后的图像进行膨胀操作\n    dilated_img = cv2.dilate(edged_img, kernel, iterations=2)\n    cv2.imshow(&quot;Dilated Image&quot;, dilated_img)\n\n    \n    # 轮廓检测\n    contours, _ = cv2.findContours(edged_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)[:5]\n    \n    \n    i = 0\n    screenCnt = []  # 在循环开始前初始化screenCnt\n    output = []\n    for c in sorted_contours:\n        # 计算轮廓周长\n        peri = cv2.arcLength(c, True)\n        # 近似轮廓  0.02*peri为近似精度\n        approx = cv2.approxPolyDP(c, 0.02*peri, True)\n        \n        # 如果近似轮廓有四个顶点，则认为找到了数字边缘\n        if len(approx)==4 and cv2.contourArea(approx) &gt; 10000 and cv2.contourArea(approx) &lt; 45000:\n            #num_detect.shrink_approx(approx, 3)\n            \n            screenCnt.append(approx)\n            i += 1\n            if i == 2:\n                break\n    # 根据检测到的两个数字的x左坐标来判断左右并排序\n    \n    if len(screenCnt) == 2:\n        num_detect.left_right_sort(screenCnt)\n\n        # print(f&quot;左:&#123;screenCnt[0]&#125;, 右:&#123;screenCnt[1]&#125;&quot;)     \n\n\n    # 在尝试展示轮廓之前检查screenCnt是否已定义\n    if screenCnt != []:\n        cv2.drawContours(frame, screenCnt, -1, (255, 255, 0), 2)\n        postion_count = 0\n        warped_list = []\n        thresh_list = []\n        for c in screenCnt:\n            \n            # 检查轮廓c是否有点（即是否有效）\n            if c.size == 0:\n                print(&quot;找到一个空的轮廓，跳过。&quot;)\n                continue  # 跳过当前轮廓，继续下一个轮廓\n            # 透视变换\n            warped_list.append(num_detect.four_point_transform(orig_frame, c.reshape(4, 2) * ratio))\n            warped_list[postion_count] = cv2.cvtColor(warped_list[postion_count], cv2.COLOR_BGR2GRAY)\n            thresh_list.append(cv2.threshold(warped_list[postion_count], 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1])\n            \n            # 显示透视变换结果\n            cv2.imshow(f&quot;Thresh_&#123;postion_count&#125;&quot;, thresh_list[postion_count])ROI区域首先将图像预处理得到的二值化图像进行重排，变成（150，100）的图像，再进行高斯模糊和腐蚀操作滤除噪点，边缘检测再次提取矩形框边缘，轮廓检测提取矩形框轮廓，最后将整个图像按比例内截，在重排成（100，150）的ROI区域送给下步模板匹配。\n\n\n\n\n\npython\nif roi.size &gt; 0:  # 检查roi是否为空\n    roi = cv2.resize(thresh_list[postion_count], (100, 150))\n    # 定义结构元素\n    kernel = np.ones((5,5), np.uint8)\n\n    roi_blur = cv2.GaussianBlur(roi, (7, 7), 0)\n    \n    roi_edge = cv2.Canny(roi_blur, 80, 200)\n    roi_eroded = cv2.erode(roi_edge, kernel, iterations=2)\n    # 找到边缘的轮廓\n    roi_contours, _ = cv2.findContours(roi_edge, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    for cnt in roi_contours:\n        (x,y,w,h) = cv2.boundingRect(roi_edge)\n    \n    cv2.rectangle(roi, (x, y), (x+w, y+h), (0, 255, 0), 2)\n    cv2.imshow(&quot;roi_edge&quot;,roi_edge)\n    \n    \n    roi = cv2.resize(roi[y:y+h, x:x+w],(100, 150))\n    roi = cv2.resize(roi[0+15:150-15, 0+12:100-12],(100, 150))\n    cv2.imshow(f&quot;ROI&#123;postion_count&#125;&quot;, roi)\nelse:\n    print(&quot;ROI为空，跳过调整大小。&quot;)\n\npostion_count += 1模板匹配将字典里的数字与模板图像取出与识别图像的ROI区域进行模板匹配，找到最适合的数字，再进行个赋值。\npython\n# 初始化模板匹配\nscores = []\n\nfor (digit, digitROI) in digit_dict.items():\n    result = cv2.matchTemplate(roi, digitROI, cv2.TM_CCOEFF) # 模板匹配\n    (_, score, _, _) = cv2.minMaxLoc(result)\n    scores.append(score)\ntrust = False\n# 找到最适合的数字，从scores中找到最大值的索引 + 1     1~8\nfor score in scores:\n    if score &gt; 0.8:\n        trust = True      \nif trust:\n    output.append(str(scores.index(max(scores)) + 1))下位机通信通过串口去传输数据，将左右数字传给下位机MCU进行进一步的控制。\npython\nif output != []:# 如果识别结果不为空\n    if to_detect_num is not None: # 如果这不是第一次检测\n        for i in range(len(output)):\n            cv2.putText(frame, output[i], (screenCnt[i][0][0][0], screenCnt[i][0][0][1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n        if len(output) == 2:\n            print(f&quot;左：&#123;output[0]&#125;，右：&#123;output[1]&#125;&quot;)\n            if(hardware.UART_read(ser)==b&#39;O&#39;):\n                print(f&quot;发送&#123;output[0]&#125;、&#123;output[1]&#125;成功&quot;)\n                hardware.UART_write(ser, f&#39;(&#123;output[0]&#125;&#123;output[1]&#125;)\\r\\n&#39;.encode())\n    else:\n        while(hardware.UART_read(ser)==b&#39;O&#39;):\n            hardware.UART_write(ser, f&#39;(&#123;output[0]&#125;&#123;output[0]&#125;)\\r\\n&#39;.encode())\n            print(f&quot;发送&#123;output[0]&#125;成功&quot;)\n            to_detect_num = int(output[0])\n            print(f&quot;需要识别的数字为 &#123;output[0]&#125;&quot;)\n            print(&quot;等待下位机OK&quot;)\n            start_wait_OK = time.time()\n        \n            if(((start_wait_OK - time.time())*10)%10 &gt; 1):\n                start_wait_OK = time.time()\n                print(&quot;等待超时&quot;)\n                hardware.UART_write(ser, f&#39;(&#123;output[0]&#125;&#123;output[0]&#125;)\\r\\n&#39;.encode())\n                print(f&quot;发送&#123;output[0]&#125;成功&quot;)识别效果仅部分图片，有待补充\n\n\n\n\n\n\n  \n\n\n\n","slug":"树莓派数字识别——openCV方法","date":"2024-07-22T15:50:17.000Z","categories_index":"MashineLearning","tags_index":"python,openCV","author_index":"Yesord"},{"id":"714f64421d876371e3d936b45ca967be","title":"Jetson Nano部署Yolo模型笔记","content":"前言本人在大二期间就学习了计算机视觉相关的模型搭建、训练和预测。大致了解目标检测的训练流程，却没有去实际在端设备上进行过模型的部署。在准备电赛期间，拿到 jetson nano 开始玩一玩模型部署，就有了这篇博客。\n\n项目前置\n\n\n\n\n\n \n深度模型YOLOv8(pytorch框架)\n深度模型部署框架infer (基于TensorRT和onnx模型的框架)\n\n\n项目主体\n\n\n\n\n\n\n \n使用YOLOv8对数字进行识别\n\n\n整体流程\n\n\n\n\n\n\nWARNING\n电脑上进行YOLOv8模型训练—&gt; 输出.pt模型—&gt; 转换成onnx模型(通用的模型)—&gt; 在infer框架下转换成.engine—&gt; 运行.engine开始运行\n\n\n模型训练\n\n\n\n\n\nTIP\n在PC端或是GPU服务器上进行会比较顺利。\n\n\n下载YOLOv8YOLOv8_GitHub库\nbash# 使用命令行\ngit clone https://github.com/ultralytics/ultralytics.gitpython安装ultralytics库（推荐在conda上进行）\nbashpip install ultralytics安装其他依赖库\nbash# 在ultralytics的仓库里找到requirements.txt,注意当前工作目录（转移到requirements.txt所在目录）\npip install -r requirements.txt数据集获取\n\n\n\n\n\nTIP\n根据最终需要识别的目标，可以去网上找现成的数据集\n\n\n在这里推荐查找数据集的两个网站\n\nKaggle\n\n\n\nroboflow Universe\n\n\n\n\n数据集采集（可选）\n\n\n\n\n\nTIP\n也可以自己去做数据集，像本项目就是自己去采的数据集。\n\n\n本项目\n总共400张，通过写opencv的脚本去实现采集。\n\n\n\n\n\n\n\n\nWARNING\n这里提供一个可以通过摄像头去获取图片的脚本auto_get_img.py\n\npythonimport cv2 \nimport os\nimport os.path as osp\nimport numpy as np\n# 保存目录需要自己去设置\nscript_dir = osp.dirname(__file__)\nimg_dir = osp.join(script_dir, &#39;image&#39;)\nsave_dir = osp.join(img_dir, &#39;captured_image_dir&#39;)\n\n# 创建保存目录,如果不存在\nif not osp.exists(save_dir):\n    os.makedirs(save_dir)\n    print(f&quot;保存目录&#123;save_dir&#125;已创建&quot;)\nelse:\n    print(f&quot;保存目录&#123;save_dir&#125;已存在&quot;)\n# 创建视频捕获对象\ncap = cv2.VideoCapture(0)\n\nif not cap.isOpened():\n    print(&quot;无法打开摄像头&quot;)\n    exit()\ntimes = int(input(&quot;请输入需要截取几个图片\\n&quot;))\ni = 0\nwhile True:\n    # 读取一帧\n    ret, frame = cap.read()\n    if ret:\n        # 显示图像\n        cv2.imshow(&#39;frame&#39;, frame)\n        \n        # 保存图像\n        if cv2.waitKey(1) &amp; 0xFF == ord(&#39;s&#39;): # 按s保存图片\n            cv2.imwrite(osp.join(save_dir , f&quot;img_&#123;i&#125;.jpg&quot;), frame)\n            print(f&quot;图像已保存为 img_&#123;i&#125;.jpg&quot;)\n            times -= 1\n            i += 1\n            \n        \n    else:\n        print(&quot;无法读取摄像头图像&quot;)\n    if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;) or times == 0:  # 按q退出\n        break\n# 释放摄像头\ncap.release()\n# 关闭所有OpenCV窗口\ncv2.destroyAllWindows()数据集的标定（可选）本项目使用的是精灵标注助手，使用labelme之类的标注工具也是可以的，我们的目的是对数据集图片打标，对我们感兴趣的区域进行标注，画出bondnigbox。我们做的是目标检测detect，只画矩形框即可。\n\n大致如图\n\n\n\nXML转txt(YOLO数据集格式)XML是labelme标注完之后生成的文件格式，详细可以去google看看。Yolo接受的数据集格式是txt，详细去google。\n\n\n\n\n\n\n\nWARNING\n这里提供一个XML转txt的脚本xml_to_txt.py\n\npython\n#########################\n# xml2txt.py\n#########################\n\nimport xml.etree.ElementTree as ET\nimport os, cv2\nimport numpy as np\nfrom os import listdir\nfrom os.path import join\n\nclasses = [&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;,&#39;6&#39;,&#39;7&#39;,&#39;8&#39;] # 写入你做目标检测的物体的分类\n\n\ndef convert(size, box):\n    dw = 1. / (size[0])\n    dh = 1. / (size[1])\n    x = (box[0] + box[1]) / 2.0 - 1\n    y = (box[2] + box[3]) / 2.0 - 1\n    w = box[1] - box[0]\n    h = box[3] - box[2]\n    x = x * dw\n    w = w * dw\n    y = y * dh\n    h = h * dh\n    return (x, y, w, h)\n\n\ndef convert_annotation(xmlpath, xmlname):\n    with open(xmlpath, &quot;r&quot;, encoding=&#39;utf-8&#39;) as in_file:\n        txtname = xmlname[:-4] + &#39;.txt&#39;\n        txtfile = os.path.join(txtpath, txtname)\n        tree = ET.parse(in_file)\n        root = tree.getroot()\n        filename = root.find(&#39;filename&#39;)\n        img = cv2.imdecode(np.fromfile(&#39;&#123;&#125;/&#123;&#125;.&#123;&#125;&#39;.format(imgpath, xmlname[:-4], postfix), np.uint8), cv2.IMREAD_COLOR)\n        h, w = img.shape[:2]\n        res = []\n        for obj in root.iter(&#39;object&#39;):\n            cls = obj.find(&#39;name&#39;).text\n            if cls not in classes:\n                classes.append(cls)\n            cls_id = classes.index(cls)\n            xmlbox = obj.find(&#39;bndbox&#39;)\n            b = (float(xmlbox.find(&#39;xmin&#39;).text), float(xmlbox.find(&#39;xmax&#39;).text), float(xmlbox.find(&#39;ymin&#39;).text),\n                 float(xmlbox.find(&#39;ymax&#39;).text))\n            bb = convert((w, h), b)\n            res.append(str(cls_id) + &quot; &quot; + &quot; &quot;.join([str(a) for a in bb]))\n        if len(res) != 0:\n            with open(txtfile, &#39;w+&#39;) as f:\n                f.write(&#39;\\n&#39;.join(res))\n\n\nif __name__ == &quot;__main__&quot;:\n    postfix = &#39;jpg&#39;\n    imgpath = &#39;xxx&#39; # 图片存储位置\n    xmlpath = &#39;xxx&#39; # xml存储位置\n    txtpath = &#39;xxx&#39; # txt存储位置\n\n    if not os.path.exists(txtpath):\n        os.makedirs(txtpath, exist_ok=True)\n\n    list = os.listdir(xmlpath)\n    error_file_list = []\n    for i in range(0, len(list)):\n        try:\n            path = os.path.join(xmlpath, list[i])\n            if (&#39;.xml&#39; in path) or (&#39;.XML&#39; in path):\n                convert_annotation(path, list[i])\n                print(f&#39;file &#123;list[i]&#125; convert success.&#39;)\n            else:\n                print(f&#39;file &#123;list[i]&#125; is not xml format.&#39;)\n        except Exception as e:\n            print(f&#39;file &#123;list[i]&#125; convert error.&#39;)\n            print(f&#39;error message:\\n&#123;e&#125;&#39;)\n            error_file_list.append(list[i])\n    print(f&#39;this file convert failure\\n&#123;error_file_list&#125;&#39;)\n    print(f&#39;Dataset Classes:&#123;classes&#125;&#39;)转换得到的文件要做成类似如下文件结构\nbash——dataset\n|\n|\n————train—————images\n|       |\n|       ——————labels\n|\n————test——————images\n|       |\n|       ——————labels\n|\n————val———————images\n        |\n        ——————labels\n配置数据集在yolo中通过yaml文件来对数据集进行管理和读取，在训练时调用数据集就是看yaml的。\n以下提供一个简单的yaml文件的范例，建议把yaml文件和dataset放在同一个文件夹\nyaml\npath: F:\\AI\\Yolov8\\Project\\datasets\\number\\dataset #数据集的位置\ntrain: images/train # train图片的文件夹与数据集path的相对位置\nval: images/val # val图片的文件夹与数据集path的相对位置\n\nnc: 8 # 目标检测物体类的数目 \nnames: [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;] # 物体对应的名称\nYOLO模型训练通过yolo官方提供的库进行训练，选取yolov8n.pt作为预训练模型。yolov8n是规模最小的模型，之后部署在jetson nano上运行会相对比较流畅。\n\n\n\n\n\n\n\nWARNING\n这里提供一个训练脚本train.py\n\npythonfrom ultralytics import YOLO\nimport os.path as osp\nroot_dir = osp.dirname(osp.dirname(osp.abspath(__file__))) #&lt;-- absolute dir the script is in\nscript_dir = osp.dirname(__file__) #&lt;-- absolute dir the script is in\nsave_dir = osp.join(root_dir, &#39;exports/runs/train/exp&#39;) # 数据会保存到../exports/runs/train/exp\nif not osp.exists(save_dir):\n    os.makedirs(save_dir)\ndataset_dir = osp.join(root_dir, &#39;datasets&#39;) # 数据集存放文件夹位置\n\nif __name__ == &#39;__main__&#39;: \n    model = YOLO(f&quot;&#123;root_dir&#125;//yolov8n.pt&quot;)  # 改成你的模型的存放地址\n    # YOLO(&quot;model.pt&quot;)  use pre-trained model if available\n    model.info()  # 展示模型信息\n    model.train(task = &#39;detect&#39;, data= dataset_dir +&quot;\\\\number\\\\num_detection.yaml&quot;, epochs=200, workers=2, save_dir=save_dir)  # 进行模型训练\n    # task      -&gt;  表示任务 detect表示做目标检测\n    # data      -&gt;  表示数据集yaml文件的存放位置\n    # epochs    -&gt;  表示训练的轮数\n    # workers   -&gt;  表示装载时CPU的线程数\n    # save_dir  -&gt;  表示训练出的模型的保存路径\n模型验证训练出模型之后我们需要检验一下模型的识别效果，可以去runs里看混淆矩阵或召回率之类的，不过这不是我这篇博客的重点。\n\n\n\n\n\n\n\nWARNING\n我在这提供一个摄像头的验证demo\n\npython\nimport os.path as osp\nimport cv2\nfrom ultralytics import YOLO\nfrom cv2 import getTickCount, getTickFrequency\n# 加载 YOLOv8 模型\nscript_dir = osp.dirname(osp.abspath(__file__)) #&lt;-- absolute dir the script is in\nparent_dir = osp.dirname(script_dir)\ngrandpa_dir = osp.dirname(parent_dir)\nmodel_path = osp.join(grandpa_dir, &#39;runs/detect/train/weights/best.pt&#39;)\nmodel = YOLO(f&quot;&#123;model_path&#125;&quot;)\n\n# 获取摄像头内容，参数 0 表示使用默认的摄像头\ncap = cv2.VideoCapture(0)\n\nwhile cap.isOpened():\n    loop_start = getTickCount()\n    success, frame = cap.read()  # 读取摄像头的一帧图像\n\n    if success:\n        results = model.predict(source=frame) # 对当前帧进行目标检测并显示结果\n    annotated_frame = results[0].plot()\n\n    # 中间放自己的显示程序\n    loop_time = getTickCount() - loop_start\n    total_time = loop_time / (getTickFrequency())\n    FPS = int(1 / total_time)\n    # 在图像左上角添加FPS文本\n    fps_text = f&quot;FPS: &#123;FPS:.2f&#125;&quot;\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    font_scale = 1\n    font_thickness = 2\n    text_color = (0, 0, 255)  # 红色\n    text_position = (10, 30)  # 左上角位置\n\n    cv2.putText(annotated_frame, fps_text, text_position, font, font_scale, text_color, font_thickness)\n    cv2.imshow(&#39;img&#39;, annotated_frame)\n    # 通过按下 &#39;q&#39; 键退出循环\n    if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):\n        break\n\ncap.release()  # 释放摄像头资源\ncv2.destroyAllWindows()  # 关闭OpenCV窗口模型导出yolo是基于pytorch深度学习框架的一种视觉模型，经过训练产生的模型是后缀.pth的pytorch模型，是无法直接在端侧（非pytorch框架下）直接运行的。因此我们需要将其转换成其他类型的模型文件。一般pytorch模型都是导出成onnx模型（一种相对通用的模型）。\n\n\n\n\n\n\n\nWARNING\n这里提供一个pth转onnx的脚本pth_to_onnx.py\n\npython\nimport os.path as osp\nfrom ultralytics import YOLO\n\nscript_dir = osp.dirname(osp.abspath(__file__)) #&lt;-- absolute dir the script is in\nparent_dir = osp.dirname(script_dir)\nroot_dir = osp.dirname(parent_dir)\nmodel_path = osp.join(root_dir, &#39;runs\\\\detect\\\\train\\\\weights\\\\best.pt&#39;) # 更改成你的模型存放位置 \n\nmodel = YOLO(model_path)\n\nsuccess = model.export(format=&quot;onnx&quot;, batch=1, save_dir=osp.join(parent_dir, &#39;exports&#39;))#导出为onnx模型\n\n# 静态batch 输入batch张图片作batch个的推理\n# success = model.export(format=&quot;onnx&quot;, dynamic=Ture)\n# 动态batch 输入多少张图片作多少张推理 server时使用(服务器)\n# 此处导出的模型为静态batch\n将生成的onnx模型文件上传到jetson nano上，在上位机上的模型训练工作我们就完成了。\n模型部署本次模型部署使用的是infer的模型部署框架详细操作可以参考其GitHub库。\n首先我们在github上拉取infer的仓库\nbashgit clone https://github.com/shouxieai/infer.git\ninfer框架的文件结构\n\nsrc 源文件夹，存放最终我们cmake的c++文件，进行检测的代码workspace 存放我们的onnx模型和提供一个节点v8格式到infer格式转换脚本v8trans.pyworkspace&#x2F;inference 存放我们的待检测的图片\n\n需要在配置trtexec工具的环境变量\n打开.bashrc文件\nbashnano ~/.bashrc在最后一行添加如下命令\nbashexport PATH=/usr/src/tensorrt/bin:$PATH按下ctrl+x y Enter保存并退出\n刷新一下环境变量\nbashsource ~/.bashrconnx模型转换将onnx模型转移到infer的workspace文件夹中，将当前工作路径移动到workspace文件夹中，执行infer提供的onnx转换到infer框架下的onnx的脚本v8trans.py\npythonpython v8trans.py xx.onnx运行后会在当前工作目录下生成一个xx.trans.onnx模型\nonnx模型优化onnxoptimizer、onnxsim被誉为onnx的优化利器，其中onnxsim可以优化常量，onnxoptimizer可以对节点进行压缩\n\n\n\n\n\n\n\nWARNING\n这里提供一个onnx模型优化脚本v8onnxsim.py\n\npython\nimport onnx\nfrom onnxsim import simplify\n\nonnx_model = onnx.load(&quot;xx.transd.onnx&quot;)\nmodel_simp, check = simplify(onnx_model)\nassert check, &quot;Simplified ONNX model could not be Validated&quot;\nonnx.save(model_simp, &quot;xx.transd.sim.onnx&quot;)\n执行完成会在当前工作目录生成一个xx.transd.sim.onnx模型，其在端侧运行的能效会高一些。\n生成engineinfer框架可以直接通过trtexec工具直接生成engine文件。比较简单，没有复杂的封装，适合初学者。\n终端使用trtexec工具直接构建engine\nbashtrtexec --onnx=workspace/xx.transd.sim.onnx --saveEngine=workspace/xx.transd.sim.engine\ntrtexec的扩展应用\ntrtexec是NVIDIA TensorRT SDK中的一个实用工具，它允许用户从命令行轻松运行和测试TensorRT引擎。trtexec命令行工具可以使用以下参数：\nbash./trtexec [-h] [--uff model.uff [model.uff ...]] [--onnx model.onnx] [--model=model.plan] [--deploy=&lt;deploy.prototxt&gt;] [--output=&lt;output_name&gt;] [--batch=N] [--device=N] [--workspace=N] [--fp16] [--int8] [--calib=&lt;dir&gt;] [--useDLA=N] [--allowGPUFallback] [--iterations=N] [--avgRuns=N] [--verbose] [--nshapes=N] [--optShapes=NxN ... NxN] [--minShapes=NxN ... NxN] [--maxShapes=NxN ... NxN] [--shapeInput=&#123;0,1,2,...&#125;] [--tacticSources] [--loadEngine=&lt;filename&gt;] [--saveEngine=&lt;filename&gt;] [--plugins=&lt;XML file&gt;] [--dumpOutput=&lt;filename&gt;] [--excludePlugin=&lt;name&gt;] [--start=&lt;tag&gt;] [--streams=N] [--batchTile] [--engine=&lt;filename&gt;] [--uffInput=input_name,input_shape] [--uffNHWC] [--uffNCHW] [--uffHW=&lt;value&gt;] [--workspaceSize=&lt;size&gt;] [--buildOnly] [--engineFormat=&lt;format&gt;] [--refit] [--saveRefinedEngine=&lt;filename&gt;] [--calibrator=&lt;classname&gt;] [--detach] [--check] [--fp16Char] [--int8Calib=&lt;dir&gt;] [--int8IO] [--disableTensorCores] [--useSpinWait] [--shapes=&lt;shape&gt;] [--maxBatch=&lt;size&gt;]\n\n其中一些重要的参数如下：\n\n--uff：指定输入为UFF模型，后面跟上模型文件的路径。\n--onnx：指定输入为ONNX模型，后面跟上模型文件的路径。\n--model：指定输入为序列化的引擎文件，后面跟上文件路径。\n--deploy：指定输入为Caffe deploy文件的路径。\n--output：指定输出Tensor名称。\n--batch：指定执行推理时每个batch的大小，默认为1。\n--device：指定执行推理的设备编号，默认为0。\n--workspace：指定GPU内存的最大使用量，默认为1GB。\n--fp16：启用FP16精度，可提高推理性能和减少内存使用。\n--int8：启用INT8精度，可进一步提高推理性能和减少内存使用。\n--calib：指定INT8校准数据集的路径。\n--useDLA：指定使用哪个DLA，以及在DLA上运行哪些层。\n--allowGPUFallback：如果使用DLA，当某些层无法在DLA上运行时，是否允许将其回退到GPU。\n--iterations：指定测试迭代次数。\n--avgRuns：指定平均运行次数。\n--verbose：打印更详细的输出信息。\n--loadEngine：指定加载的TensorRT引擎文件，后面跟上文件路径\n--saveEngine：指定生成的TensorRT引擎文件，后面跟上文件路径\n\n\nengine构建完成之后就可以开始运行了\n开始检测实现视频流的实时目标检测，需提前给jetson nano装上USB摄像头。\n\n\n\n\n\n\n\nWARNING\nMakefile文件修改\n\nmakefilecc        := g++\nnvcc      = /usr/local/cuda-10.2/bin/nvcc\n\ncpp_srcs  := $(shell find src -name &quot;*.cpp&quot;)\ncpp_objs  := $(cpp_srcs:.cpp=.cpp.o)\ncpp_objs  := $(cpp_objs:src/%=objs/%)\ncpp_mk\t  := $(cpp_objs:.cpp.o=.cpp.mk)\n\ncu_srcs\t  := $(shell find src -name &quot;*.cu&quot;)\ncu_objs   := $(cu_srcs:.cu=.cu.o)\ncu_objs\t  := $(cu_objs:src/%=objs/%)\ncu_mk\t  := $(cu_objs:.cu.o=.cu.mk)\n\ninclude_paths := src        \\\n            /usr/include/opencv4 \\\n            /usr/include/aarch64-linux-gnu \\\n            /usr/local/cuda-10.2/include\n\nlibrary_paths := /usr/lib/aarch64-linux-gnu \\\n            /usr/local/cuda-10.2/lib64\n\nlink_librarys := opencv_core opencv_highgui opencv_imgproc opencv_videoio opencv_imgcodecs \\\n            nvinfer nvinfer_plugin nvonnxparser \\\n            cuda cublas cudart cudnn \\\n            stdc++ dl\n\nempty\t\t  :=\nexport_path   := $(subst $(empty) $(empty),:,$(library_paths))\n\nrun_paths     := $(foreach item,$(library_paths),-Wl,-rpath=$(item))\ninclude_paths := $(foreach item,$(include_paths),-I$(item))\nlibrary_paths := $(foreach item,$(library_paths),-L$(item))\nlink_librarys := $(foreach item,$(link_librarys),-l$(item))\n\ncpp_compile_flags := -std=c++11 -fPIC -w -g -pthread -fopenmp -O0\ncu_compile_flags  := -std=c++11 -g -w -O0 -Xcompiler &quot;$(cpp_compile_flags)&quot;\nlink_flags        := -pthread -fopenmp -Wl,-rpath=&#39;$$ORIGIN&#39;\n\ncpp_compile_flags += $(include_paths)\ncu_compile_flags  += $(include_paths)\nlink_flags        += $(library_paths) $(link_librarys) $(run_paths)\n\nifneq ($(MAKECMDGOALS), clean)\n-include $(cpp_mk) $(cu_mk)\nendif\n\npro\t   := workspace/pro\nexpath := library_path.txt\n\nlibrary_path.txt : \n    @echo LD_LIBRARY_PATH=$(export_path):&quot;$$&quot;LD_LIBRARY_PATH &gt; $@\n\nworkspace/pro : $(cpp_objs) $(cu_objs)\n        @echo Link $@\n        @mkdir -p $(dir $@)\n        @$(cc) $^ -o $@ $(link_flags)\n\nobjs/%.cpp.o : src/%.cpp\n    @echo Compile CXX $&lt;\n    @mkdir -p $(dir $@)\n    @$(cc) -c $&lt; -o $@ $(cpp_compile_flags)\n\nobjs/%.cu.o : src/%.cu\n    @echo Compile CUDA $&lt;\n    @mkdir -p $(dir $@)\n    @$(nvcc) -c $&lt; -o $@ $(cu_compile_flags)\n\nobjs/%.cpp.mk : src/%.cpp\n    @echo Compile depends CXX $&lt;\n    @mkdir -p $(dir $@)\n    @$(cc) -M $&lt; -MF $@ -MT $(@:.cpp.mk=.cpp.o) $(cpp_compile_flags)\n    \nobjs/%.cu.mk : src/%.cu\n    @echo Compile depends CUDA $&lt;\n    @mkdir -p $(dir $@)\n    @$(nvcc) -M $&lt; -MF $@ -MT $(@:.cu.mk=.cu.o) $(cu_compile_flags)\n\nrun   : workspace/pro\n          @cd workspace &amp;&amp; ./pro\n\nclean :\n    @rm -rf objs workspace/pro\n    @rm -rf library_path.txt\n    @rm -rf workspace/Result.jpg\n\n# 导出符号，使得运行时能够链接上\nexport LD_LIBRARY_PATH:=$(export_path):$(LD_LIBRARY_PATH)\n\n\n\n\n\n\nWARNING\n源码修改，在main.cpp中进行添加与改动\n\ncppstatic void yolo_video_demo(const string&amp; engine_file)&#123;\t\t// 修改1 新增函数\n  auto yolo = yolo::load(engine_file, yolo::Type::V8);\n  if (yolo == nullptr)  return;\n  \n  // auto remote_show = create_zmq_remote_show();\n\n  cv::Mat frame;\n  cv::VideoCapture cap(0);\n  if (!cap.isOpened())&#123;\n    printf(&quot;Engine is nullptr&quot;);\n    return;\n  &#125;\n\n  while(true)&#123;\n    cap.read(frame);\n    auto objs = yolo-&gt;forward(cvimg(frame));\n    \n    for(auto &amp;obj : objs) &#123;\n      uint8_t b, g, r;\n      tie(b, g, r) = yolo::random_color(obj.class_label);\n      cv::rectangle(frame, cv::Point(obj.left, obj.top), cv::Point(obj.right, obj.bottom),\n                    cv::Scalar(b, g, r), 5);\n      \n      auto name = mylabels[obj.class_label];\n      auto caption = cv::format(&quot;%s %.2f&quot;, name, obj.confidence);\n      int width = cv::getTextSize(caption, 0, 1, 2, nullptr).width + 10;\n      cv::rectangle(frame, cv::Point(obj.left - 3, obj.top - 33),\n                    cv::Point(obj.left + width, obj.top), cv::Scalar(b, g, r), -1);\n      cv::putText(frame, caption, cv::Point(obj.left, obj.top - 5), 0, 1, cv::Scalar::all(0), 2, 16);\n    &#125;\n      imshow(&quot;frame&quot;, frame);\n      // remote_show-&gt;post(frame);\n      int key = cv::waitKey(1);\n      if (key == 27)\n          break;\n  &#125;\n\n  cap.release();\n  cv::destroyAllWindows();\n  return;\n&#125;\n\nint main() &#123;\t// 修改2 调用该函数\n  // perf();\n  // batch_inference();\n  // single_inference();\n  yolo_video_demo(&quot;best.transd.sim.engine&quot;);\n  return 0;\n&#125;识别效果肉眼估计能有个10几20来帧，对于Jetson Nano这种老硬件来说已经挺不错的了。\n\n\nshow more\n\n\n","slug":"Jetson-Nano部署Yolo模型笔记","date":"2024-07-22T15:34:15.000Z","categories_index":"Electronic","tags_index":"jetson nano,ComputerVision","author_index":"Yesord"},{"id":"4e554f9d434ef086687ddd88dc6d7f86","title":"树莓派搭建MC服务器","content":"硬件配置树莓派4B：树莓派系统\n下载java根据你希望搭建的MC版本来选择java版本\n\n\n\n\n\n\n\nWARNING\njava8 : MC &lt;&#x3D; 1.16.5java17: MC &gt;&#x3D; 1.16.5\n\n搭建MCSmanager面板我们使用MCSmanager面板来对我们的服务器进行管理\nbashwget -qO- https://gitee.com/mcsmanager/script/raw/master/setup.sh | sudo bash启动面板\nbash\n# 先启动面板守护进程。\n# 这是用于进程控制，终端管理的服务进程。\nsystemctl start mcsm-daemon.service\n# 再启动面板 Web 服务。\n# 这是用来实现支持网页访问和用户管理的服务。\nsystemctl start mcsm-web.service\n\n# 重启面板命令\nsystemctl restart mcsm-daemon.service\nsystemctl restart mcsm-web.service\n\n# 停止面板命令\nsystemctl stop mcsm-web.service\nsystemctl stop mcsm-daemon.service\n\n# 面板启用开机自启的命令\nsystemctl enable mcsm-web.service\nsystemctl enable mcsm-daemon.service\n\n# 面板禁用开机自启的命令\nsystemctl disable mcsm-web.service\nsystemctl disable mcsm-daemon.service进入面板在同一局域网的电脑上输入树莓派服务器ip:23333端口就可以进入面板，按照实例教程就可以进行服务器的创建。\n\nMore\n\n\n\n下载MC servertxt这个网站提供MC的各个版本的jar文件，注意要下载server版本的。\n\nMC server下载forge下载\n要将对应的jar文件上传到树莓派上，通过MSCM就可以实现\n安装installer直接通过MSCM中就可以安装，但是注意要提前装好java。\n\n\n安装完成后的文件\n\n启动server端\n\n根据不同版本要填不同的启动命令，像我下的forge 1.20.1就直接默认就行\n\n\n\n\n第一次启动可能不成功，要去eula.txt里面将false改为true\n\n\n\n至此服务器搭建完毕，之后要加入mod就直接在MCSM上加入就好了。\n","slug":"树莓派搭建MC服务器","date":"2024-04-20T01:07:29.000Z","categories_index":"game","tags_index":"raspberrypi","author_index":"Yesord"},{"id":"0c01823a69be2694393fe22da12fd1e6","title":"Zerotier赋能Parsec——低延迟高分辨率远程办公解决方案","content":"前言用过todesk或向日葵免费版的都知道，虽然他俩可以实现远程连接，但是延迟和画质真的不敢恭维，本文提供了一个解决方案，适合愿意折腾的windows用户实现高分辨率低延迟的远程办公，最好手里正好有云服务器。\n准备环境购买云服务器可以选择阿里云或腾讯云，国内这两家最大，1核1G就够，本方案主要图的是公网ip，对服务器带宽没有要求。如果额外有建站需求的可以考虑Amazon等的香港云服务器（不用备案）。为了追求更低的延迟可以选择距离自己比较近的云服务器。尽量选择Ubuntu或Debian镜像，如选其他镜像，命令可能会有所出入。\n安装ZerotierZerotier的基础配置\n\n\n\n\n\nTIP\n参考本博主以前的文章\n\n\n\n\n\n\n\n\n\nWARNING\n在需要远程的设备中都要下载\n\n\nZerotier Moon服务器搭建云服务器终端输入如下命令\n安装Zerotier\nbashcurl -s https://install.zerotier.com | sudo bash 生成 moon.json 签名文件\nbashzerotier-idtool initmoon /var/lib/zerotier-one/identity.public &gt;&gt; /var/lib/zerotier-one/moon.json编辑moon.json\nbashnano /var/lib/zerotier-one/moon.json将你的公网IP添加到”stableEndpoints”: [] 中，如 “stableEndpoints”: [ “132.313.11.313&#x2F;9993”] ，服务器9993端口防火墙也要打开。\n生成 .moon 签名文件(0000xxxxxxxxx.moon)\nbashzerotier-idtool genmoon moon.json创建moons.d存放.moon文件\nbashmkdir /var/lib/zerotier-one/moons.d\nmv /var/lib/zerotier-one/0000xxxxxx.moon /var/lib/zerotier-one/moons.d重启zerotier服务\nbashsystemctl restart zerotier-one.service查看Moon服务器id\nbashzerotier-cli info 找到Moon结尾的那一行，对应的就是Moon服务器id，也可以去zerotier center直接查看member\nmoon服务器搭建完毕，接下来要其他端接入moon服务器。\nwindows端cmd中输入命令\n将设备接入Moon服务器\nbash zerotier-cli.bat orbit &lt;Moon服务器的id&gt; &lt;Moon服务器的id&gt;Moon中转服务器搭建完成\n如果上述方法不行，还可以将.moon文件从云服务器上下载下来，去zerotier安装目录，windows默认C:\\ProgramData\\ZeroTier\\One，创建moons.d文件夹，将.moon文件移动进去再重启zerotier服务也可实现接入moon服务器。\n更换节点控制器zerotier的虚拟局域网实际上的运作如图正常情况下，终端设备A通过Planet根服务器与终端设备B连接，但是节点是有唯一节点ID的，一个虚拟局域网要确认节点ID才能保证A能连接到B。这个时候就需要节点控制器来告诉Planet根服务器，A提供的虚拟ip对应的节点ID是B的，这样A才能连接到B。有时候，你搭建的Moon服务器不一定能连接到节点控制器，此时就算A、B都连接到Moon服务器，Moon中转也是走不通的。为了解决上面这种情况，我们可以将Moon服务器本身变成节点控制器，让它中转设备的同时可以解析节点。\n这里用到一个开源项目ztncui\n在云服务器端输入命令安装deb\nbashcurl -O https://s3-us-west-1.amazonaws.com/ket-networks/deb/ztncui/1/x86_64/ztncui_0.8.14_amd64.deb\nsudo apt install ./ztncui_0.8.14_amd64.deb查看zerotier密钥\nbashsudo cat /var/lib/zerotier-one/authtoken.secret复制cat的内容到下面单引号’ ‘内\nbashsh -c &quot;echo ZT_TOKEN=&#39;sudo cat /var/lib/zerotier-one/authtoken.secret&#39;&gt;/opt/key-networks/ztncui/.env&quot;bashsh -c &quot;echo HTTP_ALL_INTERFACES=yes &gt;&gt; /opt/key-networks/ztncui/.env&quot;bashsh -c &quot;echo NODE_ENV=production &gt;&gt; /opt/key-networks/ztncui/.env&quot;设置.env的权限\nbashchmod 400 /opt/key-networks/ztncui/.env更改.env的所有者\nbashchown ztncui.ztncui /opt/key-networks/ztncui/.env启动ztncui\nbashsystemctl enable ztncui重启ztncui\nbashsystemctl restart ztncui在服务器防火墙处打开3000端口，在浏览器中输入公网ip:3000，打开云服务器的3000端口，显示如图即为成功右上角login，默认账户admin，密码password\n创建新网络\neasy setup 快速配置\n选择网段，submit\n将设备加入网络\n如果需要保证这个管理界面安全性可以去上SSL证书。\n安装parsec\n\n\n\n\n\nTIP\nparsec官网\n\n\n需要在你希望远程办公的设备上都下载客户端(只支持windows用户)\n打开如下界面，如果配完网出现要远程的设备ID就是成功了\n","slug":"Zerotier赋能Parsec-低延迟高分辨率远程办公Solution","date":"2024-03-06T07:12:15.000Z","categories_index":"IT","tags_index":"solution","author_index":"Yesord"},{"id":"913a88b1308a2ce8d0499d200801961e","title":"Zerotier——内网穿透神器 for free","content":"Zerotier简介zerotier是一个异地组网的solution，可以进行内网穿透从而实现远程办公，将公司、学校、家里的设备牢牢控制在自己手心里。\nZerotier安装\n\n\n\n\n\nTIP\nzerotier官网\n\n\nzerotier center配置在zerotier官网打开Get ZeroTier登录账户后会显示如下界面,点击Create A Network新建一个虚拟局域网Base就是一些基础信息，要记住Network ID，这之后其他设备接入该虚拟局域网要用比较重要的是这个member，你之后往zerotier这个虚拟局域网里加入其他设备需要在这里授权如果有中转服务器，需要在这里添加路由，这样才能使你的设备连接通过这个中转服务器\nzerotier 常用命令(Linux)1.安装zerotier\nbashcurl -s https://install.zerotier.com | sudo bash 2.启动Zerotier\nbashsudo systemctl start zerotier-one.service 3.重启Zerotier\nbashsudo systemctl restart zerotier-one.service 4.设置开机自启动Zerotier\nbashsudo systemctl enable zerotier-one.service 5.查看服务状态\nbashsudo zerotier-cli status   or\nbashsudo zerotier-cli info6.加入网络 \nbashzerotier-cli join &lt;16位虚拟网络ID&gt;7.离开网络 \nbashzerotier-cli leave &lt;16位虚拟网络ID&gt;8.查看所有的网络 \nbashzerotier-cli listnetworks 9.显示详细信息\nbashsudo zerotier-cli listpeers10.授权设备\nbashsudo zerotier-cli orbit &lt;16位虚拟网络ID&gt; &lt;16位要授权的设备的唯一标识符&gt;zerotier使用(windows)拥有图形界面，是个人应该都会用。\n简单介绍一下如何添加设备到你的虚拟局域网\n点击join new network,然后输入你的虚拟网络ID(16位码)，在去zerotier center确认授权即可\n\n\n\n\n\n\n\nWARNING\nzerotier windows端下载好了之后会自动缩到任务栏，要在任务栏操作\n\n\nzerotier 效果可以通过zerotier虚拟局域网下提供的虚拟ip地址去进行不同设备之间的通信，从而实现内网穿透。如下我配置了两个设备进入我的虚拟局域网，这两个设备现实中不在同一个局域网内在cmd中ping另个设备可以看到已经可以ping通了，延迟还可以，其实是我配了中转服务器的缘故正常情况如果不能P2P直连的话就会走zerotier的免费planet根服务器，但是zerotier的planet根服务器是部署在国外的，这一来一去延迟就会超级高，大概500ms往上，几乎到了无法使用的地步，因此强烈建议搭一个zerotier的moon中转服务器（我之后会再出教程如何配置zerotier的moon中转服务器，并实现高分辨率低延迟远程办公）。\n","slug":"Zerotier——内网穿透神器-for-free","date":"2024-03-05T15:26:29.000Z","categories_index":"IT","tags_index":"tool","author_index":"Yesord"},{"id":"7ef338d136f909941106f7733815b8c9","title":"基于运放的信号发生电路总结","content":"前言最近在导师那里接了一个做蓄电池检测仪的项目，在测量电池内阻时需要向电池打一个稳定的小正弦交流，因此开始研究如何使用运算放大器实现正弦信号的发生和最终的电流输出。\n正弦信号发生电路正弦信号作为最基本的测试信号、参考信号和载波信号而被广泛使用。尽管正弦波本身十分简单，但是产生正弦波的电路可没有那么简单。使用运算放大器来搭建正弦信号发生电路最适合的就是文氏电桥振荡器和正交振荡器。下面将给出二者的实际电路实现.\n文氏电桥振荡电路基本文氏电桥振荡器\n\n\n\n\n\n\n\n\nTIP\n\n虚短虚断之后就是一个电桥电路\n运放正反馈放大信号经过RC高通滤波和RC低通滤波构成的带通滤波器，达到选频的效果，通过f&#x3D;1&#x2F;2πRC，计算出最后的通频段。\n运放负反馈≈同向放大器，将每次经过选频之后的信号进行放大，最后稳定。\n巴克豪森准则：当电子振荡器系统的信号从输入到输出再到反馈输入的相位差为360度，并且增益为1时，这是振荡器振荡的必要条件。由此可以得到文氏电桥振荡电路的起振条件: R3&#x2F;R4 &#x3D; 2 (有点偏差没问题，&#x3D;2时振荡效果最好)\n\n\n\n仿真结果\n\n\n\n\n\n\n\nWARNING\n通道1为输出Vo通道2为运放同向端Vp大概可以得到Vo&#x2F;Vp &#x3D; 3起振时间大约100ms产生正弦波频率：f&#x3D;1&#x2F;(2π158kΩ1nF)≈1kHz\n\n\n拓展文氏电桥振荡器\n\n\n\n\n\n\nTIP\n采用二级管控制负反馈端的电阻阻值稳定，保证生成正弦波的稳定性\n\n\n\n\n\n\n\n\n\nTIP\n利用限幅器来稳定幅度的文氏电桥振荡电路\n\n\n正交振荡器\n嗨嗨嗨\n待更新~~~\n\n\n","slug":"基于运放的信号发生电路总结","date":"2024-01-18T13:46:56.000Z","categories_index":"Electronic","tags_index":"design","author_index":"Yesord"},{"id":"150ed4b39e17f0239763704a3281f203","title":"基于pytorch的简单BP神经网络搭建","content":"前言神经网络(个人理解)神经网络本质就是权重，就是计算机在那算数，计算你输入的值经过网络迭代后得到另一个值，就很像函数对应一样，但是能进行推理，输入定义域以外的数，它能给你推理出来一个接近的解。喂数据可以提高网络的精确性，本质也是让它改变更新权重（改变对应法则），让网络更倾向去得到你想要的值。\n最基本神经网络结构由三部分构成：输入层、隐藏层、输出层这三部分之间是存在逻辑关系的，人为指定，一般比较简单，无非就是一些简单的函数变换，因为每步的计算简单，所以需要占用大量的算力，适合使用GPU。\nBP算法(个人理解)前向传播：完成从输入到隐藏在到输出的过程，由输入得到输出后向传播：完成从输出到输入的过程，由误差更新每一步的权重\npython代码实现train.pypython############################################\n#               简单训练模块                #\n############################################\n####一定要在train.py目录下创建weight文件夹####\n\nimport os\nfrom sys import path as sys_path\n\nscript_path = os.path.dirname(os.path.abspath(__file__)) # 获取当前脚本的绝对路径\nparent_path = os.path.dirname(os.path.dirname(script_path)) # 获取当前脚本的上级目录\nsys_path.append(parent_path) # 将上级目录加入到系统路径中\n\nfrom torch.utils.data import Dataset, DataLoader # 导入数据集和数据加载器\nimport torch\nimport torch.nn as nn # 导入神经网络模块\nfrom torch.optim import SGD # 导入随机梯度下降优化器\nimport time # 导入时间模块\nimport matplotlib.pyplot as plt # 导入绘图模块\n\n##################################################################################################\n\n# 创建输入数据,手敲数据集\n\nx = [[1,2],[3,4],[5,6],[6,7],[8,9],[10,11],[10,12],[11,13],[12,13],[14,15],[16,17],[18,19]] \ny = [[3],[7],[11],[13],[17],[21],[22],[24],[25],[29],[33],[37]] \n\nX = torch.tensor(x).float() # 将x转换成张量\nY = torch.tensor(y).float() # 将y转换成张量\n\ndevice = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;  # 判断是否有cuda\n\nX = X.to(device)  # 将x放到cuda上\nY = Y.to(device)  # 将y放到cuda上\n\n##################################################################################################\n# 创建神经网络的类\nclass MyNeuralNetwork(nn.Module):\n    def __init__(self):\n        super(MyNeuralNetwork, self).__init__()\n        # 创建神经网络的层就是直接调用一些nn提供的函数来实现的\n        self.input_to_hidden_layer = nn.Linear(2, 10) # 输入层到隐藏层 nn.Linear(2,10)表示输入层有2个神经元，隐藏层有10个神经元\n        self.hidden_to_output_layer = nn.Linear(10, 1) # 隐藏层到输出层\n        self.hidden_layer_activation = nn.ReLU() # 激活函数\n\n    def forward(self, x):\n        # 前向传播 表达层与层之间的逻辑关系\n        x = self.input_to_hidden_layer(x) # 输入层到隐藏层\n        x = self.hidden_layer_activation(x) # 激活函数\n        x = self.hidden_to_output_layer(x) # 隐藏层到输出层\n        return x\n    \n# 创建数据集类\nclass MyDataset(Dataset):\n    def __init__(self, x, y): # 初始化数据集\n        self.x = torch.tensor(x).float() # 将x转换成张量\n        self.y = torch.tensor(y).float() # 将y转换成张量\n        self.len = len(x) #数据集的长度\n    def __getitem__(self, index): #获取数据集中的一条数据\n        return self.x[index], self.y[index] #返回数据\n    def __len__(self): #获取数据集的长度\n        return self.len\n    \n##################################################################################################    \n# 实例化神经网络\n\n\ntorch.manual_seed(0) # 设置随机数种子\n\nmynet = MyNeuralNetwork().to(device) # 创建神经网络\nprint(mynet.input_to_hidden_layer.weight) # 查看输入层到隐藏层的权重\n\nfor par in mynet.parameters(): # 查看神经网络的参数\n    print(par)\n\ndataset = MyDataset(X, Y) # 实例化数据集 \ndataloader = DataLoader(dataset, batch_size=2, shuffle=True) # 实例化数据加载器 \n# 从dataset加载 batch_size=2表示每次加载2条数据，shuffle=True表示打乱数据\nfor data in dataloader: # 从dataloader中循环取出数据\n    print(data) # 打印数据\n\n##################################################################################################\n# 创建损失函数和优化器\n\nloss_func = nn.MSELoss() # 均方误差损失函数 \n\noptimizer = SGD(mynet.parameters(), lr=0.0015) # 随机梯度下降优化器 lr 学习率\n\n# optimizer.zero_grad() # 梯度清零\n# loss_value = loss_func(mynet(X), Y) # 计算损失\n# loss_value.backward() # 反向传播\n# optimizer.step() # 更新权重\n\n##################################################################################################\n\nloss_history = [] # 创建一个空列表，用于保存损失值\ntrain_epoch = 10000 # 训练次数\n\nstart = time.time() # 记录开始时间\nfor epoch in range(train_epoch):\n    for data in dataloader: # 从dataloader中循环取出数据\n        x, y = data\n        optimizer.zero_grad() # 梯度清零\n        loss_value = loss_func(mynet(x), y) # 计算损失\n        loss_value.backward() # 反向传播\n        optimizer.step() # 更新权重\n        loss_history.append(loss_value.item()) # 将损失值添加到列表中\n    if epoch % 100 == 0 and epoch &lt; train_epoch - 1: \n            plt.ion() # 打开交互模式\n            plt.plot(loss_history) # 绘制损失曲线\n            plt.title(&#39;Loss over the increasing number of epoch&#39;) # 设置标题\n            plt.xlabel(&#39;Epoch&#39;) # 设置x轴标签\n            plt.ylabel(&#39;Loss&#39;) # 设置y轴标签\n            plt.pause(0.1) # 暂停0.1秒\n    elif epoch == train_epoch - 1:\n            plt.ioff() # 关闭交互模式\n            plt.show() # 显示图像\nend = time.time() # 记录结束时间\nprint(&quot;训练时间：&quot;, end - start) # 打印训练时间\n\nval_x = [[13,19]] # 1*2的list\nval_x = torch.tensor(val_x).float().to(device) # 将val_x转换成张量\nprediction = mynet(val_x) # 预测val_x的值\nprediction_int = round(prediction.item()) # 将预测值转换成整数\nprint(&quot;&#123;&#125; + &#123;&#125; = &quot;.format(val_x[0][0],val_x[0][1]) + str(prediction_int)) # 打印预测值\n\n# 获取train.py的路径\nscript_dir = os.path.dirname(os.path.abspath(__file__))\n\n# 构造模型文件的路径\ndirectory = os.path.join(script_dir, &#39;weight&#39;)\n\n# 如果目录不存在，创建它\nif not os.path.exists(directory):\n    os.makedirs(directory)\n\n# 保存模型参数\ntorch.save(mynet.state_dict(), os.path.join(directory, &#39;BP_Pytorch.pth&#39;)) # 保存模型参数是以当前路径为参考路径的\n\n\nval.pypython\nimport torch\nimport torch.nn as nn\nfrom torchsummary import summary\n\nclass MyNeuralNetwork(nn.Module):\n    def __init__(self):\n        super(MyNeuralNetwork, self).__init__()\n        # 创建神经网络的层就是直接调用一些nn提供的函数来实现的\n        self.input_to_hidden_layer = nn.Linear(2, 10) # 输入层到隐藏层 nn.Linear(2,10)表示输入层有2个神经元，隐藏层有10个神经元\n        self.hidden_to_output_layer = nn.Linear(10, 1) # 隐藏层到输出层\n        self.hidden_layer_activation = nn.ReLU() # 激活函数\n\n    def forward(self, x):\n        # 前向传播 表达层与层之间的逻辑关系\n        x = self.input_to_hidden_layer(x) # 输入层到隐藏层\n        x = self.hidden_layer_activation(x) # 激活函数\n        x = self.hidden_to_output_layer(x) # 隐藏层到输出层\n        return x\n\nmodel = MyNeuralNetwork().to(&#39;cuda&#39;) # 创建神经网络\n\nstatic_dict = torch.load(&#39;BP_Pytorch.pth&#39;)\n\nmodel.load_state_dict(static_dict) # 加载模型参数 保证模型参数一致即可，不需要保证模型结构一致\n#类创建的神经网络和nn.Squential（序贯）创建的神经网络的参数保存和加载方式不同，二者不能混用\n\nsummary(model, input_data=(1,2)) # 打印模型结构\n\nprint(&quot;请输入两个数，我来帮你计算它们的和：&quot;)\nx1 = float(input(&#39;请输入第一个数：&#39;))\nx2 = float(input(&#39;请输入第二个数：&#39;))\n\nval_x = [[x1,x2]] #创建列表\nval_x = torch.tensor(val_x).float().to(&#39;cuda&#39;)\nprediction = model(val_x) # 预测val_x的值\nprediction_int = round(prediction.item()) # 将预测值转换成整数\nprint(&quot;&#123;&#125; + &#123;&#125; = &quot;.format(val_x[0][0],val_x[0][1]) + str(prediction.item())) # 打印预测值\n\n","slug":"基于pytorch的简单BP神经网络搭建","date":"2023-11-21T15:56:50.000Z","categories_index":"AI","tags_index":"ANN","author_index":"Yesord"},{"id":"c7f2a0b48c2c1da08799955658fe960e","title":"计算方法——一些算法的python实现","content":"前言本文是基于python实现的计算方法课程的算法，仅实数域内计算。部分算法提供迭代曲线的绘制（基于matplotlib），可以在控制台打印每一步的迭代结果\nGitHub库\nSolution_for_Nonlinear_algebra_equation二分法bash\nimport math\nimport matplotlib.pyplot as plt\n\n##############################################################################################################\n# name: Dichotomy_method\n# function: 二分法\n# parameter: f: 函数\n#            a: 区间左端点\n#            b: 区间右端点\n#            tol: 精度要求\n#            maxiter: 最大迭代次数\n#            info: 是否输出迭代信息\n#            plot: 是否输出迭代曲线\n# return: x1: 迭代结果，如果无法判断是否有根则返回None\n#         epoch: 迭代次数，如果无法判断是否有根则返回None\n# date: 2023/11/21\n# author: Yesord\n# note 需要确定区间内有且仅有一个根\n##############################################################################################################\n\ndef Dichotomy_method(f, a, b, tol, maxiter, info=False, plot=False): \n    x0 = a\n    x1 = b\n    if f(a) * f(b) &gt; 0:\n        print(&quot;Error: f(&#123;&#125;) * f(&#123;&#125;) &gt; 0, we can&#39;t ensure that there is root in the interval [&#123;&#125;, &#123;&#125;]&quot;.format(a, b, a, b))\n        # %操作符用于格式化字符串\n        return None, None\n    if plot:\n        x_history = []\n        epoch_history = []\n    for epoch in range(maxiter):\n        x1 = (a + b) / 2\n        if f(x1) * f(a) &lt; 0:\n            b = x1\n        else:\n            a = x1\n        if info:\n            print(&quot;epoch = %d, x1 = %f&quot; % (epoch, x1))\n        if plot:# 绘制迭代过程\n            plt.ion()\n            plt.xlabel(&quot;epoch&quot;)\n            plt.ylabel(&quot;x&quot;)\n            plt.title(&quot;Dichotomy method&quot;)\n            plt.plot(epoch_history, x_history, color=&#39;r&#39;, linestyle=&#39;-&#39;)\n            plt.pause(0.1)\n        if abs(x1 - x0) &lt; tol:\n            plt.ioff()\n            plt.show()\n            return x1, epoch\n        epoch_history.append(epoch)\n        x_history.append(x1)\n        x0 = x1\n\ndef Dichotomy_method_convergence_judgment(f, a, b):\n    if f(a) * f(b) &gt; 0:\n        print(&quot;Error: f(%f) * f(%f) &gt; 0, we can&#39;t ensure that there is root in the interval [%f, %f]&quot; % (a, b, a, b))\n        # format()函数用于格式化字符串\n        print(&quot;Dichotomy method isn&#39;t recommended.&quot;)\n\n# 测试\nif __name__ == &quot;__main__&quot;: # __name__是当前模块名，当模块被直接运行时模块名为__main__。没有被直接运行时模块名为文件名\n    print(&quot;\\n*****************************************************************&quot;)\n    print(&quot;Dichotomy method test start...&quot;)\n    print(&quot;*****************************************************************\\n&quot;)\n    def f(x):\n        y = x**2 - 0.2\n        return y\n    a = -1\n    b = 1\n    tol = 1e-5\n    maxiter = 1000\n    x, epoch = Dichotomy_method(f, a, b, tol, maxiter, info=True, plot=True)\n    if x is None or epoch is None:\n        print(&quot;Dichotomy_method did not return valid values.&quot;)\n    else:\n        print(&quot;\\nx = %f, epoch = %d\\n&quot; % (x, epoch))\n    print(&quot;\\n*****************************************************************&quot;)\n    print(&quot;Dichotomy method test end...&quot;)\n    print(&quot;*****************************************************************\\n&quot;)\n迭代法bash\nimport math\nimport matplotlib.pyplot as plt\n\n##############################################################################################################\n# name: iterative_method\n# function: 迭代法\n# parameter: g: 迭代函数\n#            x0: 初始值\n#            tol: 精度要求\n#            maxiter: 最大迭代次数\n#            info: 是否输出迭代信息\n#            plot: 是否输出迭代曲线\n# return: x1: 迭代结果\n#         epoch: 迭代次数\n# date: 2023/11/21\n# author: Yesord\n# note 需要人工计算得到迭代函数以及验证是否收敛\n##############################################################################################################\n\n# 迭代法\ndef iterative_method(g, x0, tol, maxiter, info=False, plot=False):\n    if plot:\n        x_history = []\n        epoch_history = []\n    for epoch in range(maxiter):\n        x1 = g(x0)\n        if info:\n            print(&quot;epoch = %d, x1 = %f&quot; % (epoch, x1))\n        if plot:# 绘制迭代过程\n            plt.ion()\n            plt.xlabel(&quot;epoch&quot;)\n            plt.ylabel(&quot;x&quot;)\n            plt.title(&quot;Iterative method&quot;)\n            plt.plot(epoch_history, x_history, color=&#39;r&#39;, linestyle=&#39;-&#39;)\n            plt.pause(0.1)\n        if abs(x1 - x0) &lt; tol:\n            plt.ioff()\n            plt.show()\n            return x1, epoch\n        epoch_history.append(epoch)\n        x_history.append(x1)\n        x0 = x1\n\n\n\n\n# 测试\n\n\n\nif __name__ == &quot;__main__&quot;: \n    print(&quot;\\n*****************************************************************&quot;)\n    print(&quot;Iterative method test start...&quot;)\n    print(&quot;*****************************************************************\\n&quot;)\n    def g(x): # 迭代函数\n        y = x - (x**3 - 2*x**2 + 3*x - 1)/(3*x**2 - 4*x + 3)\n        return y\n    x0 = float(input(&quot;Please input the initial value x0: &quot;))\n    tol = 1e-5\n    maxiter = 1000\n    x, epoch = iterative_method(g, x0, tol, maxiter, info=True, plot=True)\n    print(&quot;\\nx = %f, epoch = %d\\n&quot; % (x, epoch))\n    print(&quot;*****************************************************************&quot;)\n    print(&quot;Iterative method test end...&quot;)\n    print(&quot;*****************************************************************\\n&quot;)\n    牛顿法bash\nimport math\nimport matplotlib.pyplot as plt\n\n##############################################################################################################\n# name: Newton_method\n# function: 牛顿法\n# parameter: f: 函数\n#            df: f的导函数\n#            x0: 初始值\n#            tol: 精度要求\n#            maxiter: 最大迭代次数\n#            info: 是否输出迭代信息\n#            plot: 是否输出迭代曲线\n# return: x1: 迭代结果\n#         epoch: 迭代次数\n# date: 2023/11/21\n# author: Yesord\n# note 需要人工计算得到导函数以及验证是否收敛\n##############################################################################################################\n\ndef Newton_method(f, df, x0, tol, maxiter, info=False, plot=False):\n    if df == None:# 如果df没有定义，则使用数值微分\n        df = lambda f, x: (f(x+1e-6) - f(x-1e-6)) / (2e-6) # lambda x: df(f, x)是一个匿名函数，等价于def df(x): return df(f, x)\n    if plot:\n        x_history = []\n        epoch_history = []\n    for epoch in range(maxiter):\n        x1 = x0 - f(x0)/df(f, x0)\n        if info:\n            print(&quot;epoch = %d, x1 = %f&quot; % (epoch, x1))\n        if plot:# 绘制迭代过程\n            plt.ion()\n            plt.xlabel(&quot;epoch&quot;)\n            plt.ylabel(&quot;x&quot;)\n            plt.title(&quot;Newton method&quot;)\n            plt.plot(epoch_history, x_history, color=&#39;r&#39;, linestyle=&#39;-&#39;)\n            plt.pause(0.1)\n        if abs(x1 - x0) &lt; tol: # 精度达到要求输出结果\n            plt.ioff()\n            plt.show()\n            return x1, epoch\n        epoch_history.append(epoch) \n        x_history.append(x1)\n        x0 = x1\n\n# 收敛性判断\ndef Newton_method_convergence_judgment(f, df, x0):\n    if df(f, x0) == 0:\n        print(&quot;Error: df(%f) = 0, Newton method isn&#39;t recommended.&quot; % x0)\n\n\n# 测试\nif __name__ == &quot;__main__&quot;: \n    f = lambda x: math.pow(x,3) - 2*math.pow(x,2) + 3*x - 4 # sample function\n    df = lambda f, x: (f(x+1e-6) - f(x-1e-6)) / (2e-6) # lambda f,x :用于定义匿名函数，冒号前面的x表示函数参数，冒号后面的x表示函数返回值\n    print(&quot;\\n*****************************************************************&quot;)\n    print(&quot;Newton method test start...&quot;)\n    print(&quot;*****************************************************************\\n&quot;)\n    print(&quot;test function: f(x) = x**3 - 2*x**2 + 3*x - 4&quot;)\n    #输出x0初值并将其转换成浮点数\n    x0 = float(input(&quot;Please input the initial value x0: &quot;)) \n    tol = 1e-6 # 精度要求\n    maxiter = 100 # 最大迭代次数\n    x, epoch = Newton_method(f, df, x0, tol, maxiter, info=True, plot=True) # Netwon method\n    print(&quot;x = %f, epoch = %d&quot; % (x, epoch))\n    Newton_method_convergence_judgment(f, df, x0) # 收敛性判断\n    print(&quot;\\n*****************************************************************&quot;)\n    print(&quot;Newton method test done!&quot;)\n    print(&quot;*****************************************************************\\n&quot;)\n\nIterative_method_for_Linear_algebra_equations雅可比迭代法bash\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# jacobi迭代法\n##############################################################################################################\n# name: jacobi_iterative_method\n# function: jacobi迭代法\n# parameter: A: 系数矩阵\n#            b: 常数项\n#            x0: 初始值\n#            tol: 精度要求\n#            max_iter: 最大迭代次数\n#            info: 是否输出迭代信息\n#            plot: 是否输出迭代曲线\n# return: x1: 迭代结果\n#         epoch: 迭代次数\n# date: 2023/11/20\n# author: Yesord\n##############################################################################################################\n\ndef jacobi_iterative_method(A, b, x0, tol, max_iter, info=False, plot=False):\n    A = np.array(A) # 转换为numpy数组\n    b = np.array(b) # 转换为numpy数组\n    x0 = np.array(x0) # 转换为numpy数组\n    n = len(A) # 方程组的阶数\n    x1 = np.zeros(n) # 初始化x1\n    epoch = 0 # 迭代次数\n    if plot:\n        x1_history = []\n        x2_history = []\n        x3_history = []\n        epoch_value = []\n    for epoch in range(max_iter): # 迭代\n        for i in range(n):\n            x1[i] = b[i]\n            for j in range(n):\n                if i != j: # 不是对角元素\n                    x1[i] -= A[i][j] * x0[j]\n            x1[i] /= A[i][i] \n\n        if info: # 输出迭代信息\n            print(&quot;x^= &quot;, x1) # 输出结果\n            print(&quot;epoch= &quot;, epoch) # 输出迭代次数\n\n        if plot: # 输出迭代曲线\n            \n            x1_history.append(x1[0])\n            x2_history.append(x1[1])\n            x3_history.append(x1[2])\n            epoch_value.append(epoch)\n\n            plt.ion() # 打开交互模式\n            plt.xlabel(&quot;epoch&quot;) # 设置x轴标签\n            plt.ylabel(&quot;x_iterative&quot;) # 设置y轴标签\n            plt.title(&quot;jacobi iterative method&quot;) # 设置标题\n\n            line_x1, = plt.plot(epoch_value, x1_history, &#39;r.-&#39;) # 画x1图 r.-表示红色点线图\n            line_x2, = plt.plot(epoch_value, x2_history, &#39;g.-&#39;) # 画x2图 g.-表示绿色点线图\n            line_x3, = plt.plot(epoch_value, x3_history, &#39;b.-&#39;) # 画x3图 b.-表示蓝色点线图\n            \n            plt.pause(0.1)   \n        if np.linalg.norm(x1 - x0) &lt; tol: # 判断是否满足精度要求\n            if plot:\n                line_x1.set_label(&quot;x1&quot;) # 设置图例\n                line_x2.set_label(&quot;x2&quot;) \n                line_x3.set_label(&quot;x3&quot;) \n                plt.legend() # 显示图例\n                plt.ioff() # 关闭交互模式\n                plt.show() # 显示图像\n            break\n        x0 = x1.copy() # 更新x0\n        \n        \n    \n    return x1, epoch # 返回结果\n\n\n# 测试\ndef test():\n    x0 = [0, 0, 0] # 初始值\n    A = [[5, -2, 1], [1, 5, -3], [2, 1, -5]]\n    B = [4, 2, -11] # 常数项\n    tol = 0.5e-3 # 精度要求\n    max_iter = 100 # 最大迭代次数\n    x, epoch = jacobi_iterative_method(A, B, x0, tol, max_iter, info=True, plot=True) # jacobi迭代法\n    return x, epoch # 返回结果\n\n        高斯赛德尔迭代法bash\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# 高斯-赛德尔迭代法\n##############################################################################################################\n# name: GS_iterative_method\n# function: 高斯-赛德尔迭代法\n# parameter: A: 系数矩阵\n#            b: 常数项\n#            x0: 初始值\n#            acurrate: 精度要求\n#            max_iter: 最大迭代次数\n#            info: 是否输出迭代信息\n#            plot: 是否输出迭代曲线\n# return: x1: 迭代结果\n#         epoch: 迭代次数\n# date: 2023/11/20\n# author: Yesord\n##############################################################################################################\ndef GS_iterative_method(A, b, x0, acurrate, max_iter, info=False, plot=False):\n    A = np.array(A) # 转换为numpy数组\n    b = np.array(b) # 转换为numpy数组\n    x0 = np.array(x0) # 转换为numpy数组\n    n = len(A) # 方程组的阶数\n    epoch = 0 # 迭代次数\n    if plot:\n        x1_history = []\n        x2_history = []\n        x3_history = []\n        epoch_value = []\n    x1 = np.zeros(n) # 初始化x1\n    for epoch in range(max_iter):\n        for i in range(n):\n            x1[i] = b[i]\n            for j in range(n):\n                if i != j:\n                    x1[i] -= A[i][j] * x1[j]\n            x1[i] /= A[i][i]\n\n        if info: # 输出迭代信息\n            print(&quot;x^= &quot;, x1) # 输出结果\n            print(&quot;epoch= &quot;, epoch) # 输出迭代次数\n\n        if plot: # 输出迭代曲线\n            \n            x1_history.append(x1[0])\n            x2_history.append(x1[1])\n            x3_history.append(x1[2])\n            epoch_value.append(epoch)\n\n            plt.ion() # 打开交互模式\n            plt.xlabel(&quot;epoch&quot;) # 设置x轴标签\n            plt.ylabel(&quot;x_iterative&quot;) # 设置y轴标签\n            plt.title(&quot;GS iterative method&quot;) # 设置标题\n\n            line_x1, = plt.plot(epoch_value, x1_history, &#39;r.-&#39;) # 画x1图 r.-表示红色点线图\n            line_x2, = plt.plot(epoch_value, x2_history, &#39;g.-&#39;) # 画x2图 g.-表示绿色点线图\n            line_x3, = plt.plot(epoch_value, x3_history, &#39;b.-&#39;) # 画x3图 b.-表示蓝色点线图\n            \n            plt.pause(0.1)  \n        if np.linalg.norm(x1 - x0) &lt; acurrate: # 判断是否满足精度要求\n            if plot:\n                line_x1.set_label(&quot;x1&quot;) # 设置图例\n                line_x2.set_label(&quot;x2&quot;) \n                line_x3.set_label(&quot;x3&quot;) \n                plt.legend() # 显示图例\n                plt.ioff() # 关闭交互模式\n                plt.show() # 显示图像\n            break\n        x0 = x1.copy() # 更新x0\n    return x1, epoch # 返回结果\n\n# 测试\ndef test():\n    x0 = [0, 0, 0] # 初始值\n    A = [[5, -2, 1], [1, 5, -3], [2, 1, -5]] # 系数矩阵\n    B = [4, 2, -11] # 常数项\n    tol = 0.5e-3 # 精度要求\n    max_iter = 100 # 最大迭代次数\n    x, epoch = GS_iterative_method(A, B, x0, tol, max_iter,info=True, plot=True) # GS迭代法\n    return x, epoch # 返回结果\n\nSOC迭代法bash\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# SOC迭代法\n##############################################################################################################\n# name: SOC_iterative_method\n# function: SOC迭代法\n# parameter: A: 系数矩阵\n#            b: 常数项\n#            x0: 初始值\n#            w: 松弛因子\n#            tol: 精度要求\n#            max_iter: 最大迭代次数\n#            info: 是否输出迭代信息\n#            plot: 是否输出迭代曲线,仅支持三元方程组\n# return: x1: 迭代结果\n#         epoch: 迭代次数\n# date: 2023/11/20\n# author: Yesord\n##############################################################################################################\n\ndef SOC_iterative_method(A, b, x0, w, tol, max_iter, info=False, plot=False):\n    A = np.array(A) # 转换为numpy数组\n    b = np.array(b) # 转换为numpy数组\n    x0 = np.array(x0) # 转换为numpy数组\n    n = len(A) # 方程组的阶数\n    x1 = np.zeros(n) # 初始化x1\n    epoch = 0 # 迭代次数\n    if plot and n == 3:\n        x1_history = []\n        x2_history = []\n        x3_history = []\n        epoch_value = []\n\n    for epoch in range(max_iter): # 迭代\n        for i in range(n):\n            x1[i] = b[i]\n            for j in range(n):\n                if i != j: # 不是对角元素\n                    x1[i] -= A[i][j] * x1[j]\n            x1[i] /= A[i][i]\n            x1[i] = (1 - w) * x0[i] + w * x1[i]\n        \n        if info: # 输出迭代信息\n            print(&quot;x^= &quot;, x1) # 输出结果\n            print(&quot;epoch= &quot;, epoch) # 输出迭代次数\n\n        if plot and n == 3: # 输出迭代曲线\n            \n            x1_history.append(x1[0])\n            x2_history.append(x1[1])\n            x3_history.append(x1[2])\n            epoch_value.append(epoch)\n\n            plt.ion() # 打开交互模式\n            plt.xlabel(&quot;epoch&quot;) # 设置x轴标签\n            plt.ylabel(&quot;x_iterative&quot;) # 设置y轴标签\n            plt.title(&quot;SOC iterative method&quot;) # 设置标题\n\n            line_x1, = plt.plot(epoch_value, x1_history, &#39;r.-&#39;) # 画x1图 r.-表示红色点线图\n            line_x2, = plt.plot(epoch_value, x2_history, &#39;g.-&#39;) # 画x2图 g.-表示绿色点线图\n            line_x3, = plt.plot(epoch_value, x3_history, &#39;b.-&#39;) # 画x3图 b.-表示蓝色点线图\n            \n            plt.pause(0.1)  \n                \n        if np.linalg.norm(x1 - x0) &lt; tol: # 判断是否满足精度要求\n            if plot:\n                line_x1.set_label(&quot;x1&quot;) # 设置图例\n                line_x2.set_label(&quot;x2&quot;) \n                line_x3.set_label(&quot;x3&quot;) \n                plt.legend() # 显示图例\n                plt.ioff() # 关闭交互模式\n                plt.show() # 显示图像\n            break\n        x0 = x1.copy() # 更新x0\n        \n    \n    return x1, epoch # 返回结果\n\n# 测试\ndef test():\n    x0 = [0, 0, 0] # 初始值\n    A = [[4,3,0], [3,4,-1], [0,-1,4]] # 系数矩阵\n    B = [16,20,-12] # 常数项\n    w = 1.24 # 松弛因子\n    tol = 1e-3 # 精度要求\n    max_iter = 100 # 最大迭代次数\n    x, epoch = SOC_iterative_method(A, B, x0, w, tol, max_iter,info=True, plot=True) # jacobi迭代法\n    return x, epoch\n","slug":"计算方法——一些算法的python实现","date":"2023-11-20T16:14:56.000Z","categories_index":"","tags_index":"python","author_index":"Yesord"},{"id":"a6d09d63bbb6828b08c842fdfa1f3e69","title":"pytorch环境搭建下遇到的问题","content":"前言总所周知，ai的发展是极度迅速的，ai相关的软件与库的更新速度极快，很容易出现库依赖之间的不兼容，因而基于anaconda下的pytorch环境搭建总是会遇到问题，本博文就总结了我在搭建pytorch环境时所遇到的问题\nVscode下的环境搭建\n\n\n\n\n\nconda报错\n解决Anaconda关联VsCode使用时powershell终端conda报错的问题\n\n\n\n\n\n\n\n\npytorch、cuda、python版本对应\npytorch官网\n\n\n","slug":"pytorch环境搭建下遇到的问题","date":"2023-11-15T13:53:53.000Z","categories_index":"AI","tags_index":"ComputerVision","author_index":"Yesord"},{"id":"61f52ac56de5cfb5fc47f2f3f1fa4dd8","title":"FreeRTOS学习笔记","content":"前言本博文只是整理了本人学习 FreeRTOS 的一些笔记，仅作简单记录。\nFreeRTOS的任务基础操作任务创建和删除的API函数调用FreeRTOS的API函数\n\n\n\nAPI函数\n描述\n\n\n\nxTaskCreate()\n动态方式创建任务\n\n\nxTaskCreateStatic()\n静态方式创建任务\n\n\nvTaskDelete()\n删除任务\n\n\ntxt动态创建：FreeRTOS内核负责分配从FreeRTOS 管理的堆中分配的任务控制块以及任务的栈空间所需的内存\n\n静态创建：用户自己分配任务的任务控制块以及任务的栈空间所需的内存任务创建动态方法创建\n\n   将宏configSUPPORT_DYNAMIC_ALLOCATION 配置为 1\n   定义函数入口参数xTaskCreate()\n   编写任务函数\n\n\n\n\n\n\n\nTIP\n此函数创建的任务会立刻进入就绪态，由任务调度器调度运行\n\n\n动态创建任务函数内部实现\n\n   申请堆栈内存&amp;任务控制块内存\n   TCB结构体成员赋值\n   添加新任务到就绪列表中\n\n入口函数参数\n\n    \n\n\n\n\n\n返回值\n描述\n\n\n\npdPASS\n任务创建成功\n\n\nerrCOULD_NOT_ALLOCATE_REQUIRED_MEMORY\n任务创建失败\n\n\nTCB任务控制块\n\n    \n\n\n\n任务栈栈顶，在任务切换时的任务上下文保存、任务恢复息息相关\n\n\n\n\n\n\n\nWARNING\n每个任务都有属于自己的任务控制块，类似身份证\n\n\n静态方法\n\n   需将宏configSUPPORT_STATIC_ALLOCATION 配置为 1\n   定义空闲任务&amp;定时器任务的任务堆栈及TCB\n   实现两个接口函数（vApplicationGetIdleTaskMemory( ) vApplicationGetTimerTaskMemory () ）\n   定义函数入口参数\n   编写任务函数\n\n\n\n\n\n\n\nTIP\n此函数创建的任务会立刻进入就绪态，由任务调度器调度运行\n\n\n静态创建内部实现\n\n   TCB结构体成员赋值\n   添加新任务到就绪列表中\n\n任务删除void vTaskDelete(TaskHandle_t xTaskToDelete);\n\n\n\n形参\n描述\n\n\n\nxTaskToDelete\n待删除任务的任务句柄\n\n\n用于删除已被创建的任务被删除的任务将从就绪态任务列表、阻塞态任务列表、挂起态任务列表和事件列表中移除\n\n\n\n\n\n\n\nWARNING\n\n当传入的参数为NULL，则代表删除任务自身（当前正在运行的任务）但是得等到空闲\n空闲任务会负责释放被删除任务中由系统分配的内存，但是由用户在任务删除前申请的内存， 则需要由用户在任务被删除前提前释放，否则将导致内存泄露 （正在运行是不会立刻移除的）\n\n\n\n删除任务流程\n\n使用删除任务函数，需将宏INCLUDE_vTaskDelete 配置为 1 \n入口参数输入需要删除的任务句柄（NULL代表删除本身）\n\n内部操作\n\n获取所要删除任务的控制块\n将被删除任务，移除所在列表\n判断所需要删除的任务\n更新下个任务的阻塞时间\n\n任务的挂起与恢复API函数\n\n\nAPI函数\n描述\n\n\n\nvTaskSuspend()\n挂起任务\n\n\nvTaskResume()\n恢复被挂起的任务\n\n\nxTaskResumeFromISR()\n在中断中恢复被挂起的任务\n\n\n\n\n\n\n\n\nTIP\n挂起：挂起任务类似暂停，可恢复； 删除任务，无法恢复，类似“人死两清”恢复：恢复被挂起的任务FromISR：带FromISR后缀是在中断函数中专用的API函数\n\n\n任务挂起函数介绍void vTaskSuspend(TaskHandle_t xTaskToSuspend) \n\n\n\n形参\n描述\n\n\n\nxTaskSuspend\n待挂起任务的任务句柄\n\n\n无论优先级如何，被挂起的任务都将不再被执行，直到任务被恢复 。\n\n\n\n\n\n\n\nWARNING\n当传入的参数为NULL，则代表挂起任务自身（当前正在运行的任务）\n\n\n任务恢复函数介绍void vTaskResume(TaskHandle_t xTaskToResume)\n\n\n\n形参\n描述\n\n\n\nxTaskToResume\n待恢复任务的任务句柄\n\n\n\n\n\n\n\n\n\nWARNING\n任务无论被 vTaskSuspend() 挂起多少次，只需在任务中调用  vTakResume() 恢复一次，就可以继续运行。且被恢复的任务会进入就绪态！\n\n\n任务恢复函数介绍BaseType_t xTaskResumeFromISR(TaskHandle_t xTaskToResume)  \n\n\n\n形参\n描述\n\n\n\nxTaskToResume\n待恢复任务的任务句柄\n\n\n\n\n\n返回值\n描述\n\n\n\npdTRUE\n任务恢复后需要进行任务切换\n\n\npdFALSE\n任务恢复后不需要进行任务切换\n\n\n\n\n\n\n\n\n\nWARNING\n中断服务程序中要调用freeRTOS的API函数则中断优先级不能高于FreeRTOS所管理的最高优先级(默认5-15)\n\n\n参考文献正点原子\n","slug":"FreeRTOS学习笔记","date":"2023-11-15T09:19:00.000Z","categories_index":"Embedded","tags_index":"RTOS,STM32","author_index":"Yesord"},{"id":"7654cddd589d5117806bb3f535f9e67b","title":"滤波电路设计","content":"前言滤波电路是一种用于过滤信号的电路，它将仅传递所需的信号并滤去不需要的信号。通常，滤波电路由无源元件或有源元件设计。\n· 无源元件包括电阻器、电感器和电容器。\n· 有源元件包括BJT、MOSFET和运算放大器。\n\nClick to see more\n低通滤波电路:一种允许频率低于一个确定频率阈值的信号通过，并且阻止高于该确定频率阈值的信号通过的电子电路。低通滤波电路允许低频信号通过，并抑制高频信号。其核心思想是在频率域上通过移除高频成分来平滑信号。这在去噪、平滑和提取基本频率成分时非常有用。\n高通滤波电路:一种允许频率高于一个确定频率阈值的信号通过，并且阻止低于该确定频率阈值的信号通过的电子电路。高通滤波电路允许高频信号通过，并抑制低频信号。它的工作原理与低通滤波电路相反，通过移除低频成分来突出高频特征。\n\n\n\n无源高通滤波电路仅由无源器件构成的通高频阻低频的滤波电路。\n最简单的无源高通滤波电路就是一个电阻+一个电容\n截止频率 $f_L &#x3D; \\frac{1}{2πRC}$ f &#x3D; 1&#x2F;(2πRC),高于 $f_L$ f可通，低于$f_L$ f衰减\n\n\n\n\n\n\nTIP\n此处的通高频阻低频都是相对的，本人习惯认为k量级以下的为低频，G量级以上的为高频。这不唯一，实际电路应根据实际需要的信号进行分析。\n\n\n无源低通滤波电路仅由无源器件构成的通低频阻高频的滤波电路。\n最简单的无源低通滤波电路也就是一个电阻+一个电容\n截止频率 $ f_L &#x3D; \\frac{1}{2πRC} $ f &#x3D; 1&#x2F;(2πRC) ，低于$ f_L $ f可通，高于$ f_L $ f衰减\n感兴趣的读者可以去Multisim上验证\n\n\n\n\n\n\n\nWARNING\n无源滤波通常放大倍数及其截止频率随负载变化，在实际电路中负载会变化会导致无源滤波的参数改变而可能导致信号的失真。\n\n\n有源低通滤波电路由有源器件构成的通低频阻高频的滤波电路。\n介绍基础的有源一阶低通滤波电路\n在无源低通滤波电路中加入高输入阻抗，低输出阻抗的隔离电路（后级电路输入低阻抗）-&gt;电压跟随器\n\n\n\n\n\n\nTIP\n高输入电抗可以保证原先无源低通滤波电路的参数的稳定，负载都趋于无穷了。。。低输出电抗可以保证信号的纯净度，减小干扰。\n\n\n将无源滤波电路与运放组合可以做到滤波+信号放大\n有源高通滤波电路由有源器件构成的通高频阻低频的滤波电路。\n带通滤波电路只保留某一段频率带，抑制或衰减其他频率带的滤波电路\n懂点电路原理知道了LRC电路就可以知道实际上这有个通频带的概念。\n带阻滤波电路只抑制或衰减某一段频率带，对其他频率无影响的滤波电路，可以认为是与带通滤波电路互补的。\n实际电路中的滤波电路分析电容的实际等效模型\n\n\n\n\n\n\n\n阻抗计算公式\n阻抗$ Z &#x3D; ESR + jωL + &#x2F;frac{1}{jωC} $Z &#x3D; ESR + jωL + 1&#x2F;jωC其中$ω &#x3D; 2πf$ ω &#x3D; 2πf\n\n\n\n\n\n\n\n\n\n性质\n当 f &#x3D; f 时，Z&#x3D;ESR  此时电容相当于一个纯电阻， f 为其谐振频率当 f &gt; f 时， 电容呈感性(像电感一样阻高频)当 f &lt; f 时， 电容呈容性(像电容一样阻低频)\n\n\n\n\n\n\n\n\n\n举例\n因此，当电容作高频滤波时，应工作在容性即f &lt; f 的条件下\n一般电容厂商会提供电容的数据手册上会有电容的阻抗-频率曲线，可以对应到谐振频率\n\n\n芯片电源引脚为什么加100nF电容？该电容常常称为旁路电容\n该电容需要尽量靠近电源引脚，因为如果摆放很远的话，电容滤除噪声后的电源会在这个段路径上又串扰进新的噪声，那么这个电容的作用就没有太大的意义。。\n\n\n\n\n\n\n作用\n\n滤除电源上的高频噪声–显然的\n储能–芯片电源端进行供电时可以就近供能，减小电源平面干扰，不然拉长从电源平面易产生噪声（电压波动）\n减小高频信号的回流路径–高频回路路径大会导致对其他器件的电磁干扰\n\n\n\n常用数字芯片信号频率基本都在10MHz以下，电源上的干扰或其自身干扰大概100nF的谐振频率之下，因此电源引脚旁路电容经验值100nF。\n\nClick to see more\n芯片信号频率比较高可以选择小容值\n容值与频率的对应参考（懒人不想看数据手册）\n\n\n\n频率范围\n推荐容值\n\n\n\nDC-100KHz\n10uF\n\n\n100KHz-10MHz\n100nF\n\n\n10MHz-100MHz\n10nF\n\n\n&gt;100M\n1nF\n\n\n可以去Multisim上进行电路仿真感受以下滤波的工作机理\n\n\n","slug":"滤波电路设计","date":"2023-11-07T15:33:17.000Z","categories_index":"Electronic","tags_index":"design","author_index":"Yesord"},{"id":"e3a2cd7dc7fecebd46e5f83ebb105431","title":"PCB设计中遇到的疑问","content":"疑问\n在设计四层板时，为什么标准的叠层设计是SIN1+PWR+GND+SIN2，能否不要GND，搞双电源层？\n\n\n\n\n\n\n\nTIP\n开始时不理解为啥要在内电层花一整层作为地层，认为只要在铺铜时铺GND的网络就可以做到和内电层作地层同样的效果，实际上GND仅铺铜还是会造成信号的回路变长，显然的，跨过半个板子到地和原地打个孔到地传输速度还是没法比。因此在设计四层板时，一般还是得搞个地平面保证信号质量。\n\n\n\n内层作电源分割和地层分割\n\n\n\n\n\n\n\nTIP\n注意电源输入，不要作截断，在分割电源区时，这又得在元件布局的时候注意、\n\n\n","slug":"PCB设计中遇到的疑问","date":"2023-10-31T14:02:21.000Z","categories_index":"Electronic","tags_index":"PCB","author_index":"Yesord"},{"id":"d50032c68beed9718f289c6681698c9b","title":"PCB设计原则","content":"前言\n\n\n\n\n\nTIP\nPCB设计是一个很吃经验的活，很多原则要在多次实践中慢慢摸索与牢记。\n\n资料\n\n\n\n\n\n\n\n\n\nPCB设计时如何选择合适的叠层方案\n丝印注意事项\n内电层电源平面分割\n\n","slug":"PCB设计原则","date":"2023-10-31T13:52:00.000Z","categories_index":"Electronic","tags_index":"PCB","author_index":"Yesord"},{"id":"e51893df1f169b9a3df8a540ead30580","title":"Docker资料汇总","content":"前言本文是基于Windows的Docker资料汇总\nDocker 官网https://www.docker.com/\n下载链接https://www.docker.com/get-started/\n改非C盘安装\nhttps://blog.csdn.net/weixin_41166529/article/details/128597650\n\n按1资料的方法不要用命令行启动installer，本人亲测出了问题，直接点击installer图标下载即可\n下载wsl管理员身份打开cmd运行如下命令\nbash\ndism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\n\ndism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart\nMicrosoft官网下载Linux更新包https://learn.microsoft.com/zh-cn/windows/wsl/install-manual#step-4---download-the-linux-kernel-update-package\n设置wsl2为默认版本在命令行里执行如下命令\nbashwsl –set-default-version2Microsoft store 下载linux内核\ndocker 简单创建命令行使用\nbash\ndocker version \n查找docker版本\ndocker pull ubuntu:20.04 \n拉取docker数据库里的ubuntu镜像\ndocker run -it ubuntu:20.04 /bin/ bash \n创建并运行容器（相当于一个空白ubuntu环境）\ndocker exer -it 容器名      \n再次进入时输入，记得要在docker中打开容器\n可能问题wsl打开Ubuntu20.04时,出现0x8007a1bc原因：没有内核升级解决方案：Microsoft官网下载Linux更新包https://learn.microsoft.com/zh-cn/windows/wsl/install-manual#step-4---download-the-linux-kernel-update-package\nwsl创建用户时，出现参考对象类型不支持尝试解决方案：改注册表记事本新建文档写下如下信息：\nbash\nWindows Registry Editor Version 5.00\n \n[HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\WinSock2\\Parameters\\AppId_Catalog\\0408F7A3]\n&quot;AppFullPath&quot;=&quot;C:\\\\Windows\\\\System32\\\\wsl.exe&quot;\n&quot;PermittedLspCategories&quot;=dword:80000000\n保存加后缀.reg为注册表文件然后执行即可解决问题\n","slug":"Docker资料汇总","date":"2023-10-26T03:22:11.000Z","categories_index":"","tags_index":"Linux","author_index":"Yesord"},{"id":"9cc33b411dfac4ea702c9c37d469d71e","title":"Yolov5初次尝试","content":"前言本人刚入门学习计算机视觉，完全看不懂理论算法，就想着先搭建一个环境去跑一个目标识别，这样能在大体上对该领域有个粗略的认知\n正文简述Yolov5的检测原理\n\nResize image改变图像大小，目标检测一般要统一输入图像的大小方便后面神经网络的预测和推理\n\n缩放图像-&gt;设定锚框\n\nRun convolutional network在神经网络中处理图像（很复杂，这才是yolo的精髓）\n\n多尺度融合：最后得到大小不同的含有检测信息的特征图，将其融合计算以便于小目标的检测\n\nNom-maximum suppression把多个预选框融合成一个预选框，得到最后的bounding box，输出就是最后的检测结果了\n\n参考文档\nhttps://blog.csdn.net/qq_45104603/article/details/121783848\nhttps://blog.csdn.net/m0_51261924/article/details/131650365\nhttps://blog.csdn.net/HUASHUDEYANJING/article/details/126023842\n\n","slug":"YOLOv5初次尝试","date":"2023-10-25T15:07:58.000Z","categories_index":"AI","tags_index":"","author_index":"Yesord"},{"id":"26be7765875c7794e84f5b7513278f2a","title":"Hexo + GitHub Pages 自定义域名的设置","content":"背景我在创建个人博客之后就有打算改域名了，只是碍于空闲时间不够充裕没有去实现。到了周末，我抽了大段空闲时间来搞这个，在其中花费了大量时间去配置DNS和SSL，走了许多弯路，我本人不是IT专业的，希望本文能帮到那些和我一样遇到问题（想搞个自己的域名装13）的人吧。\n具体操作本文只演示如何将Hexo + GitHub Pages 创建的静态网站域名从 xxx.github.io 改成 自定义域名\n域名获取可以在万网上购买自己想要的域名https://wanwang.aliyun.com/\n要先注册账号，购买域名是要实名认证的，一般流程是要等待身份审核通过，但这就要花2~3天去等待。可以先购买域名（使用阿里云的公共云账号），之后的操作会与一般流程不同，但可以不需要等待，短时间完成。\n配置本地博客文件夹在public文件夹中创建CNAME文件（注意不要有后缀）\n在CNAME中添加如下内容\nbash自定义域名（不要https://）再将本地文件上传到GitHub仓库\nbashhexo g &amp;&amp; hexo d完成上述步骤后，在仓库的Setting的Page下就可以看到域名指向你自己的域名啦\n如果是在GitHub仓库里创建CNAME的话就会导致之后你上传blog时会导致CNAME文件被删除（因为本地没有），就要重新写一份CNAME。\n配置DNS在阿里云的控制台里可以进行域名解析。使用Cloudflare可以不需要实名认证进行域名解析，详细操作可以去参考其他博客。\n\n工作台进入云解析，快速实名认证，认证完之后要进行账户转移，把账户从阿里云转到你的账户中，不然无法执行域名解析\n完成上述操作之后就可以执行域名解析了，可以在解析设置中添加记录，详细配置如下，配置完成之后等待几分钟之后DNS就配置好了\n\nbash\n记录类型：CNAME\n\n主机记录：填你的域名\n\n\n解析请求来源：默认\n\n记录值： xxx.github.io\n\nTTL：10分钟\n域名解析之后就可以以你自定义的域名去打开GitHub Pages创建的静态网站了。注：输入之前的xxx.github.io也会被指向你自定义的域名到这一步实际上就已经基本完成了，github.io域名已经指向你自定义的域名了。但是还是会有一点点小瑕疵，那就是这个域名没有配置SSL加密，它的前缀是http会导致你在浏览器上打开时会一直显示不安全，yysy，个人博客一直挂着一个不安全就是真没事看着也难受。\n配置SSL可以在阿里云上申请免费SSL证书。（一年为期限）通过配置免费证书可以使你的域名加密，从http变成https\n\n打开阿里云控制台数字证书管理服务-&gt;SSL证书-&gt;免费证书 提交申请会获得20个网站的SSL证书\n创建证书-&gt;证书申请-&gt;提交审核\n\nbash证书绑定域名： 你自定义的域名\n域名验证方式： 手工DNS验证\nCSR生成方式： 系统生成系统会帮你配置好DNS不用管，只要在域名解析里把新生成的SSL的域名解析重启就行。\n配置GitHub仓库在Pages界面Custom domain下勾选Enforce HTTPS 即可。前头不重启SSL的域名解析会导致这里的Enforce HTTPS是不可选取的\n结语这次配置二级域名是真的折磨，基本把能踩的坑都踩了一遍。\n参考网站：\n\nhttps://blog.csdn.net/weixin_44129672/article/details/104763893\n\nhttps://www.moerats.com/archives/616/\n\n\n","slug":"Hexo-GitHub-Pages-二级域名的设置","date":"2023-10-22T13:02:08.000Z","categories_index":"IT","tags_index":"blog","author_index":"Yesord"},{"id":"bd7f3e90fea1abdc8b58a7bfc4c3d088","title":"颜色对应的代码","content":"颜色代码6位码可直接复制bash\n红色        #FF0000 \n深紫色      #871F78 \n褐红色      #8E236B \n石英色      #D9D9F3\n绿色        #00FF00 \n深石板蓝    #6B238E \n中海蓝色    #32CD99 \n艳蓝色      #5959AB\n蓝色        #0000FF \n深铅灰色    #2F4F4F \n中蓝色      #3232CD \n鲑鱼色      #6F4242\n牡丹红      #FF00FF \n深棕褐色    #97694F \n中森林绿    #6B8E23 \n猩红色      #BC1717\n青色        #00FFFF \n深绿松石色  #7093DB \n中鲜黄色    #EAEAAE \n海绿色      #238E68\n黄色        #FFFF00 \n暗木色      #855E42 \n中兰花色    #9370DB \n半甜巧克力色 #6B4226\n黑色        #000000 \n淡灰色      #545454 \n中海绿色    #426F42 \n赭色        #8E6B23\n海蓝        #70DB93 \n土灰玫瑰红色 #856363 \n中石板蓝色  #7F00FF \n银色        #E6E8FA\n巧克力色    #5C3317 \n长石色      #D19275 \n中春绿色    #7FFF00 \n天蓝        #3299CC\n蓝紫色      #9F5F9F \n火砖色      #8E2323 \n中绿松石色  #70DBDB \n石板蓝      #007FFF\n黄铜色      #B5A642 \n森林绿      #238E23 \n中紫红色    #DB7093 \n艳粉红色    #FF1CAE\n亮金色      #D9D919 \n金色        #CD7F32 \n中木色      #A68064 \n春绿色      #00FF7F\n棕色        #A67D3D \n鲜黄色      #DBDB70 \n深藏青色    #2F2F4F \n钢蓝色      #236B8E\n青铜色      #8C7853 \n灰色        #C0C0C0 \n海军蓝      #23238E \n亮天蓝色    #38B0DE\n2号青铜色   #A67D3D \n铜绿色      #527F76 \n霓虹篮      #4D4DFF \n棕褐色      #DB9370\n士官服蓝色  #5F9F9F \n青黄色      #93DB70 \n霓虹粉红    #FF6EC7 \n紫红色      #D8BFD8\n冷铜色      #D98719 \n猎人绿      #215E21 \n新深藏青色  #00009C \n石板蓝色    #ADEAEA\n铜色        #B87333 \n印度红      #4E2F2F \n新棕褐色    #EBC79E \n浓深棕色    #5C4033\n珊瑚红      #FF7F00 \n土黄色      #9F9F5F \n暗金黄色    #CFB53B \n淡浅灰色    #CDCDCD\n紫蓝色      #42426F\n浅蓝色      #C0D9D9\n橙色        #FF7F00 \n紫罗兰色    #4F2F4F\n深棕        #5C4033 \n浅灰色      #A8A8A8 \n橙红色      #FF2400 \n紫罗兰红色  #CC3299\n深绿        #2F4F2F \n浅钢蓝色    #8F8FBD \n淡紫色      #DB70DB \n麦黄色      #D8D8BF\n深铜绿色    #4A766E \n浅木色      #E9C2A6 \n浅绿色      #8FBC8F \n黄绿色      #99CC32\n深橄榄绿    #4F4F2F \n石灰绿色    #32CD32 \n粉红色      #BC8F8F\n深兰花色    #9932CD \n桔黄色      #E47833 \n李子色      #EAADEA\n图解\n\n\n\n\n\n\n\n\n\n\n\n\n\n在线6位色彩码转换器https://www.rapidtables.org/zh-CN/convert/color/index.html\n8位码未完待续\nEND","slug":"颜色对应的代码","date":"2023-10-21T08:59:35.000Z","categories_index":"","tags_index":"undefined","author_index":"Yesord"},{"id":"0efe1085ac0265965eb4a094f23bbe8f","title":"Keil编译常见Error及其解决方案","content":"Keil编译常见Error及其解决方案这里总结了本人使用Keil所遇到过的所有编译报错，并附上解决方案可供参考。\nError: L6218E: Undefined symbol __aeabi_assert (referred from xxx.o).原因：引用 #include &lt;assert.h&gt; 断言功能缺失方案一未定义的符号__aeabi_assert,原因是keil没有添加依赖项，请在 Manage Run-time Evironment 中添加即可。Compiler–&gt; I&#x2F;O –&gt; STDERR\n方案二bash\n__attribute__((weak,noreturn))\nvoid __aeabi_assert (const char *expr, const char *file, int line) &#123;\n  char str[12], *p;\n\n  fputs(&quot;*** assertion failed: &quot;, stderr);\n  fputs(expr, stderr);\n  fputs(&quot;, file &quot;, stderr);\n  fputs(file, stderr);\n  fputs(&quot;, line &quot;, stderr);\n\n  p = str + sizeof(str);\n  *--p = &#39;\\0&#39;;\n  *--p = &#39;\\n&#39;;\n  while (line &gt; 0) &#123;\n    *--p = &#39;0&#39; + (line % 10);\n    line /= 10;\n  &#125;\n  fputs(p, stderr);\n\n  abort();\n&#125;\n\n__attribute__((weak))\nvoid abort(void) &#123;\n  for (;;);\n&#125;\n在使用Arm MicroLIB C库时，实现一个自定义的__aeabi_assert()函数。使用上面的代码作为模板。\n禁用assert(): On Project -&gt; Options For Target -&gt; On C&#x2F; c++选项卡，用于定义类型“NDEBUG”。-这会导致对assert()函数的调用不起作用。\n禁用MicroLIB:在Target对话框右上方的Project -&gt; Options For Target -&gt;下，取消选中Use MicroLIB以禁用Arm MicroLIB C库。\n修改完成后，重新构建项目使其生效。————————————————可参考链接：https://blog.csdn.net/qq_29246181/article/details/128777718\nError: L6218E: Undefined symbol xxx (referred from xxx.o)原因：函数未定义解决方案：确定该函数的调用在什么地方出现问题1.检查头文件路径2.探查函数是否有定义 ctrl+F\n","slug":"Keil编译常见Error及其解决方案","date":"2023-10-20T14:28:16.000Z","categories_index":"Embedded","tags_index":"IDE","author_index":"Yesord"},{"id":"7742e69e813648d5c85a45d015f995f0","title":"STM32F103RCT6核心板设计","content":"原理图总览\n功能bash1. 四按键模块（共阴极）\n2. ST-LINK接口\n3. TypeC通信和XH座子通信\n4. 内置CH340可以串口ISP\n5. 外部FLASH和EEPROM\n6. 2个引脚LED\n7. 7线0.96寸OLED屏接口\n8. 拉出STM32F103RCT6基本所有引脚\n9. 蜂鸣器\n10. 可使用跳线和按键控制外部设备的启动应该加个电源开关的，搞忘了。\n电源模块采用AMS1117-3.3线性稳压器性能一般般，主要是便宜，设计的电路也不复杂，相比其他什么开关电源（MP2315之类的），发热比较严重吧，纹波也比较大。\n蜂鸣器可使用有源蜂鸣器or无源蜂鸣器\nPCB设计\n\n\n\n\n\nTIP\n使用四层板\n\n\n\n图层名\n作用\n\n\n\ntoplayer\n信号层1\n\n\ninnerlayer1\n5V内电层\n\n\ninnerlayer2\n3V3内电层\n\n\nbottomlayer\n信号层2\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING\n\n在设计晶振时需要保证晶振电路和其他电路相隔离（通过GND），保证晶振的高频信号不会对其他电路产生电磁干扰。\n芯片的电源输入一定要有旁路电容接地（经验值100nF），而且该电容一定要放在芯片电源引脚边上，保证信号的回路尽量短，该电容其实很有讲究的，功能挺多的，感兴趣之后可以再讨论。\n高频信号尽量不过两次孔，低频信号不过三次孔，减少过孔寄生效应的不利影响，高频信号线之间尽量铺GND隔离，需要严格同步时序的高频信号要保证信号线长度相等。\n焊接时候先焊电源，不然电源一炸之后的单片机也跟着炸，直接输光光。\n\n\n\n物理尺寸整板大小：7cm x 6cm定位孔：M3OLED定位孔：M2\n后记本板子还未经验证，等验证完之后我会开源在JLC。之后还会设计一个扩展板用于心电检测。\nPS: \n\n本来想着把板子做小，做成6*6，奈何layout水平不够。。。\n在最后布地线是真的折磨，我本来以为最后铺铜就可以保证大部分GND相连了。。最后还是改了半个多小时。。。（还好这个板子可以全部设备共地，不用分什么模拟地、数字地、直流地、交流地的）\n\n参考资料数据手册\n\n\n\n\n\n\n\n\n\nCH340\nBY25Q64ASSIG\nAT24C02\nASM1117\nSTM32F1x\n\nPCB布线原则https://blog.csdn.net/zerokingwang/article/details/127651767\n","slug":"STM32F103RCT6核心板设计","date":"2023-10-20T13:50:03.000Z","categories_index":"Electronic","tags_index":"STM32","author_index":"Yesord"},{"id":"507d422f151c505ace4132bc7d4f4fff","title":"LVGL的移植","content":"前言LVGL是一个免费好用的嵌入式GUI库，其控件扁平化，符合当今大众审美，主要应用在一些像智能手表之类的产品之上。前几天刚搞到一块STM32F103RCT6的最小系统板+ST7735S的TFT-LCD（可惜了不是触摸屏QAQ），今咱就来尝试移植一下LVGL到STM32上，整点炫的。\nLVGL的移植流程以STM32F103RCT6为例\nLVGL源码下载Github仓库地址 https://github.com/lvgl/lvgl\n准备工作准备LVGL源码修改lv_conf.h文件，修改条件编译\n删除不必要文件\n添加工程文件需要添加文件需要添加头文件路径特殊配置(可选)屏蔽warning\n修改工程文件(裸机开发)总体配置修改lv_conf.h\nbash\n/* clang-format off */\n#if 1 /*Set it to &quot;1&quot; to enable content*/\n// 原先这里是#if 0 需改成 #if 1\n添加output设备修改lv_port_disp_template.h为lv_port_disp.h对应的头文件名在对应c文件中也要更改\nbash\n/*Copy this file as &quot;lv_port_disp.h&quot; and set this value to &quot;1&quot; to enable content*/\n#if 1 //使能lv_port_disp.h\n\n修改lv_port_disp_template.c\nbash\n// 添加自己的屏幕尺寸\n\n #define MY_DISP_HOR_RES    128 //水平\n #define MY_DISP_VER_RES    160 //垂直\n\n// 修改使用的buffer算法（三选一）\n// 有详细介绍，可以认为从上到下性能递增，占用内存也递增\n/* Example for 1) */\n    static lv_disp_draw_buf_t draw_buf_dsc_1;\n    static lv_color_t buf_1[MY_DISP_HOR_RES * 10];                          /*A buffer for 10 rows*/\n    lv_disp_draw_buf_init(&amp;draw_buf_dsc_1, buf_1, NULL, MY_DISP_HOR_RES * 10);   /*Initialize the display buffer*/\n\n//    /* Example for 2) */\n//    static lv_disp_draw_buf_t draw_buf_dsc_2;\n//    static lv_color_t buf_2_1[MY_DISP_HOR_RES * 10];                        /*A buffer for 10 rows*/\n//    static lv_color_t buf_2_2[MY_DISP_HOR_RES * 10];                        /*An other buffer for 10 rows*/\n//    lv_disp_draw_buf_init(&amp;draw_buf_dsc_2, buf_2_1, buf_2_2, MY_DISP_HOR_RES * 10);   /*Initialize the display buffer*/\n\n//    /* Example for 3) also set disp_drv.full_refresh = 1 below*/\n//    static lv_disp_draw_buf_t draw_buf_dsc_3;\n//    static lv_color_t buf_3_1[MY_DISP_HOR_RES * MY_DISP_VER_RES];            /*A screen sized buffer*/\n//    static lv_color_t buf_3_2[MY_DISP_HOR_RES * MY_DISP_VER_RES];            /*Another screen sized buffer*/\n//    lv_disp_draw_buf_init(&amp;draw_buf_dsc_3, buf_3_1, buf_3_2,\n//                          MY_DISP_VER_RES * LV_VER_RES_MAX);   /*Initialize the display buffer*/\n\n// 修改 disp_init 放入自己的屏幕驱动\n\n/*Initialize your display and the required peripherals.*/\nstatic void disp_init(void)\n&#123;\n    /*You code here*/\n  LCD_Init();\n  \n&#125;\n\n// 修改 disp_flush 放入自己屏幕的色块填充程序\n\n/*Flush the content of the internal buffer the specific area on the display\n *You can use DMA or any hardware acceleration to do this operation in the background but\n *&#39;lv_disp_flush_ready()&#39; has to be called when finished.*/\nstatic void disp_flush(lv_disp_drv_t * disp_drv, const lv_area_t * area, lv_color_t * color_p)\n&#123;\n//    if(disp_flush_enabled) &#123;\n//        /*The most simple case (but also the slowest) to put all pixels to the screen one-by-one*/\n\n//        int32_t x;\n//        int32_t y;\n//        for(y = area-&gt;y1; y &lt;= area-&gt;y2; y++) &#123;\n//            for(x = area-&gt;x1; x &lt;= area-&gt;x2; x++) &#123;\n//                /*Put a pixel to the display. For example:*/\n//                /*put_px(x, y, *color_p)*/\n//                color_p++;\n//            &#125;\n//        &#125;\n//    &#125;\n    LCD_Fill(area-&gt;x1,area-&gt;y1,area-&gt;x2,area-&gt;y2,(uint16_t)color_p);\n    /*IMPORTANT!!!\n     *Inform the graphics library that you are ready with the flushing*/\n    lv_disp_flush_ready(disp_drv);\n&#125;\n添加input设备LVGL支持编码器、按键、触摸屏等输入\nbash\n// 还没写捏~~\n// 产能不足惹~~\n添加lvgl时基定时器驱动文件加入 #include”lvgl.h”定时器中断设置1ms中断，放入lv_tick_inc\nbash\nvoid TIM3_Handler(void)&#123; //定时器配置1ms中断\n    lv_tick_inc(1);\n&#125;\n\n测试程序可能无法显示需要去改startup文件的堆栈大小 \nFreeRTOS下的移植基本同裸机的配置相差不大，也就需要改个时基，使用FreeRTOS的\n时基配置lv_conf.h中添加自定义时基\nbash\n/*Use a custom tick source that tells the elapsed time in milliseconds.\n *It removes the need to manually update the tick with `lv_tick_inc()`)*/\n#define LV_TICK_CUSTOM 1 //使能1，默认是0\n#if LV_TICK_CUSTOM\n    #define LV_TICK_CUSTOM_INCLUDE &quot;FreeRTOS.h&quot;         /*Header for the system time function*/\n    #define LV_TICK_CUSTOM_SYS_TIME_EXPR (xTaskGetTickCount())    /*Expression evaluating to current system time in ms*/\n注意该配置与裸机移植的配置不能兼容。\n后记刚学LVGL，移个植都费劲，库函数也还没理清，路漫漫其修远兮啊。。。\n","slug":"LVGL的移植","date":"2023-10-18T14:26:45.000Z","categories_index":"Embedded","tags_index":"TPSW","author_index":"Yesord"},{"id":"11caf7d9cf7a688f2728c9360ab7d311","title":"图片转链接--SM.MS图床","content":"SM.SM图床网站推荐引言平时大家编写.md文件时肯定有对图片有所苦恼。正常从本机引用的话，跨设备图片就会无法显示，如果调到github仓库里又会比较麻烦。\n大家肯定会想要一个方法既能解决跨设备图片显示，操作起来又没那么困难的方法。\n正文在这里给大家推荐一个网站 https://smms.app/\n其界面UI如下：\n可以支持五种链接格式\n结语阿巴阿巴阿巴阿巴阿巴\n","slug":"图片转链接-SM-MS图床","date":"2023-10-17T14:01:58.000Z","categories_index":"IT","tags_index":"tool","author_index":"Yesord"},{"id":"7306721a27e9ec7e2372287f94d883c8","title":"Cmake基础语法","content":"Cmake基础语法bash# 设置cmake最低版本\ncmake_minimum_required(VERSION 3.2)\n\n# project命令用于指定cmake工程的名称，实际上，它还可以指定cmake工程的版本号（VERSION关键字）、简短的描述（DESCRIPTION关键字）、主页URL（HOMEPAGE_URL关键字）和编译工程使用的语言（LANGUAGES关键字）\n# project(&lt;PROJECT-NAME&gt; [&lt;language-name&gt;...])\n# project(&lt;PROJECT-NAME&gt; [VERSION &lt;major&gt;[.&lt;minor&gt;[.&lt;patch&gt;[.&lt;tweak&gt;]]]] [DESCRIPTION &lt;project-description-string&gt;][HOMEPAGE_URL &lt;url-string&gt;] [LANGUAGES &lt;language-name&gt;...])\n# $&#123;PROJECT_SOURCE_DIR&#125; 和 &lt;PROJECT-NAME&gt;_SOURCE_DIR：本CMakeLists.txt所在的文件夹路径\n# $&#123;PROJECT_NAME&#125;：本CMakeLists.txt的project名称\nproject(xxx)\nproject(mytest VERSION 1.2.3.4)\nproject (mytest HOMEPAGE_URL “https://www.XXX(示例).com”)\n\n# 获取路径下所有的.cpp/.c/.cc文件（不包括子目录），并赋值给变量中\naux_source_directory(路径 变量)\n\n# GLOB_RECURSE 获取目录下的所有cpp文件（不包括子目录），并赋值给SOURCES\nfile(\n        GLOB SOURCES\n        $&#123;PROJECT_SOURCE_DIR&#125;/*.c\n\n)\n# GLOB_RECURSE 获取目录下的所有cpp文件（包括子目录），并赋值给NATIVE_SRC\nfile(\n      GLOB_RECURSE NATIVE_SRC \n      $&#123;PROJECT_SOURCE_DIR&#125;/lib/*.cpp\n)\n\n# 给文件名/路径名或其他字符串起别名，用$&#123;变量&#125;获取变量内容\nset(变量 文件名/路径/...)\n\n# 添加编译选项FOO BAR\n# add_definitions定义宏，但是这种定义方式无法给宏具体值 等价C语言中的#define  MG_ENABLE_OPENSSL\nadd_definitions(-DFOO -DBAR ...)\n\n# add_compile_definitions定义宏，这种方式可以给宏具体值，但是这个指令只要高版本的cmake支持 等价C语言中 #define  MG_ENABLE_OPENSSL   1 \nadd_compile_definitions(MG_ENABLE_OPENSSL=1)\n\n# 打印消息\nmessage(消息)\n\n# 编译子文件夹的CMakeLists.txt\nadd_subdirectory(子文件夹名称)\n\n# 将.cpp/.c/.cc文件生成.a静态库\n# 注意，库文件名称通常为libxxx.so，在这里只要写xxx即可\nadd_library(库文件名称 STATIC 文件)\n\n# 将.cpp/.c/.cc文件生成可执行文件\nadd_executable(可执行文件名称 文件)\n\n# 规定.h头文件路径\ninclude_directories(路径)\n\n# 规定.so/.a库文件路径\nlink_directories(路径)\n\n# 设置编译选项及默认值\noption(TEST_DEBUG &quot;option for debug&quot; OFF)\n\n# 对add_library或add_executable生成的文件进行链接操作\n# 注意，库文件名称通常为libxxx.so，在这里只要写xxx即可\ntarget_link_libraries(库文件名称/可执行文件名称 链接的库文件名称)\n\n \n基础配置流程\nproject(xxx)                                          #必须\n\nadd_subdirectory(子文件夹名称)                         #父目录必须，子目录不必\n\nadd_library(库文件名称 STATIC 文件)                    #通常子目录(二选一)\nadd_executable(可执行文件名称 文件)                     #通常父目录(二选一)\n\ninclude_directories(路径)                              #必须\nlink_directories(路径)                                 #必须\n\ntarget_link_libraries(库文件名称/可执行文件名称 链接的库文件名称)       #必须\n     ","slug":"Cmake基础语法","date":"2023-10-17T11:57:43.000Z","categories_index":"Embedded","tags_index":"Linux,Cmake","author_index":"Yesord"},{"id":"92468dc8f04e07f9e105708cbbdde9c2","title":"FreeRTOS的移植","content":"FreeRTOS的移植介绍下FreeRTOS源码内容\n\n\n名称\n描述\n\n\n\nFreeRTOS\nFreeRTOS内核\n\n\nFreeRTOS-Plus\nFreeRTOS组件\n\n\ntools\n工具\n\n\nGitHub-FreeRTOS-Home\nFreeRTOS的GitHub仓库链接\n\n\nQuick_Start_Guide\n快速入门指南官方文档链接\n\n\nUpgrading-to-FreeRTOS-xxx\n升级到指定FreeRTOS版本官方文档链接\n\n\nHistory.txt\nFreeRTOS历史更新记录\n\n\n其他\n其他\n\n\nFreeRTOS内核\n\n\n名称\n描述\n\n\n\nDemo\nFreeRTOS演示例程\n\n\nLicense\nFreeRTOS相关许可\n\n\nSource\nFreeRTOS源码（主要拷贝）\n\n\nTest\n公用以及移植层测试代码\n\n\nSource文件夹freeRTOS源码\n\n\n\n名称\n描述\n\n\n\ninclude\n内包含了FreeRTOS的头文件\n\n\nportable\n内包含了FreeRTOS的移植文件\n\n\ncroutine.c\n协程相关文件\n\n\nevent_groups.c\n事件相关文件\n\n\nlist.c\n列表相关文件（状态相关，必要）\n\n\nqueue.c\n队列相关文件（状态相关，必要）\n\n\nstream_buffer.c\n流式缓冲区相关文件\n\n\ntasks.c\n任务相关文件（必要）\n\n\ntimers.c\n软件定时器相关文件\n\n\nportable文件夹freeRTOS与硬件交互的桥梁\n\n\n\n名称\n描述\n\n\n\nKeil\n指向RVDS文件夹\n\n\nRVDS\n不同内核芯片的移植文件\n\n\nMemMang\n内存管理文件（5种算法）\n\n\n其他的文件用不着可以删掉\nFreeRTOS移植步骤以STM32F103为例\n1．添加FreeRTOS源码添加入工程\nFreeRTOS源码导入工程\nportable里这些文件导入工程(其他的删掉!!)\n在Keil中建立如下组\n注意配置头文件地址\n2．添加FreeRTOSConfig.h作用：对FreeRTOS进行配置和裁剪，以及API函数的使能操作系统配置文件:获取途径\na.自己手写FreeRTOS官网有详细说明（https://www.freertos.org/a00110.html）\nb.Demo例程或许有官方支持（但也不全，还没很多的注释，建议还是自己写）\nDemo这个里的FreeRTOSConfig.h可使用\nc.参考我写哒\nbash\n#ifndef FREERTOS_CONFIG_H\n#define FREERTOS_CONFIG_H\n\n/*-----------------------------------------------------------\n * Application specific definitions.\n *\n * These definitions should be adjusted for your particular hardware and\n * application requirements.\n *\n * THESE PARAMETERS ARE DESCRIBED WITHIN THE &#39;CONFIGURATION&#39; SECTION OF THE\n * FreeRTOS API DOCUMENTATION AVAILABLE ON THE FreeRTOS.org WEB SITE.\n *\n * See http://www.freertos.org/a00110.html\n *----------------------------------------------------------*/\n\n#define configUSE_PREEMPTION\t\t1   //使用抢占式内核\n//#define configUSE_TIME_SLICING  1   //使用时间片调度（默认是使能的）\n#define configUSE_IDLE_HOOK\t\t\t0   //使用模式的钩子函数\n#define configUSE_TICK_HOOK\t\t\t0   //使用TICK的钩子函数\n#define configUSS_PORT_OPTIMISED_SELECTION  1  //1使用硬件选择下一个执行任务 （需要架构拥有计算前导零[CLZ]的指令）\n#define configUSE_TICKLESS_IDLE 0   //1使用低功耗模式\n#define configUSE_QUEUE_SETS    1   //1使用队列集\n#define configCPU_CLOCK_HZ\t\t\t( ( unsigned long ) 72000000 )  //CPU主频\n//#define configSYSTICK_CLOCK_HZ  (configCPU_CLOCK_HZ)  //定义systick的时钟频率 默认与系统时钟相同。\n#define configTICK_RATE_HZ\t\t\t( ( TickType_t ) 1000 )\n#define configMAX_PRIORITIES\t\t( 32 )  //可使用最大优先级\n#define configMINIMAL_STACK_SIZE\t( ( unsigned short ) 128 )\n\n#define configMAX_TASK_NAME_LEN\t\t( 16 )\n\n#define configUSE_16_BIT_TICKS\t\t0   //设置系统节拍计数器的数据类型 0为32位\n#define configIDLE_SHOULD_YIELD\t\t1   //1同优先级的任务可以抢占空闲任务\n#define configUSE_TASK_NOTIFCATIONS 1 //1 使能任务间的消息传递\n#define configENABLE_BACKWARD_COMPATIBILITY 0 //1 兼容老版本API函数\n\n/*Memory Allocation*/\n#define configTOTAL_HEAP_SIZE\t\t( ( size_t ) ( 17 * 1024 ) )  //17k总堆栈\n#define configSUPPORT_STATIC_ALLOCATION   0   //1 支持静态申请分配内存，默认0\n#define configSUPPORT_DYNAMIC_ALLOCATION  1   //1 支持动态申请分配内存，默认1\n\n/*Debug*/\n#define configUSE_TRACE_FACILITY\t1   //使能可视化追踪调试\n#define configUSE_STATS_FORMATTING_FUNCTION 1\n\n/*Software Timer*/\n#define configUSE_TIMERS  1 //1 使能软件定时器\n#define configTIMER_TASK_PRIORITY (configMAX_PRIORITIES - 1)\n#define configTIMER_QUEUE_LENGTH  5\n#define configTIMER_TASK_STACK_DEPTH  (configMINIMAL_STACK_SIZE * 2)\n\n\n/* Set the following definitions to 1 to include the API function, or zero\nto exclude the API function. */\n\n#define INCLUDE_vTaskPrioritySet\t\t    1   //设置任务优先级 \n#define INCLUDE_uxTaskPriorityGet\t\t    1   //获取任务优先级\n#define INCLUDE_vTaskDelete\t\t\t\t      1   //删除任务\n#define INCLUDE_vTaskCleanUpResources\t  1   //\n#define INCLUDE_vTaskSuspend\t\t\t      1   //挂起任务\n#define INCLUDE_xResumeFromISR          1   //恢复在中断中挂起的任务\n#define INCLUDE_vTaskDelayUntil\t\t\t    1   //任务绝对延时\n#define INCLUDE_vTaskDelay\t\t\t\t      1   //任务延时\n#define INCLUDE_xTaskGetSchedulerState  1   //\n#define INCLUDE_xTaskGetCurrentTaskHandle 1 //\n//#define INCLUDE_xTimerPendFunctionCall  1   //\n#define INCLUDE_eTaskGetState           1   //\n\n/* This is the raw value as per the Cortex-M3 NVIC.  Values can be 255\n(lowest) to 0 (1?) (highest). */\n#define configKERNEL_INTERRUPT_PRIORITY \t\t255\n/* !!!! configMAX_SYSCALL_INTERRUPT_PRIORITY must not be set to zero !!!!\nSee http://www.FreeRTOS.org/RTOS-Cortex-M3-M4.html. */\n#define configMAX_SYSCALL_INTERRUPT_PRIORITY \t191 /* equivalent to 0xb0, or priority 11. */\n\n/*FreeRTOS interrupt relative define*/\n#define xPortPendSVHandler    PendSV_Handler\n#define vPortSVCHandler       SVC_Handler\n\n/* This is the value being used as per the ST library which permits 16\npriority values, 0 to 15.  This must correspond to the\nconfigKERNEL_INTERRUPT_PRIORITY setting.  Here 15 corresponds to the lowest\nNVIC value of 255. */\n#define configLIBRARY_KERNEL_INTERRUPT_PRIORITY\t15\n\n/*Assert*/\n//#define vAssertCalled(char, int) printf(&quot;Error: %s, %d\\r\\n&quot;, char, int)\n//#define configASSERT(x) if((x) == 0) vAssertCalled(__FILE__,__LINE__)\n\n\n#endif /* FREERTOS_CONFIG_H */\n3．修改系统相关配置Systick 系统滴答时钟需要与FreeRTOSConfig.h里的configTICK_RATE_HZ对应滴答时钟中断需要加上任务切换\n4．修改中断相关配置Systick中断（自己配置）、SVC中断、PendSV中断（这俩在freeRTOS中有定义）因而在stm32f1xx_it.c中需要将相关中断定义注释掉\n一般要在SysTick_Handler中写\nbashvoid SysTick_Handler(void)&#123;\n  if(xTaskGetSchedulerState() != taskSCHEDULER_NOT_STARTED)&#123;//任务切换相关\n    xPortSysTickHandler(); //SVC和PendSV中断入口\n  &#125;\n&#125;没有注释编译就会出现这种情况\nbash\n.\\Objects\\Project.axf: Error: L6200E: Symbol SVC_Handler multiply defined (by port.o and stm32f10x_it.o).\n.\\Objects\\Project.axf: Error: L6200E: Symbol PendSV_Handler multiply defined (by port.o and stm32f10x_it.o).5．添加应用程序因人而异啦~\n源码参考基于STM32F103C8T6的FreeRTOS移植与简单配置（没有测试程序捏~~）https://github.com/Yesord/FreeROTS_STM32F103C8T6\nFreeRTOS移植可能出现的问题1.在学习FreeRTOS移植后，编译出现bash…\\OBJ\\LED.axf: Error: L6218E: Undefined symbol xTaskGetSchedulerState (referred from delay.o).出现这个错误的原因是xTaskGetSchedulerState的值没有改，需要在FreeRTOS.h中将其宏定义的值改为1即可。也可以去FreeRTOSConfig.h中把INCLUDE_xTaskGetScheduler 赋1\n后记配图还待更新，小编比较懒，不定时更新 ^.^\n","slug":"FreeRTOS的移植","date":"2023-10-17T09:33:25.000Z","categories_index":"Embedded","tags_index":"RTOS,STM32","author_index":"Yesord"},{"id":"5fb0b2232f7b008486299018a9a2ee11","title":"markdown-example","content":"@TOC\n欢迎使用Markdown编辑器你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。\n新的改变我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客：\n\n全新的界面设计 ，将会带来全新的写作体验；\n在创作中心设置你喜爱的代码高亮样式，Markdown 将代码片显示选择的高亮样式 进行展示；\n增加了 图片拖拽 功能，你可以将本地的图片直接拖拽到编辑区域直接展示；\n全新的 KaTeX数学公式 语法；\n增加了支持甘特图的mermaid语法^1 功能；\n增加了 多屏幕编辑 Markdown文章功能；\n增加了 焦点写作模式、预览模式、简洁写作模式、左右区域同步滚轮设置 等功能，功能按钮位于编辑区域与预览区域中间；\n增加了 检查列表 功能。\n\n功能快捷键撤销：Ctrl/Command + Z重做：Ctrl/Command + Y加粗：Ctrl/Command + B斜体：Ctrl/Command + I标题：Ctrl/Command + Shift + H无序列表：Ctrl/Command + Shift + U有序列表：Ctrl/Command + Shift + O检查列表：Ctrl/Command + Shift + C插入代码：Ctrl/Command + Shift + K插入链接：Ctrl/Command + Shift + L插入图片：Ctrl/Command + Shift + G查找：Ctrl/Command + F替换：Ctrl/Command + G\n合理的创建标题，有助于目录的生成直接输入1次#，并按下space后，将生成1级标题。输入2次#，并按下space后，将生成2级标题。以此类推，我们支持6级标题。有助于使用TOC语法后生成一个完美的目录。\n如何改变文本的样式强调文本 强调文本\n加粗文本 加粗文本\n&#x3D;&#x3D;标记文本&#x3D;&#x3D;\n删除文本\n\n\n\n\n\n\n\n\n\n引用文本\nH2O is是液体。\n2^10^ 运算结果是 1024.\n插入链接与图片链接: link.\n图片: \n带尺寸的图片: ![Alt](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hdmF0YXIuY3Nkbi5uZXQvNy83L0IvMV9yYWxmX2h4MTYzY29tLmpwZw &#x3D;30x30)\n居中的图片: \n居中并且带尺寸的图片: ![Alt](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hdmF0YXIuY3Nkbi5uZXQvNy83L0IvMV9yYWxmX2h4MTYzY29tLmpwZw#pic_center &#x3D;30x30)\n当然，我们为了让用户更加便捷，我们增加了图片拖拽功能。\n如何插入一段漂亮的代码片去博客设置页面，选择一款你喜欢的代码片高亮样式，下面展示同样高亮的 代码片.\n​javascript // An highlighted block var foo = &#39;bar&#39;; ​\n生成一个适合你的列表\n项目\n项目\n项目\n\n\n\n\n\n\n项目1\n项目2\n项目3\n\n\n 计划任务\n 完成任务\n\n创建一个表格一个简单的表格是这么创建的：\n\n\n\n项目\nValue\n\n\n\n电脑\n$1600\n\n\n手机\n$12\n\n\n导管\n$1\n\n\n设定内容居中、居左、居右使用:---------:居中使用:----------居左使用----------:居右\n\n\n\n第一列\n第二列\n第三列\n\n\n\n第一列文本居中\n第二列文本居右\n第三列文本居左\n\n\nSmartyPantsSmartyPants将ASCII标点字符转换为“智能”印刷标点HTML实体。例如：\n\n\n\nTYPE\nASCII\nHTML\n\n\n\nSingle backticks\n&#39;Isn&#39;t this fun?&#39;\n‘Isn’t this fun?’\n\n\nQuotes\n&quot;Isn&#39;t this fun?&quot;\n“Isn’t this fun?”\n\n\nDashes\n-- is en-dash, --- is em-dash\n– is en-dash, — is em-dash\n\n\n创建一个自定义列表MarkdownText-to-HTML conversion toolAuthors:  JohnLuke如何创建一个注脚一个具有注脚的文本。^2\n注释也是必不可少的Markdown将文本转换为 HTML。\n*[HTML]:   超文本标记语言\nKaTeX数学公式您可以使用渲染LaTeX数学表达式 KaTeX:\nGamma公式展示 $\\Gamma(n) &#x3D; (n-1)!\\quad\\foralln\\in\\mathbb N$ 是通过欧拉积分\n$$\\Gamma(z) &#x3D; \\int_0^\\infty t^{z-1}e^{-t}dt,.$$\n\n\n\n\n\n\n\n\n\n你可以找到更多关于的信息 LaTeX 数学表达式here.\n新的甘特图功能，丰富你的文章​mermaid gantt         dateFormat  YYYY-MM-DD         title Adding GANTT diagram functionality to mermaid         section 现有任务         已完成               :done,    des1, 2014-01-06,2014-01-08         进行中               :active,  des2, 2014-01-09, 3d         计划一               :         des3, after des2, 5d         计划二               :         des4, after des3, 5d ​\n\n关于 甘特图 语法，参考 这儿,\n\nUML 图表可以使用UML图表进行渲染。 Mermaid. 例如下面产生的一个序列图：\n​&#96;&#96;&#96;mermaidsequenceDiagram张三 -&gt;&gt; 李四: 你好！李四, 最近怎么样?李四–&gt;&gt;王五: 你最近怎么样，王五？李四–x 张三: 我很好，谢谢!李四-x 王五: 我很好，谢谢!Note right of 王五: 李四想了很长时间, 文字太长了不适合放在一行.\n李四–&gt;&gt;张三: 打量着王五…张三-&gt;&gt;王五: 很好… 王五, 你怎么样?​&#96;&#96;&#96;\n这将产生一个流程图。:\n​mermaid graph LR A[长方形] -- 链接 --&gt; B((圆)) A --&gt; C(圆角长方形) B --&gt; D&#123;菱形&#125; C --&gt; D ​\n\n关于 Mermaid 语法，参考 这儿,\n\nFLowchart流程图我们依旧会支持flowchart的流程图：\n​&#96;&#96;&#96;mermaidflowchatst&#x3D;&gt;start: 开始e&#x3D;&gt;end: 结束op&#x3D;&gt;operation: 我的操作cond&#x3D;&gt;condition: 确认？\nst-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op​&#96;&#96;&#96;\n\n关于 Flowchart流程图 语法，参考 这儿.\n\n导出与导入导出如果你想尝试使用此编辑器, 你可以在此篇文章任意编辑。当你完成了一篇文章的写作, 在上方工具栏找到 文章导出 ，生成一个.md文件或者.html文件进行本地保存。\n导入如果你想加载一篇你写过的.md文件，在上方工具栏可以选择导入功能进行对应扩展名的文件导入，继续你的创作。\n自己的理解\nmarkdown可以兼容html语法\n\n","slug":"markdown-example","date":"2023-10-17T09:01:15.000Z","categories_index":"","tags_index":"blog","author_index":"Yesord"},{"id":"b9663f58f18133b35bfe243f3e916a80","title":"Hello World","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.My own theme: aurora.(https://aurora.tridiamond.tech/)\nQuick StartCreate a new postbash$ hexo new &quot;My New Post&quot;More info: Writing\nRun serverbash$ hexo serverMore info: Server\nGenerate static filesbash$ hexo generateMore info: Generating\nDeploy to remote sitesbash$ hexo deployBy the Wayactually you can combine Generate with Deploy\nbash$ hexo g &amp;&amp; hexo dthe new markdown file will be restored in yourblog&#x2F;source&#x2F;_post&#x2F;\nOf course SSH deployment is much slower than the local deployment, you can use the command as follow to deploy in local so that you can check your new blog quickly.\nbash$ hexo sMore info: Deployment\n","slug":"hello-world","date":"2023-10-17T01:55:02.000Z","categories_index":"","tags_index":"blog","author_index":"Yesord"}]